{
  "skills": [
    {
      "slug": "000-jeremy-content-consistency-validator",
      "name": "000-jeremy-content-consistency-validator",
      "description": "Validate messaging consistency across website, GitHub repos, and local documentation generating read-only discrepancy reports. Use when checking content alignment or finding mixed messaging. Trigger with phrases like \"check consistency\", \"validate documentation\", or \"audit messaging\". allowed-tools: Read, WebFetch, WebSearch, Grep, Bash(diff:*), Bash(grep:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# 000 Jeremy Content Consistency Validator\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Access to website content (local build or deployed site)\n- Access to GitHub repositories\n- Local documentation in {baseDir}/docs/ or claudes-docs/\n- WebFetch permissions for remote content\n\n## Instructions\n\n1. Identify and discover all content sources (website, GitHub, local docs)\n2. Extract key messaging, features, versions from each source\n3. Compare content systematically across sources\n4. Identify critical discrepancies, warnings, and informational notes\n5. Generate comprehensive Markdown report\n6. Provide prioritized action items for consistency fixes\n\n## Output\n\n- Comprehensive consistency validation report in Markdown format\n- Executive summary with discrepancy counts by severity\n- Detailed comparison by source pairs (website vs GitHub, etc.)\n- Terminology consistency matrix\n- Prioritized action items with file locations and line numbers\n- Reports saved to consistency-reports/YYYY-MM-DD-HH-MM-SS.md\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Content consistency best practices\n- Documentation style guides\n- Version control strategies for content\n- Multi-platform content management approaches",
      "parentPlugin": {
        "name": "000-jeremy-content-consistency-validator",
        "category": "productivity",
        "path": "plugins/productivity/000-jeremy-content-consistency-validator",
        "version": "1.0.0",
        "description": "Read-only validator that generates comprehensive discrepancy reports comparing messaging consistency across ANY HTML-based website (WordPress, Hugo, Next.js, React, Vue, static HTML, etc.), GitHub repositories, and local documentation. Detects mixed messaging without making changes."
      },
      "filePath": "plugins/productivity/000-jeremy-content-consistency-validator/skills/000-jeremy-content-consistency-validator/SKILL.md"
    },
    {
      "slug": "adapting-transfer-learning-models",
      "name": "adapting-transfer-learning-models",
      "description": "Build this skill automates the adaptation of pre-trained machine learning models using transfer learning techniques. it is triggered when the user requests assistance with fine-tuning a model, adapting a pre-trained model to a new dataset, or performing... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Transfer Learning Adapter\n\nThis skill provides automated assistance for transfer learning adapter tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for transfer learning adapter tasks.\nThis skill streamlines the process of adapting pre-trained machine learning models via transfer learning. It enables you to quickly fine-tune models for specific tasks, saving time and resources compared to training from scratch. It handles the complexities of model adaptation, data validation, and performance optimization.\n\n## How It Works\n\n1. **Analyze Requirements**: Examines the user's request to understand the target task, dataset characteristics, and desired performance metrics.\n2. **Generate Adaptation Code**: Creates Python code using appropriate ML frameworks (e.g., TensorFlow, PyTorch) to fine-tune the pre-trained model on the new dataset. This includes data preprocessing steps and model architecture modifications if needed.\n3. **Implement Validation and Error Handling**: Adds code to validate the data, monitor the training process, and handle potential errors gracefully.\n4. **Provide Performance Metrics**: Calculates and reports key performance indicators (KPIs) such as accuracy, precision, recall, and F1-score to assess the model's effectiveness.\n5. **Save Artifacts and Documentation**: Saves the adapted model, training logs, performance metrics, and automatically generates documentation outlining the adaptation process and results.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Fine-tune a pre-trained model for a specific task.\n- Adapt a pre-trained model to a new dataset.\n- Perform transfer learning to improve model performance.\n- Optimize an existing model for a particular application.\n\n## Examples\n\n### Example 1: Adapting a Vision Model for Image Classification\n\nUser request: \"Fine-tune a ResNet50 model to classify images of different types of flowers.\"\n\nThe skill will:\n1. Download the ResNet50 model and load a flower image dataset.\n2. Generate code to fine-tune the model on the flower dataset, including data augmentation and optimization techniques.\n\n### Example 2: Adapting a Language Model for Sentiment Analysis\n\nUser request: \"Adapt a BERT model to perform sentiment analysis on customer reviews.\"\n\nThe skill will:\n1. Download the BERT model and load a dataset of customer reviews with sentiment labels.\n2. Generate code to fine-tune the model on the review dataset, including tokenization, padding, and attention mechanisms.\n\n## Best Practices\n\n- **Data Preprocessing**: Ensure data is properly preprocessed and formatted to match the input requirements of the pre-trained model.\n- **Hyperparameter Tuning**: Experiment with different hyperparameters (e.g., learning rate, batch size) to optimize model performance.\n- **Regularization**: Apply regularization techniques (e.g., dropout, weight decay) to prevent overfitting.\n\n## Integration\n\nThis skill can be integrated with other plugins for data loading, model evaluation, and deployment. For example, it can work with a data loading plugin to fetch datasets and a model deployment plugin to deploy the adapted model to a serving infrastructure.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "transfer-learning-adapter",
        "category": "ai-ml",
        "path": "plugins/ai-ml/transfer-learning-adapter",
        "version": "1.0.0",
        "description": "Transfer learning adaptation"
      },
      "filePath": "plugins/ai-ml/transfer-learning-adapter/skills/adapting-transfer-learning-models/SKILL.md"
    },
    {
      "slug": "adk-agent-builder",
      "name": "adk-agent-builder",
      "description": "Build production-ready AI agents using Google's Agent Development Kit with AI assistant integration, React patterns, multi-agent orchestration, and comprehensive tool libraries. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# ADK Agent Builder\n\nBuild production-ready agents with Google’s Agent Development Kit (ADK): scaffolding, tool wiring, orchestration patterns, testing, and optional deployment to Vertex AI Agent Engine.\n\n## Overview\n\n- Creates a minimal, production-oriented ADK scaffold (agent entrypoint, tool registry, config, and tests).\n- Supports single-agent ReAct-style workflows and multi-agent orchestration (Sequential/Parallel/Loop).\n- Produces a validation checklist suitable for CI (lint/tests/smoke prompts) and optional Agent Engine deployment verification.\n\n## Prerequisites\n\n- Python runtime compatible with your project (often Python 3.10+)\n- `google-adk` installed and importable\n- If deploying: access to a Google Cloud project with Vertex AI enabled and permissions to deploy Agent Engine runtimes\n- Secrets available via environment variables or a secret manager (never hardcoded)\n\n## Instructions\n\n1. Confirm scope: local-only agent scaffold vs Vertex AI Agent Engine deployment.\n2. Choose an architecture:\n   - Single agent (ReAct) for adaptive tool-driven tasks\n   - Multi-agent system (specialists + orchestrator) for complex, multi-step workflows\n3. Define the tool surface (built-in ADK tools + any custom tools you need) and required credentials.\n4. Scaffold the project:\n   - `src/agents/`, `src/tools/`, `tests/`, and a dependency file (`pyproject.toml` or `requirements.txt`)\n5. Implement the minimum viable agent and a smoke test prompt; add regression tests for tool failures.\n6. If deploying, produce an `adk deploy ...` command and a post-deploy validation checklist (AgentCard/task endpoints, permissions, logs).\n\n## Output\n\n- A repo-ready ADK scaffold (files and directories) plus starter agent code\n- Tool stubs and wiring points (where to add new tools safely)\n- A test + validation plan (unit tests and a minimal smoke prompt)\n- Optional: deployment commands and verification steps for Agent Engine\n\n## Error Handling\n\n- Dependency/runtime issues: provide pinned install commands and validate imports.\n- Auth/permission failures: identify the missing role/API and propose least-privilege fixes.\n- Tool failures/rate limits: add retries/backoff guidance and a regression test to prevent recurrence.\n\n## Examples\n\n**Example: Scaffold a single ReAct agent**\n- Request: “Create an ADK agent that summarizes PRs and proposes test updates.”\n- Result: agent entrypoint + tool registry + a smoke test command for local verification.\n\n**Example: Multi-agent orchestrator**\n- Request: “Build a supervisor + deployer + verifier team and deploy to Agent Engine.”\n- Result: orchestrator skeleton, per-agent responsibilities, and `adk deploy ...` + post-deploy health checks.\n\n## Resources\n\n- Full detailed guide (kept for reference): `{baseDir}/references/SKILL.full.md`\n- Repo standards (source of truth):\n  - `000-docs/6767-a-SPEC-DR-STND-claude-code-plugins-standard.md`\n  - `000-docs/6767-b-SPEC-DR-STND-claude-skills-standard.md`\n- ADK / Agent Engine docs: https://cloud.google.com/vertex-ai/docs/agent-engine",
      "parentPlugin": {
        "name": "jeremy-google-adk",
        "category": "jeremy-google-adk",
        "path": "plugins/jeremy-google-adk",
        "version": "1.0.0",
        "description": "Google Agent Development Kit (ADK) SDK starter kit for building Claude-powered AI agents with React patterns, multi-agent orchestration, and tool integration"
      },
      "filePath": "plugins/jeremy-google-adk/skills/adk-agent-builder/SKILL.md"
    },
    {
      "slug": "adk-deployment-specialist",
      "name": "adk-deployment-specialist",
      "description": "Deploy and orchestrate Vertex AI ADK agents using A2A protocol. Manages AgentCard discovery, task submission, Code Execution Sandbox, and Memory Bank. Use when asked to \"deploy ADK agent\" or \"orchestrate agents\". Trigger with phrases like 'deploy', 'infrastructure', or 'CI/CD'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Adk Deployment Specialist\n\n## Overview\n\nExpert in building and deploying production multi-agent systems using Google's Agent Development Kit (ADK). Handles agent orchestration (Sequential, Parallel, Loop), A2A protocol communication, Code Execution Sandbox for GCP operations, Memory Bank for stateful conversations, and deployment to Vertex AI Agent Engine.\n\n## Prerequisites\n\n- A Google Cloud project with Vertex AI enabled (and permissions to deploy Agent Engine runtimes)\n- ADK installed (and pinned to the project’s supported version)\n- A clear agent contract: tools required, orchestration pattern, and deployment target (local vs Agent Engine)\n- A plan for secrets/credentials (OIDC/WIF where possible; never commit long-lived keys)\n\n## Instructions\n\n1. Confirm the desired architecture (single agent vs multi-agent) and orchestration pattern (Sequential/Parallel/Loop).\n2. Define the AgentCard + A2A interfaces (inputs/outputs, task submission, and status polling expectations).\n3. Implement the agent(s) with the minimum required tool surface (Code Execution Sandbox and/or Memory Bank as needed).\n4. Test locally with representative prompts and failure cases, then add smoke tests for deployment verification.\n5. Deploy to Vertex AI Agent Engine and validate the generated endpoints (`/.well-known/agent-card`, task send/status APIs).\n6. Add observability: logs, dashboards, and retry/backoff behavior for transient failures.\n\n## Output\n\n- Agent source files (or patches) ready for deployment\n- Deployment commands/config (e.g., `adk deploy` invocation + required flags)\n- A verification checklist for Agent Engine endpoints (AgentCard + task APIs) and security posture\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- ADK docs: https://cloud.google.com/vertex-ai/docs/agent-engine\n- Workload Identity (CI/CD): https://cloud.google.com/iam/docs/workload-identity-federation\n- A2A / AgentCard patterns: see `000-docs/6767-a-SPEC-DR-STND-claude-code-plugins-standard.md`",
      "parentPlugin": {
        "name": "jeremy-adk-orchestrator",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-adk-orchestrator",
        "version": "1.0.0",
        "description": "Production ADK orchestrator for A2A protocol and multi-agent coordination on Vertex AI"
      },
      "filePath": "plugins/ai-ml/jeremy-adk-orchestrator/skills/adk-deployment-specialist/SKILL.md"
    },
    {
      "slug": "adk-engineer",
      "name": "adk-engineer",
      "description": "Execute software engineer specializing in creating production-ready ADK agents with best practices, code structure, testing, and deployment automation. Use when asked to \"build ADK agent\", \"create agent code\", or \"engineer ADK application\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# ADK Engineer\n\nEngineer production-ready Agent Development Kit (ADK) agents and multi-agent systems: clean structure, testability, safe tool usage, and deployment automation.\n\n## Overview\n\nUse this skill to design and implement ADK agent code that is maintainable and shippable: clear module boundaries, structured tool interfaces, regression tests, and a deployment checklist (local or Agent Engine).\n\n## Prerequisites\n\n- A target runtime (Python/Java/Go) consistent with the project’s pinned versions\n- ADK installed (and any required model/provider SDKs configured)\n- A test runner available in the repo (unit tests at minimum)\n- If deploying: access to a Google Cloud project and permissions for the chosen deployment target\n\n## Instructions\n\n1. Clarify requirements: agent goals, tool surface, latency/cost constraints, and deployment target.\n2. Propose architecture: single agent vs multi-agent, orchestration pattern, state strategy (Memory Bank / external store).\n3. Scaffold structure: agent entrypoint(s), tool modules, config, and tests.\n4. Implement incrementally:\n   - add one tool at a time with input validation and structured outputs\n   - add regression tests for each tool and critical prompt flows\n5. Add operational guardrails: retries/backoff, timeouts, logging, and safe error messages.\n6. Validate locally (tests + smoke prompts) and provide a deployment plan (when requested).\n\n## Output\n\n- A concrete architecture plan and file layout\n- Agent and tool implementations (or patches) with tests\n- A validation checklist (commands to run, expected outputs, and failure triage)\n- Optional: deployment instructions and post-deploy health checks\n\n## Error Handling\n\n- Build/test failures: isolate the failing module, minimize the repro, fix, and add a regression test.\n- Tool/runtime errors: enforce structured error responses and safe retries where appropriate.\n- Deployment failures: provide the exact failing command, logs to inspect, and least-privilege IAM fixes.\n\n## Examples\n\n**Example: Productionizing an existing ADK agent**\n- Request: “Refactor this agent into a clean module structure and add tests before we deploy.”\n- Result: reorganized `src/` layout, tool boundaries, a test suite, and a deployment checklist.\n\n**Example: Multi-agent workflow**\n- Request: “Build a validator + deployer + monitor agent team with a sequential orchestrator.”\n- Result: orchestrator skeleton, per-agent responsibilities, and smoke tests for each step.\n\n## Resources\n\n- Full detailed playbook (kept for reference): `{baseDir}/references/SKILL.full.md`\n- Repo standards (source of truth):\n  - `000-docs/6767-a-SPEC-DR-STND-claude-code-plugins-standard.md`\n  - `000-docs/6767-b-SPEC-DR-STND-claude-skills-standard.md`\n- ADK / Agent Engine docs: https://cloud.google.com/vertex-ai/docs/agent-engine",
      "parentPlugin": {
        "name": "jeremy-adk-software-engineer",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-adk-software-engineer",
        "version": "1.0.0",
        "description": "ADK software engineer for creating production-ready agents (placeholder - to be implemented)"
      },
      "filePath": "plugins/ai-ml/jeremy-adk-software-engineer/skills/adk-engineer/SKILL.md"
    },
    {
      "slug": "adk-infra-expert",
      "name": "adk-infra-expert",
      "description": "Execute use when provisioning Vertex AI ADK infrastructure with Terraform. Trigger with phrases like \"deploy ADK terraform\", \"agent engine infrastructure\", \"provision ADK agent\", \"vertex AI agent terraform\", or \"code execution sandbox terraform\". Provisions Agent Engine runtime, 14-day code execution sandbox, Memory Bank, VPC Service Controls, IAM roles, and secure multi-agent infrastructure. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(terraform:*), Bash(gcloud:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Adk Infra Expert\n\n## Overview\n\nProvision production-grade Vertex AI ADK infrastructure with Terraform: secure networking, least-privilege IAM, Agent Engine runtime, Code Execution sandbox defaults, and Memory Bank configuration. Use this skill to generate/validate Terraform modules and a deployment checklist that matches enterprise security constraints (including VPC Service Controls when required).\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Google Cloud project with billing enabled\n- Terraform 1.0+ installed\n- gcloud CLI authenticated with appropriate permissions\n- Vertex AI API enabled in target project\n- VPC Service Controls access policy created (for enterprise)\n- Understanding of Agent Engine architecture and requirements\n\n## Instructions\n\n1. **Initialize Terraform**: Set up backend for remote state storage\n2. **Configure Variables**: Define project_id, region, agent configuration\n3. **Provision VPC**: Create network infrastructure with Private Service Connect\n4. **Set Up IAM**: Create service accounts with least privilege roles\n5. **Deploy Agent Engine**: Configure runtime with code execution and memory bank\n6. **Enable VPC-SC**: Apply service perimeter for data exfiltration protection\n7. **Configure Monitoring**: Set up Cloud Monitoring dashboards and alerts\n8. **Validate Deployment**: Test agent endpoint and verify all components\n\n## Output\n\n\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Agent Engine: https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview\n- VPC-SC: https://cloud.google.com/vpc-service-controls/docs\n- Terraform Google Provider: https://registry.terraform.io/providers/hashicorp/google/latest\n- ADK Terraform examples in {baseDir}/examples/",
      "parentPlugin": {
        "name": "jeremy-adk-terraform",
        "category": "devops",
        "path": "plugins/devops/jeremy-adk-terraform",
        "version": "1.0.0",
        "description": "Terraform infrastructure as code for ADK and Vertex AI Agent Engine deployments"
      },
      "filePath": "plugins/devops/jeremy-adk-terraform/skills/adk-infra-expert/SKILL.md"
    },
    {
      "slug": "agent-context-loader",
      "name": "agent-context-loader",
      "description": "Execute proactive auto-loading: automatically detects and loads agents.md files. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(general:*), Bash(util:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore",
      "license": "MIT",
      "content": "# Agent Context Loader\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Access to project files in {baseDir}/\n- Required tools and dependencies installed\n- Understanding of skill functionality\n- Permissions for file operations\n\n## Instructions\n\n1. Identify skill activation trigger and context\n2. Gather required inputs and parameters\n3. Execute skill workflow systematically\n4. Validate outputs meet requirements\n5. Handle errors and edge cases appropriately\n6. Provide clear results and next steps\n\n## Output\n\n- Primary deliverables based on skill purpose\n- Status indicators and success metrics\n- Generated files or configurations\n- Reports and summaries as applicable\n- Recommendations for follow-up actions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Official documentation for related tools\n- Best practices guides\n- Example use cases and templates\n- Community forums and support channels",
      "parentPlugin": {
        "name": "agent-context-manager",
        "category": "productivity",
        "path": "plugins/productivity/agent-context-manager",
        "version": "1.0.0",
        "description": "Automatically detects and loads AGENTS.md files to provide agent-specific instructions alongside CLAUDE.md. Enables specialized agent behaviors without manual intervention."
      },
      "filePath": "plugins/productivity/agent-context-manager/skills/agent-context-loader/SKILL.md"
    },
    {
      "slug": "agent-patterns",
      "name": "agent-patterns",
      "description": "Execute this skill should be used when the user asks about \"SPAWN REQUEST format\", \"agent reports\", \"agent coordination\", \"parallel agents\", \"report format\", \"agent communication\", or needs to understand how agents coordinate within the sprint system. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read version: 1.0.0 author: Damien Laine <damien.laine@gmail.com> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Damien Laine <damien.laine@gmail.com>",
      "license": "MIT",
      "content": "# Agent Patterns\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "sprint",
        "category": "community",
        "path": "plugins/community/sprint",
        "version": "1.0.0",
        "description": "Autonomous multi-agent development framework with spec-driven sprints and convergent iteration"
      },
      "filePath": "plugins/community/sprint/skills/agent-patterns/SKILL.md"
    },
    {
      "slug": "aggregating-crypto-news",
      "name": "aggregating-crypto-news",
      "description": "Execute aggregate breaking crypto news, announcements, and market-moving events in real-time. Use when staying updated on crypto market events. Trigger with phrases like \"get crypto news\", \"check latest announcements\", or \"scan for updates\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:news-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Aggregating Crypto News\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:news-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "crypto-news-aggregator",
        "category": "crypto",
        "path": "plugins/crypto/crypto-news-aggregator",
        "version": "1.0.0",
        "description": "Aggregate and analyze crypto news from multiple sources with sentiment analysis"
      },
      "filePath": "plugins/crypto/crypto-news-aggregator/skills/aggregating-crypto-news/SKILL.md"
    },
    {
      "slug": "aggregating-performance-metrics",
      "name": "aggregating-performance-metrics",
      "description": "Aggregate and centralize performance metrics from applications, systems, databases, caches, and services. Use when consolidating monitoring data from multiple sources. Trigger with phrases like \"aggregate metrics\", \"centralize monitoring\", or \"collect performance data\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Bash(prometheus:*)",
        "Bash(metrics:*)",
        "Bash(monitoring:*)",
        "Grep\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Metrics Aggregator\n\nThis skill provides automated assistance for metrics aggregator tasks.\n\n## Overview\n\nThis skill empowers Claude to streamline performance monitoring by aggregating metrics from diverse systems into a unified view. It simplifies the process of collecting, centralizing, and analyzing performance data, leading to improved insights and faster issue resolution.\n\n## How It Works\n\n1. **Metrics Taxonomy Design**: Claude assists in defining a clear and consistent naming convention for metrics across all systems.\n2. **Aggregation Tool Selection**: Claude helps select the appropriate metrics aggregation tool (e.g., Prometheus, StatsD, CloudWatch) based on the user's environment and requirements.\n3. **Configuration and Integration**: Claude guides the configuration of the chosen aggregation tool and its integration with various data sources.\n4. **Dashboard and Alert Setup**: Claude helps set up dashboards for visualizing metrics and defining alerts for critical performance indicators.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Centralize performance metrics from multiple applications and systems.\n- Design a consistent metrics naming convention.\n- Choose the right metrics aggregation tool for your needs.\n- Set up dashboards and alerts for performance monitoring.\n\n## Examples\n\n### Example 1: Centralizing Application and System Metrics\n\nUser request: \"Aggregate application and system metrics into Prometheus.\"\n\nThe skill will:\n1. Guide the user in defining metrics for applications (e.g., request latency, error rates) and systems (e.g., CPU usage, memory utilization).\n2. Help configure Prometheus to scrape metrics from the application and system endpoints.\n\n### Example 2: Setting Up Alerts for Database Performance\n\nUser request: \"Centralize database metrics and set up alerts for slow queries.\"\n\nThe skill will:\n1. Help the user define metrics for database performance (e.g., query execution time, connection pool usage).\n2. Guide the user in configuring the aggregation tool to collect these metrics from the database.\n3. Assist in setting up alerts in the aggregation tool to notify the user when query execution time exceeds a defined threshold.\n\n## Best Practices\n\n- **Naming Conventions**: Use a consistent and well-defined naming convention for all metrics to ensure clarity and ease of analysis.\n- **Granularity**: Choose an appropriate level of granularity for metrics to balance detail and storage requirements.\n- **Retention Policies**: Define retention policies for metrics to manage storage space and ensure data is available for historical analysis.\n\n## Integration\n\nThis skill integrates with other plugins that manage infrastructure, deploy applications, and monitor system health. For example, it can be used in conjunction with a deployment plugin to automatically configure metrics collection after a new application deployment.\n\n## Prerequisites\n\n- Access to metrics collection tools (Prometheus, StatsD, CloudWatch)\n- Network connectivity to metric sources\n- Metrics storage configuration in {baseDir}/metrics/\n- Understanding of metrics taxonomy\n\n## Instructions\n\n1. Design consistent metrics naming convention\n2. Select appropriate aggregation tool for environment\n3. Configure metric collection from all sources\n4. Set up centralized storage and retention policies\n5. Create dashboards for visualization\n6. Define alerts for critical metrics\n\n## Output\n\n- Metrics aggregation configuration files\n- Unified naming convention documentation\n- Dashboard definitions for key metrics\n- Alert rules for performance thresholds\n- Integration guides for metric sources\n\n## Error Handling\n\nIf metrics aggregation fails:\n- Verify network connectivity to sources\n- Check authentication credentials\n- Validate metrics format compatibility\n- Review storage capacity and retention\n- Ensure aggregation tool configuration\n\n## Resources\n\n- Prometheus aggregation documentation\n- StatsD protocol specifications\n- CloudWatch metrics API reference\n- Metrics naming best practices",
      "parentPlugin": {
        "name": "metrics-aggregator",
        "category": "performance",
        "path": "plugins/performance/metrics-aggregator",
        "version": "1.0.0",
        "description": "Aggregate and centralize performance metrics"
      },
      "filePath": "plugins/performance/metrics-aggregator/skills/aggregating-performance-metrics/SKILL.md"
    },
    {
      "slug": "analyzing-capacity-planning",
      "name": "analyzing-capacity-planning",
      "description": "Execute this skill enables AI assistant to analyze capacity requirements and plan for future growth. it uses the capacity-planning-analyzer plugin to assess current utilization, forecast growth trends, and recommend scaling strategies. use this skill when the u... Use when analyzing code or data. Trigger with phrases like 'analyze', 'review', or 'examine'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Capacity Planning Analyzer\n\nThis skill provides automated assistance for capacity planning analyzer tasks.\n\n## Overview\n\nThis skill empowers Claude to analyze current resource utilization, predict future capacity needs, and provide actionable recommendations for scaling infrastructure. It generates insights into growth trends, identifies potential bottlenecks, and estimates costs associated with capacity expansion.\n\n## How It Works\n\n1. **Analyze Utilization**: The plugin analyzes current CPU, memory, database storage, network bandwidth, and request rate utilization.\n2. **Forecast Growth**: Based on historical data, the plugin forecasts future growth trends for key capacity metrics.\n3. **Generate Recommendations**: The plugin recommends scaling strategies, including vertical and horizontal scaling options, and estimates associated costs.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Analyze current infrastructure capacity and identify potential bottlenecks.\n- Forecast future resource requirements based on projected growth.\n- Develop a capacity roadmap to ensure optimal performance and availability.\n\n## Examples\n\n### Example 1: Planning for Database Growth\n\nUser request: \"Analyze database capacity and plan for future growth.\"\n\nThe skill will:\n1. Analyze current database storage utilization and growth rate.\n2. Forecast future storage requirements based on historical trends.\n3. Recommend scaling options, such as adding storage or migrating to a larger instance.\n\n### Example 2: Identifying CPU Bottlenecks\n\nUser request: \"Analyze CPU utilization and identify potential bottlenecks.\"\n\nThe skill will:\n1. Analyze CPU utilization trends across different servers and applications.\n2. Identify periods of high CPU usage and potential bottlenecks.\n3. Recommend scaling options, such as adding more CPU cores or optimizing application code.\n\n## Best Practices\n\n- **Data Accuracy**: Ensure that the data used for analysis is accurate and up-to-date.\n- **Metric Selection**: Choose the right capacity metrics to monitor based on your specific application requirements.\n- **Regular Monitoring**: Regularly monitor capacity metrics to identify potential issues before they impact performance.\n\n## Integration\n\nThis skill can be integrated with other monitoring and alerting tools to provide proactive capacity management. It can also be used in conjunction with infrastructure-as-code tools to automate scaling operations.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "capacity-planning-analyzer",
        "category": "performance",
        "path": "plugins/performance/capacity-planning-analyzer",
        "version": "1.0.0",
        "description": "Analyze and plan for capacity requirements"
      },
      "filePath": "plugins/performance/capacity-planning-analyzer/skills/analyzing-capacity-planning/SKILL.md"
    },
    {
      "slug": "analyzing-database-indexes",
      "name": "analyzing-database-indexes",
      "description": "Process use when you need to work with database indexing. This skill provides index design and optimization with comprehensive guidance and automation. Trigger with phrases like \"create indexes\", \"optimize indexes\", or \"improve query performance\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Index Advisor\n\nThis skill provides automated assistance for database index advisor tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-index-advisor/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-index-advisor/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-index-advisor/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-index-advisor-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-index-advisor-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-index-advisor-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-index-advisor",
        "category": "database",
        "path": "plugins/database/database-index-advisor",
        "version": "1.0.0",
        "description": "Analyze query patterns and recommend optimal database indexes with impact analysis"
      },
      "filePath": "plugins/database/database-index-advisor/skills/analyzing-database-indexes/SKILL.md"
    },
    {
      "slug": "analyzing-dependencies",
      "name": "analyzing-dependencies",
      "description": "Analyze dependencies for known security vulnerabilities and outdated versions. Use when auditing third-party libraries. Trigger with 'check dependencies', 'scan for vulnerabilities', or 'audit packages'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Dependency Checker\n\nThis skill provides automated assistance for dependency checker tasks.\n\n## Overview\n\nThis skill empowers Claude to automatically analyze your project's dependencies for security vulnerabilities, outdated packages, and license compliance issues. It uses the dependency-checker plugin to identify potential risks and provides insights for remediation.\n\n## How It Works\n\n1. **Detecting Package Manager**: The skill identifies the relevant package manager (npm, pip, composer, gem, go modules) based on the presence of manifest files (e.g., package.json, requirements.txt, composer.json).\n2. **Scanning Dependencies**: The skill utilizes the dependency-checker plugin to scan the identified dependencies against known vulnerability databases (CVEs), outdated package lists, and license information.\n3. **Generating Report**: The skill presents a comprehensive report summarizing the findings, including vulnerability summaries, detailed vulnerability information, outdated packages with recommended updates, and license compliance issues.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Check a project for known security vulnerabilities in its dependencies.\n- Identify outdated packages that may contain security flaws or performance issues.\n- Ensure that the project's dependencies comply with licensing requirements.\n\n## Examples\n\n### Example 1: Identifying Vulnerabilities Before Deployment\n\nUser request: \"Check dependencies for vulnerabilities before deploying to production.\"\n\nThe skill will:\n1. Detect the relevant package manager (e.g., npm).\n2. Scan the project's dependencies for known vulnerabilities using the dependency-checker plugin.\n3. Generate a report highlighting any identified vulnerabilities, their severity, and recommended fixes.\n\n### Example 2: Updating Outdated Packages\n\nUser request: \"Scan for outdated packages and suggest updates.\"\n\nThe skill will:\n1. Detect the relevant package manager (e.g., pip).\n2. Scan the project's dependencies for outdated packages.\n3. Generate a report listing the outdated packages and their available updates, including major, minor, and patch releases.\n\n## Best Practices\n\n- **Regular Scanning**: Schedule dependency checks regularly (e.g., weekly or monthly) to stay informed about new vulnerabilities and updates.\n- **Pre-Deployment Checks**: Always run a dependency check before deploying any code to production to prevent introducing vulnerable dependencies.\n- **Review and Remediation**: Carefully review the generated reports and take appropriate action to remediate identified vulnerabilities and update outdated packages.\n\n## Integration\n\nThis skill seamlessly integrates with other Claude Code tools, allowing you to use the identified vulnerabilities to guide further actions, such as automatically creating pull requests to update dependencies or generating security reports for compliance purposes.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "dependency-checker",
        "category": "security",
        "path": "plugins/security/dependency-checker",
        "version": "1.0.0",
        "description": "Check dependencies for known vulnerabilities, outdated packages, and license compliance"
      },
      "filePath": "plugins/security/dependency-checker/skills/analyzing-dependencies/SKILL.md"
    },
    {
      "slug": "analyzing-liquidity-pools",
      "name": "analyzing-liquidity-pools",
      "description": "Analyze liquidity pool metrics including TVL, volume, fees, and impermanent loss. Use when analyzing DEX liquidity pools. Trigger with phrases like \"analyze pool\", \"check TVL\", or \"calculate impermanent loss\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:liquidity-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Analyzing Liquidity Pools\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:liquidity-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "liquidity-pool-analyzer",
        "category": "crypto",
        "path": "plugins/crypto/liquidity-pool-analyzer",
        "version": "1.0.0",
        "description": "Analyze DeFi liquidity pools for impermanent loss, APY, and optimization opportunities"
      },
      "filePath": "plugins/crypto/liquidity-pool-analyzer/skills/analyzing-liquidity-pools/SKILL.md"
    },
    {
      "slug": "analyzing-logs",
      "name": "analyzing-logs",
      "description": "Analyze application logs for performance insights and issue detection including slow requests, error patterns, and resource usage. Use when troubleshooting performance issues or debugging errors. Trigger with phrases like \"analyze logs\", \"find slow requests\", or \"detect error patterns\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Bash(logs:*)",
        "Bash(grep:*)",
        "Bash(awk:*)",
        "Grep\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Log Analysis Tool\n\nThis skill provides automated assistance for log analysis tool tasks.\n\n## Overview\n\nThis skill empowers Claude to automatically analyze application logs, pinpoint performance bottlenecks, and identify recurring errors. It streamlines the debugging process and helps optimize application performance by extracting key insights from log data.\n\n## How It Works\n\n1. **Initiate Analysis**: Claude activates the log analysis tool upon detecting relevant trigger phrases.\n2. **Log Data Extraction**: The tool extracts relevant data, including timestamps, request durations, error messages, and resource usage metrics.\n3. **Pattern Identification**: The tool identifies patterns such as slow requests, frequent errors, and resource exhaustion warnings.\n4. **Report Generation**: Claude presents a summary of findings, highlighting potential performance issues and optimization opportunities.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify performance bottlenecks in an application.\n- Debug recurring errors and exceptions.\n- Analyze log data for trends and anomalies.\n- Set up structured logging or log aggregation.\n\n## Examples\n\n### Example 1: Identifying Slow Requests\n\nUser request: \"Analyze logs for slow requests.\"\n\nThe skill will:\n1. Activate the log analysis tool.\n2. Identify requests exceeding predefined latency thresholds.\n3. Present a list of slow requests with corresponding timestamps and durations.\n\n### Example 2: Detecting Error Patterns\n\nUser request: \"Find error patterns in the application logs.\"\n\nThe skill will:\n1. Activate the log analysis tool.\n2. Scan logs for recurring error messages and exceptions.\n3. Group similar errors and present a summary of error frequencies.\n\n## Best Practices\n\n- **Log Level**: Ensure appropriate log levels (e.g., INFO, WARN, ERROR) are used to capture relevant information.\n- **Structured Logging**: Implement structured logging (e.g., JSON format) to facilitate efficient analysis.\n- **Log Rotation**: Configure log rotation policies to prevent log files from growing excessively.\n\n## Integration\n\nThis skill can be integrated with other tools for monitoring and alerting. For example, it can be used in conjunction with a monitoring plugin to automatically trigger alerts based on log analysis results. It can also work with deployment tools to rollback deployments when critical errors are detected in the logs.\n\n## Prerequisites\n\n- Access to application log files in {baseDir}/logs/\n- Log parsing tools (grep, awk, sed)\n- Understanding of application log format and structure\n- Read permissions for log directories\n\n## Instructions\n\n1. Identify log files to analyze based on timeframe and application\n2. Extract relevant data (timestamps, durations, error messages)\n3. Apply pattern matching to identify slow requests and errors\n4. Aggregate and group similar issues\n5. Generate analysis report with findings and recommendations\n6. Suggest optimization opportunities based on patterns\n\n## Output\n\n- Summary of slow requests with response times\n- Error frequency reports grouped by type\n- Resource usage patterns and anomalies\n- Performance bottleneck identification\n- Recommendations for log improvements and optimizations\n\n## Error Handling\n\nIf log analysis fails:\n- Verify log file paths and permissions\n- Check log format compatibility\n- Validate timestamp parsing\n- Ensure sufficient disk space for analysis\n- Review log rotation configuration\n\n## Resources\n\n- Application logging best practices\n- Structured logging format guides\n- Log aggregation tools documentation\n- Performance analysis methodologies",
      "parentPlugin": {
        "name": "log-analysis-tool",
        "category": "performance",
        "path": "plugins/performance/log-analysis-tool",
        "version": "1.0.0",
        "description": "Analyze logs for performance insights and issues"
      },
      "filePath": "plugins/performance/log-analysis-tool/skills/analyzing-logs/SKILL.md"
    },
    {
      "slug": "analyzing-market-sentiment",
      "name": "analyzing-market-sentiment",
      "description": "Analyze crypto market sentiment from social media, news, and on-chain metrics. Use when gauging market sentiment and social trends. Trigger with phrases like \"analyze sentiment\", \"check market mood\", or \"gauge social trends\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:sentiment-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Analyzing Market Sentiment\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:sentiment-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "market-sentiment-analyzer",
        "category": "crypto",
        "path": "plugins/crypto/market-sentiment-analyzer",
        "version": "1.0.0",
        "description": "Analyze market sentiment from social media, news, and on-chain data"
      },
      "filePath": "plugins/crypto/market-sentiment-analyzer/skills/analyzing-market-sentiment/SKILL.md"
    },
    {
      "slug": "analyzing-mempool",
      "name": "analyzing-mempool",
      "description": "Monitor blockchain mempools for pending transactions, front-running, and MEV opportunities. Use when monitoring pending blockchain transactions. Trigger with phrases like \"check mempool\", \"scan pending txs\", or \"find MEV\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:mempool-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Analyzing Mempool\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:mempool-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "mempool-analyzer",
        "category": "crypto",
        "path": "plugins/crypto/mempool-analyzer",
        "version": "1.0.0",
        "description": "Advanced mempool analysis for MEV opportunities, pending transaction monitoring, and gas price optimization"
      },
      "filePath": "plugins/crypto/mempool-analyzer/skills/analyzing-mempool/SKILL.md"
    },
    {
      "slug": "analyzing-network-latency",
      "name": "analyzing-network-latency",
      "description": "Analyze network latency and optimize request patterns for faster communication. Use when diagnosing slow network performance or optimizing API calls. Trigger with phrases like \"analyze network latency\", \"optimize API calls\", or \"reduce network delays\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Bash(curl:*)",
        "Bash(ping:*)",
        "Bash(traceroute:*)",
        "Grep\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Network Latency Analyzer\n\nThis skill provides automated assistance for network latency analyzer tasks.\n\n## Overview\n\nThis skill empowers Claude to diagnose network latency issues and propose optimizations to improve application performance. It analyzes request patterns, identifies potential bottlenecks, and recommends solutions for faster and more efficient network communication.\n\n## How It Works\n\n1. **Request Pattern Identification**: Claude identifies all network requests made by the application.\n2. **Latency Analysis**: Claude analyzes the latency associated with each request, looking for patterns and anomalies.\n3. **Optimization Recommendations**: Claude suggests optimizations such as parallelization, request batching, connection pooling, and timeout adjustments.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Analyze network latency in an application.\n- Optimize network request patterns for improved performance.\n- Identify bottlenecks in network communication.\n\n## Examples\n\n### Example 1: Optimizing API Calls\n\nUser request: \"Analyze network latency and suggest improvements for our API calls.\"\n\nThe skill will:\n1. Identify all API calls made by the application.\n2. Analyze the latency of each API call.\n3. Suggest parallelizing certain API calls and implementing connection pooling.\n\n### Example 2: Reducing Page Load Time\n\nUser request: \"Optimize network request patterns to reduce page load time.\"\n\nThe skill will:\n1. Identify all network requests made during page load.\n2. Analyze the latency of each request.\n3. Suggest batching multiple requests into a single request and optimizing timeout configurations.\n\n## Best Practices\n\n- **Parallelization**: Identify serial requests that can be executed in parallel to reduce overall latency.\n- **Request Batching**: Batch multiple small requests into a single larger request to reduce overhead.\n- **Connection Pooling**: Reuse existing HTTP connections to avoid the overhead of establishing new connections for each request.\n\n## Integration\n\nThis skill can be used in conjunction with other plugins that manage infrastructure or application code, allowing for automated implementation of the suggested optimizations. For instance, it can work with a code modification plugin to automatically apply connection pooling or adjust timeout values.\n\n## Prerequisites\n\n- Access to application network configuration\n- Network monitoring tools (curl, ping, traceroute)\n- Request pattern documentation\n- Performance baseline metrics\n\n## Instructions\n\n1. Identify all network requests in the application\n2. Measure latency for each request type\n3. Analyze patterns for serial vs parallel execution\n4. Identify opportunities for batching and pooling\n5. Recommend timeout and retry configurations\n6. Provide optimization implementation plan\n\n## Output\n\n- Network latency analysis report\n- Request pattern visualizations\n- Optimization recommendations with priorities\n- Implementation examples for suggested changes\n- Expected performance improvements\n\n## Error Handling\n\nIf latency analysis fails:\n- Verify network connectivity to endpoints\n- Check DNS resolution and routing\n- Validate request authentication\n- Review firewall and security rules\n- Ensure monitoring tools are installed\n\n## Resources\n\n- HTTP connection pooling guides\n- Request batching best practices\n- Network performance optimization references\n- API design patterns for latency reduction",
      "parentPlugin": {
        "name": "network-latency-analyzer",
        "category": "performance",
        "path": "plugins/performance/network-latency-analyzer",
        "version": "1.0.0",
        "description": "Analyze network latency and optimize request patterns"
      },
      "filePath": "plugins/performance/network-latency-analyzer/skills/analyzing-network-latency/SKILL.md"
    },
    {
      "slug": "analyzing-nft-rarity",
      "name": "analyzing-nft-rarity",
      "description": "Execute calculate NFT rarity scores and floor prices across collections and marketplaces. Use when analyzing NFT collections and rarity. Trigger with phrases like \"check NFT rarity\", \"analyze collection\", or \"calculate floor price\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:nft-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Analyzing Nft Rarity\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:nft-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "nft-rarity-analyzer",
        "category": "crypto",
        "path": "plugins/crypto/nft-rarity-analyzer",
        "version": "1.0.0",
        "description": "Analyze NFT rarity scores and valuations across collections"
      },
      "filePath": "plugins/crypto/nft-rarity-analyzer/skills/analyzing-nft-rarity/SKILL.md"
    },
    {
      "slug": "analyzing-on-chain-data",
      "name": "analyzing-on-chain-data",
      "description": "Process perform on-chain analysis including whale tracking, token flows, and network activity. Use when performing crypto analysis. Trigger with phrases like \"analyze crypto\", \"check blockchain\", or \"monitor market\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:onchain-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Analyzing On Chain Data\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:onchain-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "on-chain-analytics",
        "category": "crypto",
        "path": "plugins/crypto/on-chain-analytics",
        "version": "1.0.0",
        "description": "Analyze on-chain metrics including whale movements, network activity, and holder distribution"
      },
      "filePath": "plugins/crypto/on-chain-analytics/skills/analyzing-on-chain-data/SKILL.md"
    },
    {
      "slug": "analyzing-options-flow",
      "name": "analyzing-options-flow",
      "description": "Track crypto options flow to identify institutional positioning and market sentiment. Use when tracking institutional options flow. Trigger with phrases like \"track options flow\", \"analyze derivatives\", or \"check institutional\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:options-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Analyzing Options Flow\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:options-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "options-flow-analyzer",
        "category": "crypto",
        "path": "plugins/crypto/options-flow-analyzer",
        "version": "1.0.0",
        "description": "Track institutional options flow, unusual activity, and smart money movements"
      },
      "filePath": "plugins/crypto/options-flow-analyzer/skills/analyzing-options-flow/SKILL.md"
    },
    {
      "slug": "analyzing-query-performance",
      "name": "analyzing-query-performance",
      "description": "Execute use when you need to work with query optimization. This skill provides query performance analysis with comprehensive guidance and automation. Trigger with phrases like \"optimize queries\", \"analyze performance\", or \"improve query speed\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Query Performance Analyzer\n\nThis skill provides automated assistance for query performance analyzer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/query-performance-analyzer/`\n\n**Documentation and Guides**: `{baseDir}/docs/query-performance-analyzer/`\n\n**Example Scripts and Code**: `{baseDir}/examples/query-performance-analyzer/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/query-performance-analyzer-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/query-performance-analyzer-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/query-performance-analyzer-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "query-performance-analyzer",
        "category": "database",
        "path": "plugins/database/query-performance-analyzer",
        "version": "1.0.0",
        "description": "Analyze query performance with EXPLAIN plan interpretation, bottleneck identification, and optimization recommendations"
      },
      "filePath": "plugins/database/query-performance-analyzer/skills/analyzing-query-performance/SKILL.md"
    },
    {
      "slug": "analyzing-security-headers",
      "name": "analyzing-security-headers",
      "description": "Analyze HTTP security headers of web domains to identify vulnerabilities and misconfigurations. Use when you need to audit website security headers, assess header compliance, or get security recommendations for web applications. Trigger with phrases like \"analyze security headers\", \"check HTTP headers\", \"audit website security headers\", or \"evaluate CSP and HSTS configuration\". allowed-tools: Read, WebFetch, WebSearch, Grep version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Analyzing Security Headers\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Target URL or domain name is accessible\n- Network connectivity for HTTP requests\n- Permission to scan the target domain\n- Optional: Save results to {baseDir}/security-reports/\n\n## Instructions\n\n1. Collect the target URL/domain and environment context (CDN/proxy, redirects).\n2. Fetch response headers (HTTP/HTTPS) and capture redirects/cookies.\n3. Compare headers to recommended baselines and score gaps.\n4. Provide concrete remediation steps and verify fixes.\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: Security headers analysis report\n\n**Report Structure**:\n```\n# Security Headers Analysis - example.com\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- OWASP Secure Headers Project: https://owasp.org/www-project-secure-headers/\n- MDN Security Headers Guide: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers#security\n- Security Headers Scanner: https://securityheaders.com/\n- CSP Reference: https://content-security-policy.com/\n- HSTS Preload: https://hstspreload.org/",
      "parentPlugin": {
        "name": "security-headers-analyzer",
        "category": "security",
        "path": "plugins/security/security-headers-analyzer",
        "version": "1.0.0",
        "description": "Analyze HTTP security headers"
      },
      "filePath": "plugins/security/security-headers-analyzer/skills/analyzing-security-headers/SKILL.md"
    },
    {
      "slug": "analyzing-system-throughput",
      "name": "analyzing-system-throughput",
      "description": "Analyze and optimize system throughput including request handling, data processing, and resource utilization. Use when identifying capacity limits or evaluating scaling strategies. Trigger with phrases like \"analyze throughput\", \"optimize capacity\", or \"identify bottlenecks\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Bash(performance:*)",
        "Bash(monitoring:*)",
        "Grep\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Throughput Analyzer\n\nThis skill provides automated assistance for throughput analyzer tasks.\n\n## Overview\n\nThis skill allows Claude to analyze system performance and identify areas for throughput optimization. It uses the `throughput-analyzer` plugin to provide insights into request handling, data processing, and resource utilization.\n\n## How It Works\n\n1. **Identify Critical Components**: Determines which system components are most relevant to throughput.\n2. **Analyze Throughput Metrics**: Gathers and analyzes current throughput metrics for the identified components.\n3. **Identify Limiting Factors**: Pinpoints the bottlenecks and constraints that are hindering optimal throughput.\n4. **Evaluate Scaling Strategies**: Explores potential scaling strategies and their impact on overall throughput.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Analyze system throughput to identify performance bottlenecks.\n- Optimize system performance for increased capacity.\n- Evaluate scaling strategies to improve throughput.\n\n## Examples\n\n### Example 1: Analyzing Web Server Throughput\n\nUser request: \"Analyze the throughput of my web server and identify any bottlenecks.\"\n\nThe skill will:\n1. Activate the `throughput-analyzer` plugin.\n2. Analyze request throughput, data throughput, and resource saturation of the web server.\n3. Provide a report identifying potential bottlenecks and optimization opportunities.\n\n### Example 2: Optimizing Data Processing Pipeline\n\nUser request: \"Optimize the throughput of my data processing pipeline.\"\n\nThe skill will:\n1. Activate the `throughput-analyzer` plugin.\n2. Analyze data throughput, queue processing, and concurrency limits of the data processing pipeline.\n3. Suggest improvements to increase data processing rates and overall throughput.\n\n## Best Practices\n\n- **Component Selection**: Focus the analysis on the most throughput-critical components to avoid unnecessary overhead.\n- **Metric Interpretation**: Carefully interpret throughput metrics to accurately identify limiting factors.\n- **Scaling Evaluation**: Thoroughly evaluate the potential impact of scaling strategies before implementation.\n\n## Integration\n\nThis skill can be used in conjunction with other monitoring and performance analysis tools to gain a more comprehensive understanding of system behavior. It provides a starting point for further investigation and optimization efforts.\n\n## Prerequisites\n\n- Access to throughput metrics in {baseDir}/metrics/throughput/\n- System performance monitoring tools\n- Historical throughput baselines\n- Current capacity and scaling limits\n\n## Instructions\n\n1. Identify critical system components for throughput analysis\n2. Collect request and data throughput metrics\n3. Analyze resource saturation and queue depths\n4. Identify bottlenecks and limiting factors\n5. Evaluate horizontal and vertical scaling strategies\n6. Generate capacity planning recommendations\n\n## Output\n\n- Throughput analysis reports with current capacity\n- Bottleneck identification and root cause analysis\n- Resource saturation metrics\n- Scaling strategy recommendations\n- Capacity planning projections\n\n## Error Handling\n\nIf throughput analysis fails:\n- Verify metrics collection infrastructure\n- Check system monitoring tool access\n- Validate historical baseline data\n- Ensure performance testing environment\n- Review component identification logic\n\n## Resources\n\n- Throughput optimization best practices\n- Capacity planning methodologies\n- Scaling strategy comparison guides\n- Performance bottleneck detection techniques",
      "parentPlugin": {
        "name": "throughput-analyzer",
        "category": "performance",
        "path": "plugins/performance/throughput-analyzer",
        "version": "1.0.0",
        "description": "Analyze and optimize system throughput"
      },
      "filePath": "plugins/performance/throughput-analyzer/skills/analyzing-system-throughput/SKILL.md"
    },
    {
      "slug": "analyzing-test-coverage",
      "name": "analyzing-test-coverage",
      "description": "Analyze code coverage metrics and identify untested code paths. Use when analyzing untested code or coverage gaps. Trigger with phrases like \"analyze coverage\", \"check test coverage\", or \"find untested code\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:coverage-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Test Coverage Analyzer\n\nThis skill provides automated assistance for test coverage analyzer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:coverage-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for test coverage analyzer tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "test-coverage-analyzer",
        "category": "testing",
        "path": "plugins/testing/test-coverage-analyzer",
        "version": "1.0.0",
        "description": "Analyze code coverage metrics, identify untested code, and generate comprehensive coverage reports"
      },
      "filePath": "plugins/testing/test-coverage-analyzer/skills/analyzing-test-coverage/SKILL.md"
    },
    {
      "slug": "analyzing-text-sentiment",
      "name": "analyzing-text-sentiment",
      "description": "Execute this skill enables AI assistant to analyze the sentiment of text data. it identifies the emotional tone expressed in text, classifying it as positive, negative, or neutral. use this skill when a user requests sentiment analysis, opinion mining, or emoti... Use when analyzing code or data. Trigger with phrases like 'analyze', 'review', or 'examine'. allowed-tools: Read, Write, Bash(cmd:*), Grep version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Sentiment Analysis Tool\n\nThis skill provides automated assistance for sentiment analysis tool tasks.\n\n## Overview\n\nThis skill empowers Claude to perform sentiment analysis on text, providing insights into the emotional content and polarity of the provided data. By leveraging AI/ML techniques, it helps understand public opinion, customer feedback, and overall emotional tone in written communication.\n\n## How It Works\n\n1. **Text Input**: The skill receives text data as input from the user.\n2. **Sentiment Analysis**: The skill processes the text using a pre-trained sentiment analysis model to determine the sentiment polarity (positive, negative, or neutral).\n3. **Result Output**: The skill provides a sentiment score and classification, indicating the overall sentiment expressed in the text.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Determine the overall sentiment of customer reviews.\n- Analyze the emotional tone of social media posts.\n- Gauge public opinion on a particular topic.\n- Identify positive and negative feedback in survey responses.\n\n## Examples\n\n### Example 1: Analyzing Customer Reviews\n\nUser request: \"Analyze the sentiment of these customer reviews: 'The product is amazing!', 'The service was terrible.', 'It was okay.'\"\n\nThe skill will:\n1. Process the provided customer reviews.\n2. Classify each review as positive, negative, or neutral and provide sentiment scores.\n\n### Example 2: Monitoring Social Media Sentiment\n\nUser request: \"Perform sentiment analysis on the following tweet: 'I love this new feature!'\"\n\nThe skill will:\n1. Analyze the provided tweet.\n2. Identify the sentiment as positive and provide a corresponding sentiment score.\n\n## Best Practices\n\n- **Data Quality**: Ensure the input text is clear and free from ambiguous language for accurate sentiment analysis.\n- **Context Awareness**: Consider the context of the text when interpreting sentiment scores, as sarcasm or irony can affect results.\n- **Model Selection**: Use appropriate sentiment analysis models based on the type of text being analyzed (e.g., social media, customer reviews).\n\n## Integration\n\nThis skill can be integrated with other Claude Code plugins to automate workflows, such as summarizing feedback alongside sentiment scores or triggering actions based on sentiment polarity (e.g., escalating negative feedback).\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "sentiment-analysis-tool",
        "category": "ai-ml",
        "path": "plugins/ai-ml/sentiment-analysis-tool",
        "version": "1.0.0",
        "description": "Sentiment analysis on text data"
      },
      "filePath": "plugins/ai-ml/sentiment-analysis-tool/skills/analyzing-text-sentiment/SKILL.md"
    },
    {
      "slug": "analyzing-text-with-nlp",
      "name": "analyzing-text-with-nlp",
      "description": "Execute this skill enables AI assistant to perform natural language processing and text analysis using the nlp-text-analyzer plugin. it should be used when the user requests analysis of text, including sentiment analysis, keyword extraction, topic modeling, or ... Use when analyzing code or data. Trigger with phrases like 'analyze', 'review', or 'examine'. allowed-tools: Read, Bash(cmd:*), Grep, Glob version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Nlp Text Analyzer\n\nThis skill provides automated assistance for nlp text analyzer tasks.\n\n## Overview\n\nThis skill empowers Claude to analyze text using the nlp-text-analyzer plugin, extracting meaningful information and insights. It facilitates tasks such as sentiment analysis, keyword extraction, and topic modeling, enabling a deeper understanding of textual data.\n\n## How It Works\n\n1. **Request Analysis**: Claude receives a user request to analyze text.\n2. **Text Processing**: The nlp-text-analyzer plugin processes the text using NLP techniques.\n3. **Insight Extraction**: The plugin extracts insights such as sentiment, keywords, and topics.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Perform sentiment analysis on a piece of text.\n- Extract keywords from a document.\n- Identify the main topics discussed in a text.\n\n## Examples\n\n### Example 1: Sentiment Analysis\n\nUser request: \"Analyze the sentiment of this product review: 'I loved the product! It exceeded my expectations.'\"\n\nThe skill will:\n1. Process the review text using the nlp-text-analyzer plugin.\n2. Determine the sentiment as positive and provide a confidence score.\n\n### Example 2: Keyword Extraction\n\nUser request: \"Extract the keywords from this news article about the latest AI advancements.\"\n\nThe skill will:\n1. Process the article text using the nlp-text-analyzer plugin.\n2. Identify and return a list of relevant keywords, such as \"AI\", \"advancements\", \"machine learning\", and \"neural networks\".\n\n## Best Practices\n\n- **Clarity**: Be specific in your requests to ensure accurate and relevant analysis.\n- **Context**: Provide sufficient context to improve the quality of the analysis.\n- **Iteration**: Refine your requests based on the initial results to achieve the desired outcome.\n\n## Integration\n\nThis skill can be integrated with other tools to provide a comprehensive workflow, such as using the extracted keywords to perform further research or using sentiment analysis to categorize customer feedback.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "nlp-text-analyzer",
        "category": "ai-ml",
        "path": "plugins/ai-ml/nlp-text-analyzer",
        "version": "1.0.0",
        "description": "Natural language processing and text analysis"
      },
      "filePath": "plugins/ai-ml/nlp-text-analyzer/skills/analyzing-text-with-nlp/SKILL.md"
    },
    {
      "slug": "api-contract",
      "name": "api-contract",
      "description": "Configure this skill should be used when the user asks about \"API contract\", \"api-contract.md\", \"shared interface\", \"TypeScript interfaces\", \"request response schemas\", \"endpoint design\", or needs guidance on designing contracts that coordinate backend and frontend agents. Use when building or modifying API endpoints. Trigger with phrases like 'create API', 'design endpoint', or 'API scaffold'. allowed-tools: Read version: 1.0.0 author: Damien Laine <damien.laine@gmail.com> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Damien Laine <damien.laine@gmail.com>",
      "license": "MIT",
      "content": "# Api Contract\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "sprint",
        "category": "community",
        "path": "plugins/community/sprint",
        "version": "1.0.0",
        "description": "Autonomous multi-agent development framework with spec-driven sprints and convergent iteration"
      },
      "filePath": "plugins/community/sprint/skills/api-contract/SKILL.md"
    },
    {
      "slug": "archiving-databases",
      "name": "archiving-databases",
      "description": "Process use when you need to archive historical database records to reduce primary database size. This skill automates moving old data to archive tables or cold storage (S3, Azure Blob, GCS). Trigger with phrases like \"archive old database records\", \"implement data retention policy\", \"move historical data to cold storage\", or \"reduce database size with archival\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(aws:s3:*), Bash(az:storage:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Archival System\n\nThis skill provides automated assistance for database archival system tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Database credentials with SELECT and DELETE permissions on source tables\n- Access to destination storage (archive table or cloud storage credentials)\n- Network connectivity to cloud storage services if using S3/Azure/GCS\n- Backup of database before first archival run\n- Understanding of data retention requirements and compliance policies\n- Monitoring tools configured to track archival job success\n\n## Instructions\n\n### Step 1: Define Archival Criteria\n1. Identify tables containing historical data for archival\n2. Define age threshold for archival (e.g., records older than 1 year)\n3. Determine additional criteria (status flags, record size, access frequency)\n4. Calculate expected data volume to be archived\n5. Document business requirements and compliance policies\n\n### Step 2: Choose Archival Destination\n1. Evaluate options: archive table in same database, separate archive database, or cold storage\n2. For cloud storage: select S3, Azure Blob, or GCS based on infrastructure\n3. Configure destination storage with appropriate security and access controls\n4. Set up compression settings for storage efficiency\n5. Define data format for archived records (CSV, Parquet, JSON)\n\n### Step 3: Create Archive Schema\n1. Design archive table schema matching source table structure\n2. Add metadata columns (archived_at, source_table, archive_reason)\n3. Create indexes on commonly queried archive columns\n4. For cloud storage: define bucket structure and naming conventions\n5. Test archive schema with sample data\n\n### Step 4: Implement Archival Logic\n1. Write SQL query to identify records meeting archival criteria\n2. Create extraction script to export records from source tables\n3. Implement transformation logic if archive format differs from source\n4. Build verification queries to confirm data integrity after archival\n5. Add transaction handling to ensure atomicity (delete only if archive succeeds)\n\n### Step 5: Execute Archival Process\n1. Run archival in staging environment first with subset of data\n2. Verify archived data integrity and completeness\n3. Execute archival in production during low-traffic window\n4. Monitor database performance during archival operation\n5. Generate archival report with record counts and storage savings\n\n### Step 6: Automate Retention Policy\n1. Schedule periodic archival jobs (weekly, monthly)\n2. Configure automated monitoring and alerting for job failures\n3. Implement cleanup of successfully archived records from source tables\n4. Set up expiration policies on archived data per compliance requirements\n5. Document archival schedule and retention periods\n\n## Output\n\nThis skill produces:\n\n**Archival Scripts**: SQL and shell scripts to extract, transform, and load data to archive destination\n\n**Archive Tables/Files**: Structured storage containing historical records with metadata and timestamps\n\n**Verification Reports**: Row counts, data checksums, and integrity checks confirming successful archival\n\n**Storage Metrics**: Database size reduction, archive storage utilization, and cost savings estimates\n\n**Archival Logs**: Detailed logs of each archival run with timestamps, record counts, and any errors\n\n## Error Handling\n\n**Insufficient Storage Space**:\n- Check available disk space on archive destination before execution\n- Implement storage monitoring and alerting\n- Use compression to reduce archive size\n- Clean up old archives per retention policy before new archival\n\n**Data Integrity Issues**:\n- Run checksums on source data before and after archival\n- Implement row count verification between source and archive\n- Keep source data until archive verification completes\n- Rollback archive transaction if verification fails\n\n**Permission Denied Errors**:\n- Verify database user has SELECT on source tables and INSERT on archive tables\n- Confirm cloud storage credentials have write permissions\n- Check network security groups allow connections to cloud storage\n- Document required permissions for archival automation\n\n**Timeout During Large Archival**:\n- Split archival into smaller batches by date ranges\n- Run archival incrementally over multiple days\n- Increase database timeout settings for archival sessions\n- Schedule archival during maintenance windows with extended timeouts\n\n## Resources\n\n**Archival Configuration Templates**:\n- PostgreSQL archival: `{baseDir}/templates/postgresql-archive-config.yaml`\n- MySQL archival: `{baseDir}/templates/mysql-archive-config.yaml`\n- S3 cold storage: `{baseDir}/templates/s3-archive-config.yaml`\n- Azure Blob storage: `{baseDir}/templates/azure-archive-config.yaml`\n\n**Retention Policy Definitions**: `{baseDir}/policies/retention-policies.yaml`\n\n**Archival Scripts Library**: `{baseDir}/scripts/archival/`\n- Extract to CSV script\n- Extract to Parquet script\n- S3 upload with compression\n- Archive verification queries\n\n**Monitoring Dashboards**: `{baseDir}/monitoring/archival-dashboard.json`\n**Cost Analysis Tools**: `{baseDir}/tools/storage-cost-calculator.py`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-archival-system",
        "category": "database",
        "path": "plugins/database/database-archival-system",
        "version": "1.0.0",
        "description": "Database plugin for database-archival-system"
      },
      "filePath": "plugins/database/database-archival-system/skills/archiving-databases/SKILL.md"
    },
    {
      "slug": "assisting-with-soc2-audit-preparation",
      "name": "assisting-with-soc2-audit-preparation",
      "description": "Execute automate SOC 2 audit preparation including evidence gathering, control assessment, and compliance gap identification. Use when you need to prepare for SOC 2 audits, assess Trust Service Criteria compliance, document security controls, or generate readiness reports. Trigger with phrases like \"SOC 2 audit preparation\", \"SOC 2 readiness assessment\", \"collect SOC 2 evidence\", or \"Trust Service Criteria compliance\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(audit-collect:*), Bash(compliance-check:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Assisting With Soc2 Audit Preparation\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Documentation directory accessible in {baseDir}/docs/\n- Infrastructure-as-code and configuration files available\n- Access to cloud provider logs (AWS CloudTrail, Azure Activity Log, GCP Audit Logs)\n- Security policies and procedures documented\n- Employee training records available\n- Incident response documentation accessible\n- Write permissions for audit reports in {baseDir}/soc2-audit/\n\n## Instructions\n\n1. Confirm scope (services, systems, period) and applicable SOC 2 criteria.\n2. Gather existing controls, policies, and evidence sources.\n3. Identify gaps and draft an evidence collection plan.\n4. Produce an audit-ready checklist and remediation backlog.\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: SOC 2 readiness report saved to {baseDir}/soc2-audit/readiness-report-YYYYMMDD.md\n\n**Report Structure**:\n```\n# SOC 2 Readiness Assessment\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- AICPA Trust Service Criteria: https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/trustdataintegritytaskforce.html\n- SOC 2 Compliance Checklist: https://secureframe.com/hub/soc-2/checklist\n- CIS Controls: https://www.cisecurity.org/controls/\n- NIST Cybersecurity Framework: https://www.nist.gov/cyberframework\n- Drata: SOC 2 compliance automation",
      "parentPlugin": {
        "name": "soc2-audit-helper",
        "category": "security",
        "path": "plugins/security/soc2-audit-helper",
        "version": "1.0.0",
        "description": "Assist with SOC2 audit preparation"
      },
      "filePath": "plugins/security/soc2-audit-helper/skills/assisting-with-soc2-audit-preparation/SKILL.md"
    },
    {
      "slug": "auditing-access-control",
      "name": "auditing-access-control",
      "description": "Audit access control implementations for security vulnerabilities and misconfigurations. Use when reviewing authentication and authorization. Trigger with 'audit access control', 'check permissions', or 'validate authorization'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Access Control Auditor\n\nThis skill provides automated assistance for access control auditor tasks.\n\n## Overview\n\nThis skill leverages the access-control-auditor plugin to perform comprehensive audits of access control configurations. It helps identify potential security risks associated with overly permissive access, misconfigured permissions, and non-compliance with security policies.\n\n## How It Works\n\n1. **Analyze Request**: Claude identifies the user's intent to audit access control.\n2. **Invoke Plugin**: The access-control-auditor plugin is activated.\n3. **Execute Audit**: The plugin analyzes the specified access control configuration (e.g., IAM policies, ACLs).\n4. **Report Findings**: The plugin generates a report highlighting potential vulnerabilities and misconfigurations.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Audit IAM policies in a cloud environment.\n- Review access control lists (ACLs) for network resources.\n- Assess user permissions in an application.\n- Identify potential privilege escalation paths.\n- Ensure compliance with access control security policies.\n\n## Examples\n\n### Example 1: Auditing AWS IAM Policies\n\nUser request: \"Audit the AWS IAM policies in my account for overly permissive access.\"\n\nThe skill will:\n1. Invoke the access-control-auditor plugin, specifying the AWS account and IAM policies as the target.\n2. Generate a report identifying IAM policies that grant overly broad permissions or violate security best practices.\n\n### Example 2: Reviewing Network ACLs\n\nUser request: \"Review the network ACLs for my VPC to identify any potential security vulnerabilities.\"\n\nThe skill will:\n1. Activate the access-control-auditor plugin, specifying the VPC and network ACLs as the target.\n2. Produce a report highlighting ACL rules that allow unauthorized access or expose the VPC to unnecessary risks.\n\n## Best Practices\n\n- **Scope Definition**: Clearly define the scope of the audit (e.g., specific IAM roles, network segments, applications).\n- **Contextual Information**: Provide contextual information about the environment being audited (e.g., security policies, compliance requirements).\n- **Remediation Guidance**: Use the audit findings to develop and implement remediation strategies to address identified vulnerabilities.\n\n## Integration\n\nThis skill can be integrated with other security plugins to provide a more comprehensive security assessment. For example, it can be combined with a vulnerability scanner to identify vulnerabilities that could be exploited due to access control misconfigurations. It can also be integrated with compliance tools to ensure adherence to regulatory requirements.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "access-control-auditor",
        "category": "security",
        "path": "plugins/security/access-control-auditor",
        "version": "1.0.0",
        "description": "Audit access control implementations"
      },
      "filePath": "plugins/security/access-control-auditor/skills/auditing-access-control/SKILL.md"
    },
    {
      "slug": "auditing-wallet-security",
      "name": "auditing-wallet-security",
      "description": "Execute review crypto wallet security including private key management and transaction signing. Use when auditing wallet security practices. Trigger with phrases like \"audit wallet\", \"check security\", or \"verify signatures\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:wallet-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Auditing Wallet Security\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:wallet-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "wallet-security-auditor",
        "category": "crypto",
        "path": "plugins/crypto/wallet-security-auditor",
        "version": "1.0.0",
        "description": "Crypto wallet security auditor for reviewing wallet implementations, key management, signing flows, and common vulnerability patterns."
      },
      "filePath": "plugins/crypto/wallet-security-auditor/skills/auditing-wallet-security/SKILL.md"
    },
    {
      "slug": "automating-api-testing",
      "name": "automating-api-testing",
      "description": "Test automate API endpoint testing including request generation, validation, and comprehensive test coverage for REST and GraphQL APIs. Use when testing API contracts, validating OpenAPI specifications, or ensuring endpoint reliability. Trigger with phrases like \"test the API\", \"generate API tests\", or \"validate API contracts\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:api-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Api Test Automation\n\nThis skill provides automated assistance for api test automation tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API definition files (OpenAPI/Swagger, GraphQL schema, or endpoint documentation)\n- Base URL for the API service (development, staging, or test environment)\n- Authentication credentials or API keys if endpoints require authorization\n- Testing framework installed (Jest, Mocha, Supertest, or equivalent)\n- Network connectivity to the target API service\n\n## Instructions\n\n### Step 1: Analyze API Definition\nExamine the API structure and endpoints:\n1. Use Read tool to load OpenAPI/Swagger specifications from {baseDir}/api-specs/\n2. Identify all available endpoints, HTTP methods, and request/response schemas\n3. Document authentication requirements and rate limiting constraints\n4. Note any deprecated endpoints or breaking changes\n\n### Step 2: Generate Test Cases\nCreate comprehensive test coverage:\n1. Generate CRUD operation tests (Create, Read, Update, Delete)\n2. Add authentication flow tests (login, token refresh, logout)\n3. Include edge case tests (invalid inputs, boundary conditions, malformed requests)\n4. Create contract validation tests against OpenAPI schemas\n5. Add performance tests for critical endpoints\n\n### Step 3: Execute Test Suite\nRun automated API tests:\n1. Use Bash(test:api-*) to execute test framework with generated test files\n2. Validate HTTP status codes match expected responses (200, 201, 400, 401, 404, 500)\n3. Verify response headers (Content-Type, Cache-Control, CORS headers)\n4. Validate response body structure against schemas using JSON Schema validation\n5. Test authentication token expiration and renewal flows\n\n### Step 4: Generate Test Report\nDocument results in {baseDir}/test-reports/api/:\n- Test execution summary with pass/fail counts\n- Coverage metrics by endpoint and HTTP method\n- Failed test details with request/response payloads\n- Performance benchmarks (response times, throughput)\n- Contract violation details if schema mismatches detected\n\n## Output\n\nThe skill generates structured API test artifacts:\n\n### Test Suite Files\nGenerated test files organized by resource:\n- `{baseDir}/tests/api/users.test.js` - User endpoint tests\n- `{baseDir}/tests/api/products.test.js` - Product endpoint tests\n- `{baseDir}/tests/api/auth.test.js` - Authentication flow tests\n\n### Test Coverage Report\n- Endpoint coverage percentage (target: 100% for critical paths)\n- HTTP method coverage per endpoint (GET, POST, PUT, PATCH, DELETE)\n- Authentication scenario coverage (authenticated vs. unauthenticated)\n- Error condition coverage (4xx and 5xx responses)\n\n### Contract Validation Results\n- OpenAPI schema compliance status for each endpoint\n- Breaking changes detected between specification versions\n- Undocumented endpoints or parameters found in implementation\n- Response schema violations with diff details\n\n### Performance Metrics\n- Average response time per endpoint\n- 95th and 99th percentile latencies\n- Requests per second throughput measurements\n- Timeout occurrences and slow endpoint identification\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Connection Refused**\n- Error: Cannot connect to API service at specified base URL\n- Solution: Verify service is running using Bash(test:api-healthcheck); check network connectivity and firewall rules\n\n**Authentication Failures**\n- Error: 401 Unauthorized or 403 Forbidden on protected endpoints\n- Solution: Verify API keys are valid and not expired; ensure bearer token format is correct; check scope permissions\n\n**Schema Validation Errors**\n- Error: Response does not match OpenAPI schema definition\n- Solution: Update OpenAPI specification to match actual API behavior; file bug if API implementation is incorrect\n\n**Timeout Errors**\n- Error: Request exceeded configured timeout threshold\n- Solution: Increase timeout for slow endpoints; investigate performance issues on API server; add retry logic for transient failures\n\n## Resources\n\n### API Testing Frameworks\n- Supertest for Node.js HTTP assertion testing\n- REST-assured for Java API testing\n- Postman/Newman for collection-based API testing\n- Pact for contract testing and consumer-driven contracts\n\n### Validation Libraries\n- Ajv for JSON Schema validation\n- OpenAPI Schema Validator for spec compliance\n- Joi for Node.js schema validation\n- GraphQL Schema validation tools\n\n### Best Practices\n- Test against non-production environments to avoid data corruption\n- Use test data factories to create consistent test fixtures\n- Implement proper test isolation with database cleanup between tests\n- Version control test suites alongside API specifications\n- Run tests in CI/CD pipeline for continuous validation\n\n## Overview\n\n\nThis skill provides automated assistance for api test automation tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-test-automation",
        "category": "testing",
        "path": "plugins/testing/api-test-automation",
        "version": "1.0.0",
        "description": "Automated API endpoint testing with request generation, validation, and comprehensive test coverage"
      },
      "filePath": "plugins/testing/api-test-automation/skills/automating-api-testing/SKILL.md"
    },
    {
      "slug": "automating-database-backups",
      "name": "automating-database-backups",
      "description": "Process use when you need to automate database backup processes with scheduling and encryption. This skill creates backup scripts for PostgreSQL, MySQL, MongoDB, and SQLite with compression. Trigger with phrases like \"automate database backups\", \"schedule database dumps\", \"create backup scripts\", or \"implement disaster recovery for database\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(pg_dump:*), Bash(mysqldump:*), Bash(mongodump:*), Bash(cron:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Backup Automator\n\nThis skill provides automated assistance for database backup automator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Database credentials with backup permissions (SELECT on all tables)\n- Sufficient disk space for backup files (estimate 2-3x database size with compression)\n- Cron or task scheduler access for automated scheduling\n- Backup destination storage (local disk, NFS, S3, GCS, Azure Blob)\n- Encryption tools installed (gpg, openssl) for secure backups\n- Test database available for restore validation\n\n## Instructions\n\n### Step 1: Assess Backup Requirements\n1. Identify database type (PostgreSQL, MySQL, MongoDB, SQLite)\n2. Determine backup frequency (hourly, daily, weekly, monthly)\n3. Define retention policy (how long to keep backups)\n4. Calculate expected backup size and storage needs\n5. Document RTO (Recovery Time Objective) and RPO (Recovery Point Objective)\n\n### Step 2: Design Backup Strategy\n1. Choose backup type: full, incremental, or differential\n2. Select backup destination (local, network storage, cloud)\n3. Plan backup scheduling to avoid peak usage times\n4. Define backup naming convention with timestamps\n5. Determine compression and encryption requirements\n\n### Step 3: Generate Backup Scripts\n1. Create database-specific backup command (pg_dump, mysqldump, mongodump)\n2. Add compression using gzip or zstd for storage efficiency\n3. Implement encryption using gpg or openssl for security\n4. Add error handling and logging to backup script\n5. Include backup verification (checksum, test restore)\n\n### Step 4: Configure Backup Schedule\n1. Create cron job entry for automated execution\n2. Set appropriate schedule based on backup frequency\n3. Configure environment variables for credentials\n4. Set up log rotation for backup logs\n5. Test manual execution before enabling automation\n\n### Step 5: Implement Retention Policy\n1. Create cleanup script to remove old backups\n2. Implement tiered retention (daily 7 days, weekly 4 weeks, monthly 12 months)\n3. Schedule retention cleanup after backup completion\n4. Add safeguards to prevent accidental deletion of recent backups\n5. Log all backup deletions for audit trail\n\n### Step 6: Create Restore Procedures\n1. Document step-by-step restore process\n2. Create restore scripts for each database type\n3. Include procedures for point-in-time recovery\n4. Test restore process on non-production environment\n5. Document restore time estimates and validation steps\n\n## Output\n\nThis skill produces:\n\n**Backup Scripts**: Shell scripts for database dumps with compression and encryption\n\n**Cron Configurations**: Crontab entries for automated backup scheduling\n\n**Retention Scripts**: Automated cleanup scripts implementing retention policies\n\n**Restore Procedures**: Step-by-step documentation and scripts for database restoration\n\n**Monitoring Configuration**: Log file locations and success/failure notification setup\n\n## Error Handling\n\n**Backup Failures**:\n- Check database connectivity and credentials\n- Verify sufficient disk space for backup files\n- Review database logs for lock or permission issues\n- Implement retry logic with exponential backoff\n- Send alerts on backup failures\n\n**Insufficient Disk Space**:\n- Monitor disk usage before backup execution\n- Implement pre-backup cleanup of old backups\n- Use incremental backups to reduce space requirements\n- Compress backups more aggressively\n- Move backups to remote storage immediately after creation\n\n**Encryption Errors**:\n- Verify encryption tools (gpg, openssl) are installed\n- Check encryption key availability and permissions\n- Test encryption/decryption process manually\n- Document key management procedures\n- Store encryption keys securely separate from backups\n\n**Schedule Conflicts**:\n- Ensure only one backup runs at a time (use lock files)\n- Adjust backup schedule to avoid peak database usage\n- Implement backup queuing for multiple databases\n- Monitor backup duration and adjust schedule if needed\n- Alert if backup duration exceeds acceptable window\n\n## Resources\n\n**Backup Script Templates**:\n- PostgreSQL: `{baseDir}/templates/backup-scripts/postgresql-backup.sh`\n- MySQL: `{baseDir}/templates/backup-scripts/mysql-backup.sh`\n- MongoDB: `{baseDir}/templates/backup-scripts/mongodb-backup.sh`\n- SQLite: `{baseDir}/templates/backup-scripts/sqlite-backup.sh`\n\n**Restore Procedures**: `{baseDir}/docs/restore-procedures/`\n- Point-in-time recovery\n- Full database restore\n- Selective table restore\n- Cross-server migration\n\n**Retention Policy Templates**: `{baseDir}/templates/retention-policies.yaml`\n**Cron Job Examples**: `{baseDir}/examples/crontab-entries.txt`\n**Monitoring Scripts**: `{baseDir}/scripts/backup-monitoring.sh`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-backup-automator",
        "category": "database",
        "path": "plugins/database/database-backup-automator",
        "version": "1.0.0",
        "description": "Automate database backups with scheduling, compression, encryption, and restore procedures"
      },
      "filePath": "plugins/database/database-backup-automator/skills/automating-database-backups/SKILL.md"
    },
    {
      "slug": "backtesting-trading-strategies",
      "name": "backtesting-trading-strategies",
      "description": "Test backtest crypto trading strategies against historical data with performance metrics. Use when validating trading strategies with historical data. Trigger with phrases like \"backtest strategy\", \"test trading signals\", or \"validate approach\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:backtest-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Backtesting Trading Strategies\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:backtest-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "trading-strategy-backtester",
        "category": "crypto",
        "path": "plugins/crypto/trading-strategy-backtester",
        "version": "1.0.0",
        "description": "Backtest trading strategies with historical data, performance metrics, and risk analysis"
      },
      "filePath": "plugins/crypto/trading-strategy-backtester/skills/backtesting-trading-strategies/SKILL.md"
    },
    {
      "slug": "building-api-authentication",
      "name": "building-api-authentication",
      "description": "Build secure API authentication systems with OAuth2, JWT, API keys, and session management. Use when implementing secure authentication flows. Trigger with phrases like \"build authentication\", \"add API auth\", or \"secure the API\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:auth-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Building Api Authentication\n\n## Overview\n\n\nThis skill provides automated assistance for api authentication builder tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:auth-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-authentication-builder",
        "category": "api-development",
        "path": "plugins/api-development/api-authentication-builder",
        "version": "1.0.0",
        "description": "Build authentication systems with JWT, OAuth2, and API keys"
      },
      "filePath": "plugins/api-development/api-authentication-builder/skills/building-api-authentication/SKILL.md"
    },
    {
      "slug": "building-api-gateway",
      "name": "building-api-gateway",
      "description": "Create API gateways with routing, load balancing, rate limiting, and authentication. Use when routing and managing multiple API services. Trigger with phrases like \"build API gateway\", \"create API router\", or \"setup API gateway\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:gateway-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Building Api Gateway\n\n## Overview\n\n\nThis skill provides automated assistance for api gateway builder tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:gateway-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-gateway-builder",
        "category": "api-development",
        "path": "plugins/api-development/api-gateway-builder",
        "version": "1.0.0",
        "description": "Build API gateway with routing, authentication, and rate limiting"
      },
      "filePath": "plugins/api-development/api-gateway-builder/skills/building-api-gateway/SKILL.md"
    },
    {
      "slug": "building-automl-pipelines",
      "name": "building-automl-pipelines",
      "description": "Build automated machine learning pipelines with feature engineering, model selection, and hyperparameter tuning. Use when automating ML workflows from data preparation through model deployment. Trigger with phrases like \"build automl pipeline\", \"automate ml workflow\", or \"create automated training pipeline\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(python:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Building Automl Pipelines\n\n## Overview\n\nBuild an end-to-end AutoML pipeline: data checks, feature preprocessing, model search/tuning, evaluation, and exportable deployment artifacts. Use this when you want repeatable training runs with a clear budget (time/compute) and a structured output (configs, reports, and a runnable pipeline).\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Python environment with AutoML libraries (Auto-sklearn, TPOT, H2O AutoML, or PyCaret)\n- Training dataset in accessible format (CSV, Parquet, or database)\n- Understanding of problem type (classification, regression, time-series)\n- Sufficient computational resources for automated search\n- Knowledge of evaluation metrics appropriate for task\n- Target variable and feature columns clearly defined\n\n## Instructions\n\n1. Identify problem type (binary/multi-class classification, regression, etc.)\n2. Define evaluation metrics (accuracy, F1, RMSE, etc.)\n3. Set time and resource budgets for AutoML search\n4. Specify feature types and preprocessing needs\n5. Determine model interpretability requirements\n1. Load training data using Read tool\n2. Perform initial data quality assessment\n3. Configure train/validation/test split strategy\n4. Define feature engineering transformations\n5. Set up data validation checks\n1. Initialize AutoML pipeline with configuration\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Complete Python implementation of AutoML pipeline\n- Data loading and preprocessing functions\n- Feature engineering transformations\n- Model training and evaluation logic\n- Hyperparameter search configuration\n- Best model architecture and hyperparameters\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- **Auto-sklearn**: Automated scikit-learn pipeline construction with metalearning\n- **TPOT**: Genetic programming for pipeline optimization\n- **H2O AutoML**: Scalable AutoML with ensemble methods\n- **PyCaret**: Low-code ML library with automated workflows\n- Automated feature selection techniques",
      "parentPlugin": {
        "name": "automl-pipeline-builder",
        "category": "ai-ml",
        "path": "plugins/ai-ml/automl-pipeline-builder",
        "version": "1.0.0",
        "description": "Build AutoML pipelines"
      },
      "filePath": "plugins/ai-ml/automl-pipeline-builder/skills/building-automl-pipelines/SKILL.md"
    },
    {
      "slug": "building-cicd-pipelines",
      "name": "building-cicd-pipelines",
      "description": "Execute use when you need to work with deployment and CI/CD. This skill provides deployment automation and pipeline orchestration with comprehensive guidance and automation. Trigger with phrases like \"deploy application\", \"create pipeline\", or \"automate deployment\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(git:*), Bash(docker:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Ci Cd Pipeline Builder\n\nThis skill provides automated assistance for ci cd pipeline builder tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/ci-cd-pipeline-builder/`\n\n**Documentation and Guides**: `{baseDir}/docs/ci-cd-pipeline-builder/`\n\n**Example Scripts and Code**: `{baseDir}/examples/ci-cd-pipeline-builder/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/ci-cd-pipeline-builder-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/ci-cd-pipeline-builder-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/ci-cd-pipeline-builder-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "ci-cd-pipeline-builder",
        "category": "devops",
        "path": "plugins/devops/ci-cd-pipeline-builder",
        "version": "1.0.0",
        "description": "Build CI/CD pipelines for GitHub Actions, GitLab CI, Jenkins, and more"
      },
      "filePath": "plugins/devops/ci-cd-pipeline-builder/skills/building-cicd-pipelines/SKILL.md"
    },
    {
      "slug": "building-classification-models",
      "name": "building-classification-models",
      "description": "Build and evaluate classification models for supervised learning tasks with labeled data. Use when requesting \"build a classifier\", \"create classification model\", or \"train classifier\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Classification Model Builder\n\nThis skill provides automated assistance for classification model builder tasks.\n\n## Overview\n\nThis skill empowers Claude to efficiently build and deploy classification models. It automates the process of model selection, training, and evaluation, providing users with a robust and reliable classification solution. The skill also provides insights into model performance and suggests potential improvements.\n\n## How It Works\n\n1. **Context Analysis**: Claude analyzes the user's request, identifying the dataset, target variable, and any specific requirements for the classification model.\n2. **Model Generation**: The skill utilizes the classification-model-builder plugin to generate code for training a classification model based on the identified dataset and requirements. This includes data preprocessing, feature selection, model selection, and hyperparameter tuning.\n3. **Evaluation and Reporting**: The generated model is trained and evaluated using appropriate metrics (e.g., accuracy, precision, recall, F1-score). Performance metrics and insights are then provided to the user.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Build a classification model from a given dataset.\n- Train a classifier to predict categorical outcomes.\n- Evaluate the performance of a classification model.\n\n## Examples\n\n### Example 1: Building a Spam Classifier\n\nUser request: \"Build a classifier to detect spam emails using this dataset.\"\n\nThe skill will:\n1. Analyze the provided email dataset to identify features and the target variable (spam/not spam).\n2. Generate Python code using the classification-model-builder plugin to train a spam classification model, including data cleaning, feature extraction, and model selection.\n\n### Example 2: Predicting Customer Churn\n\nUser request: \"Create a classification model to predict customer churn using customer data.\"\n\nThe skill will:\n1. Analyze the customer data to identify relevant features and the churn status.\n2. Generate code to build a classification model for churn prediction, including data validation, model training, and performance reporting.\n\n## Best Practices\n\n- **Data Quality**: Ensure the input data is clean and preprocessed before training the model.\n- **Model Selection**: Choose the appropriate classification algorithm based on the characteristics of the data and the specific requirements of the task.\n- **Hyperparameter Tuning**: Optimize the model's hyperparameters to achieve the best possible performance.\n\n## Integration\n\nThis skill integrates with the classification-model-builder plugin to automate the model building process. It can also be used in conjunction with other plugins for data analysis and visualization.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "classification-model-builder",
        "category": "ai-ml",
        "path": "plugins/ai-ml/classification-model-builder",
        "version": "1.0.0",
        "description": "Build classification models"
      },
      "filePath": "plugins/ai-ml/classification-model-builder/skills/building-classification-models/SKILL.md"
    },
    {
      "slug": "building-gitops-workflows",
      "name": "building-gitops-workflows",
      "description": "Execute use when constructing GitOps workflows using ArgoCD or Flux. Trigger with phrases like \"create GitOps workflow\", \"setup ArgoCD\", \"configure Flux\", or \"automate Kubernetes deployments\". Generates production-ready configurations, implements best practices, and ensures security-first approach for continuous deployment. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(kubectl:*), Bash(git:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Gitops Workflow Builder\n\nThis skill provides automated assistance for gitops workflow builder tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Kubernetes cluster is accessible and kubectl is configured\n- Git repository is available for GitOps source\n- ArgoCD or Flux is installed on the cluster (or ready to install)\n- Appropriate RBAC permissions for GitOps operator\n- Network connectivity between cluster and Git repository\n\n## Instructions\n\n1. **Select GitOps Tool**: Determine whether to use ArgoCD or Flux based on requirements\n2. **Define Application Structure**: Establish repository layout with environment separation (dev/staging/prod)\n3. **Generate Manifests**: Create Application/Kustomization files pointing to Git sources\n4. **Configure Sync Policy**: Set automated or manual sync with self-heal and prune options\n5. **Implement RBAC**: Define service accounts and role bindings for GitOps operator\n6. **Set Up Monitoring**: Configure notifications and health checks for deployments\n7. **Validate Configuration**: Test sync behavior and verify reconciliation loops\n\n## Output\n\nGenerates GitOps workflow configurations including:\n\n**ArgoCD Application Manifest:**\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: app-name\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/org/repo\n    path: manifests/prod\n    targetRevision: main\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: production\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n```\n\n**Flux Kustomization:**\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: app-name\n  namespace: flux-system\nspec:\n  interval: 5m\n  path: ./manifests/prod\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: app-repo\n```\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Sync Failures**\n- Error: \"ComparisonError: Failed to load target state\"\n- Solution: Verify Git repository URL, credentials, and target path exist\n\n**RBAC Permissions**\n- Error: \"User cannot create resource in API group\"\n- Solution: Grant GitOps service account appropriate cluster roles\n\n**Out of Sync State**\n- Warning: \"Application is OutOfSync\"\n- Solution: Enable automated sync or manually sync via UI/CLI\n\n**Git Authentication**\n- Error: \"Authentication failed for repository\"\n- Solution: Configure SSH keys or access tokens in {baseDir}/.git/config\n\n**Resource Conflicts**\n- Error: \"Resource already exists and is not managed by GitOps\"\n- Solution: Import existing resources or remove conflicting manual deployments\n\n## Resources\n\n- ArgoCD documentation: https://argo-cd.readthedocs.io/\n- Flux documentation: https://fluxcd.io/docs/\n- GitOps principles and patterns guide\n- Kubernetes manifest best practices\n- Repository structure templates in {baseDir}/gitops-examples/\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "gitops-workflow-builder",
        "category": "devops",
        "path": "plugins/devops/gitops-workflow-builder",
        "version": "1.0.0",
        "description": "Build GitOps workflows with ArgoCD and Flux"
      },
      "filePath": "plugins/devops/gitops-workflow-builder/skills/building-gitops-workflows/SKILL.md"
    },
    {
      "slug": "building-graphql-server",
      "name": "building-graphql-server",
      "description": "Build production-ready GraphQL servers with schema design, resolvers, and subscriptions. Use when building GraphQL APIs with schemas and resolvers. Trigger with phrases like \"build GraphQL API\", \"create GraphQL server\", or \"setup GraphQL\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:graphql-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Building Graphql Server\n\n## Overview\n\n\nThis skill provides automated assistance for graphql server builder tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:graphql-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "graphql-server-builder",
        "category": "api-development",
        "path": "plugins/api-development/graphql-server-builder",
        "version": "1.0.0",
        "description": "Build GraphQL servers with schema-first design, resolvers, and subscriptions"
      },
      "filePath": "plugins/api-development/graphql-server-builder/skills/building-graphql-server/SKILL.md"
    },
    {
      "slug": "building-neural-networks",
      "name": "building-neural-networks",
      "description": "Execute this skill allows AI assistant to construct and configure neural network architectures using the neural-network-builder plugin. it should be used when the user requests the creation of a new neural network, modification of an existing one, or assistance... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Neural Network Builder\n\nThis skill provides automated assistance for neural network builder tasks.\n\n## Overview\n\nThis skill empowers Claude to design and implement neural networks tailored to specific tasks. It leverages the neural-network-builder plugin to automate the process of defining network architectures, configuring layers, and setting training parameters. This ensures efficient and accurate creation of neural network models.\n\n## How It Works\n\n1. **Analyzing Requirements**: Claude analyzes the user's request to understand the desired neural network architecture, task, and performance goals.\n2. **Generating Configuration**: Based on the analysis, Claude generates the appropriate configuration for the neural-network-builder plugin, specifying the layers, activation functions, and other relevant parameters.\n3. **Executing Build**: Claude executes the `build-nn` command, triggering the neural-network-builder plugin to construct the neural network based on the generated configuration.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a new neural network architecture for a specific machine learning task.\n- Modify an existing neural network's layers, parameters, or training process.\n- Design a neural network using specific layer types, such as convolutional, recurrent, or transformer layers.\n\n## Examples\n\n### Example 1: Image Classification\n\nUser request: \"Build a convolutional neural network for image classification with three convolutional layers and two fully connected layers.\"\n\nThe skill will:\n1. Analyze the request and determine the required CNN architecture.\n2. Generate the configuration for the `build-nn` command, specifying the layer types, filter sizes, and activation functions.\n\n### Example 2: Text Generation\n\nUser request: \"Define an RNN architecture for text generation with LSTM cells and an embedding layer.\"\n\nThe skill will:\n1. Analyze the request and determine the required RNN architecture.\n2. Generate the configuration for the `build-nn` command, specifying the LSTM cell parameters, embedding dimension, and output layer.\n\n## Best Practices\n\n- **Layer Selection**: Choose appropriate layer types (e.g., convolutional, recurrent, transformer) based on the task and data characteristics.\n- **Parameter Tuning**: Experiment with different parameter values (e.g., learning rate, batch size, number of layers) to optimize performance.\n- **Regularization**: Implement regularization techniques (e.g., dropout, L1/L2 regularization) to prevent overfitting.\n\n## Integration\n\nThis skill integrates with the core Claude Code environment by utilizing the `build-nn` command provided by the neural-network-builder plugin. It can be combined with other skills for data preprocessing, model evaluation, and deployment.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "neural-network-builder",
        "category": "ai-ml",
        "path": "plugins/ai-ml/neural-network-builder",
        "version": "1.0.0",
        "description": "Build and configure neural network architectures"
      },
      "filePath": "plugins/ai-ml/neural-network-builder/skills/building-neural-networks/SKILL.md"
    },
    {
      "slug": "building-recommendation-systems",
      "name": "building-recommendation-systems",
      "description": "Execute this skill empowers AI assistant to construct recommendation systems using collaborative filtering, content-based filtering, or hybrid approaches. it analyzes user preferences, item features, and interaction data to generate personalized recommendations... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Recommendation Engine\n\nThis skill provides automated assistance for recommendation engine tasks.\n\n## Overview\n\nThis skill enables Claude to design and implement recommendation systems tailored to specific datasets and use cases. It automates the process of selecting appropriate algorithms, preprocessing data, training models, and evaluating performance, ultimately providing users with a functional recommendation engine.\n\n## How It Works\n\n1. **Analyzing Requirements**: Claude identifies the type of recommendation needed (collaborative, content-based, hybrid), data availability, and performance goals.\n2. **Generating Code**: Claude generates Python code using relevant libraries (e.g., scikit-learn, TensorFlow, PyTorch) to build the recommendation model. This includes data loading, preprocessing, model training, and evaluation.\n3. **Implementing Best Practices**: The code incorporates best practices for recommendation system development, such as handling cold starts, addressing scalability, and mitigating bias.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Build a personalized movie recommendation system.\n- Create a product recommendation engine for an e-commerce platform.\n- Implement a content recommendation system for a news website.\n\n## Examples\n\n### Example 1: Personalized Movie Recommendations\n\nUser request: \"Build a movie recommendation system using collaborative filtering.\"\n\nThe skill will:\n1. Generate code to load and preprocess movie rating data.\n2. Implement a collaborative filtering algorithm (e.g., matrix factorization) to predict user preferences.\n\n### Example 2: E-commerce Product Recommendations\n\nUser request: \"Create a product recommendation engine for an online store, using content-based filtering.\"\n\nThe skill will:\n1. Generate code to extract features from product descriptions and user purchase history.\n2. Implement a content-based filtering algorithm to recommend similar products.\n\n## Best Practices\n\n- **Data Preprocessing**: Ensure data is properly cleaned and formatted before training the recommendation model.\n- **Model Evaluation**: Use appropriate metrics (e.g., precision, recall, NDCG) to evaluate the performance of the recommendation system.\n- **Scalability**: Design the recommendation system to handle large datasets and user bases efficiently.\n\n## Integration\n\nThis skill can be integrated with other Claude Code plugins to access data sources, deploy models, and monitor performance. For example, it can use data analysis plugins to extract features from raw data and deployment plugins to deploy the recommendation system to a production environment.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "recommendation-engine",
        "category": "ai-ml",
        "path": "plugins/ai-ml/recommendation-engine",
        "version": "1.0.0",
        "description": "Build recommendation systems and engines"
      },
      "filePath": "plugins/ai-ml/recommendation-engine/skills/building-recommendation-systems/SKILL.md"
    },
    {
      "slug": "building-terraform-modules",
      "name": "building-terraform-modules",
      "description": "Execute this skill empowers AI assistant to build reusable terraform modules based on user specifications. it leverages the terraform-module-builder plugin to generate production-ready, well-documented terraform module code, incorporating best practices for sec... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Terraform Module Builder\n\nThis skill provides automated assistance for terraform module builder tasks.\n\n## Overview\n\nThis skill allows Claude to efficiently generate Terraform modules, streamlining infrastructure-as-code development. By utilizing the terraform-module-builder plugin, it ensures modules are production-ready, well-documented, and incorporate best practices.\n\n## How It Works\n\n1. **Receiving User Request**: Claude receives a request to create a Terraform module, including details about the module's purpose and desired features.\n2. **Generating Module Structure**: Claude invokes the terraform-module-builder plugin to create the basic file structure and configuration files for the module.\n3. **Customizing Module Content**: Claude uses the user's specifications to populate the module with variables, outputs, and resource definitions, ensuring best practices are followed.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a new Terraform module from scratch.\n- Generate production-ready Terraform configuration files.\n- Implement infrastructure as code using Terraform modules.\n\n## Examples\n\n### Example 1: Creating a VPC Module\n\nUser request: \"Create a Terraform module for a VPC with public and private subnets, a NAT gateway, and appropriate security groups.\"\n\nThe skill will:\n1. Invoke the terraform-module-builder plugin to generate a basic VPC module structure.\n2. Populate the module with Terraform code to define the VPC, subnets, NAT gateway, and security groups based on best practices.\n\n### Example 2: Generating an S3 Bucket Module\n\nUser request: \"Generate a Terraform module for an S3 bucket with versioning enabled, encryption at rest, and a lifecycle policy for deleting objects after 30 days.\"\n\nThe skill will:\n1. Invoke the terraform-module-builder plugin to create a basic S3 bucket module structure.\n2. Populate the module with Terraform code to define the S3 bucket with the requested features (versioning, encryption, lifecycle policy).\n\n## Best Practices\n\n- **Documentation**: Ensure the generated Terraform module includes comprehensive documentation, explaining the module's purpose, inputs, and outputs.\n- **Security**: Implement security best practices, such as using least privilege principles and encrypting sensitive data.\n- **Modularity**: Design the Terraform module to be reusable and configurable, allowing it to be easily adapted to different environments.\n\n## Integration\n\nThis skill integrates seamlessly with other Claude Code plugins by providing a foundation for infrastructure provisioning. The generated Terraform modules can be used by other plugins to deploy and manage resources in various cloud environments.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "terraform-module-builder",
        "category": "devops",
        "path": "plugins/devops/terraform-module-builder",
        "version": "1.0.0",
        "description": "Build reusable Terraform modules"
      },
      "filePath": "plugins/devops/terraform-module-builder/skills/building-terraform-modules/SKILL.md"
    },
    {
      "slug": "building-websocket-server",
      "name": "building-websocket-server",
      "description": "Build scalable WebSocket servers for real-time bidirectional communication. Use when enabling real-time bidirectional communication. Trigger with phrases like \"build WebSocket server\", \"add real-time API\", or \"implement WebSocket\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:websocket-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Building Websocket Server\n\n## Overview\n\n\nThis skill provides automated assistance for websocket server builder tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:websocket-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "websocket-server-builder",
        "category": "api-development",
        "path": "plugins/api-development/websocket-server-builder",
        "version": "1.0.0",
        "description": "Build WebSocket servers for real-time bidirectional communication"
      },
      "filePath": "plugins/api-development/websocket-server-builder/skills/building-websocket-server/SKILL.md"
    },
    {
      "slug": "calculating-crypto-taxes",
      "name": "calculating-crypto-taxes",
      "description": "Execute calculate cryptocurrency tax obligations with cost basis tracking and jurisdiction rules. Use when calculating crypto tax obligations. Trigger with phrases like \"calculate crypto taxes\", \"compute tax liability\", or \"generate tax report\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:tax-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Calculating Crypto Taxes\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:tax-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "crypto-tax-calculator",
        "category": "crypto",
        "path": "plugins/crypto/crypto-tax-calculator",
        "version": "1.0.0",
        "description": "Calculate crypto taxes with FIFO/LIFO methods and generate tax reports"
      },
      "filePath": "plugins/crypto/crypto-tax-calculator/skills/calculating-crypto-taxes/SKILL.md"
    },
    {
      "slug": "changelog-orchestrator",
      "name": "changelog-orchestrator",
      "description": "Draft changelog PRs by collecting GitHub/Slack/Git changes, formatting with templates, running quality gates, and preparing a branch/PR. Use when generating weekly/monthly release notes or when the user asks to create a changelog from recent merges. Trigger with \"changelog weekly\", \"generate release notes\", \"draft changelog\", \"create changelog PR\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(git:*)",
        "Bash(gh:*)",
        "Bash(python:*)",
        "Bash(date:*)\""
      ],
      "version": "\"0.1.0\"",
      "author": "\"Mattyp <mattyp@claudecodeplugins.io>\"",
      "license": "\"MIT\"",
      "content": "# Changelog Orchestrator\n\n## Overview\n\nThis skill turns raw repo activity (merged PRs, issues, commits, optional Slack updates) into a publishable changelog draft and prepares a branch/PR for review.\n\n## Prerequisites\n\n- A project config file at `.changelog-config.json` in the target repo.\n- Required environment variables set (at minimum `GITHUB_TOKEN` for GitHub source).\n- Git available in PATH; `gh` optional (used for PR creation if configured).\n\n## Instructions\n\n1. Read `.changelog-config.json` from the repo root.\n2. Validate it with `{baseDir}/scripts/validate_config.py`.\n3. Decide date range:\n1. Load the configured markdown template (or fall back to `{baseDir}/assets/weekly-template.md`).\n2. Render the final markdown using `{baseDir}/scripts/render_template.py`.\n3. Ensure frontmatter contains at least `date` (ISO) and `version` (SemVer if known; otherwise `0.0.0`).\n1. Run deterministic checks using `{baseDir}/scripts/quality_score.py`.\n2. If score is below threshold:\n1. Write the changelog file to the configured `output_path`.\n2. Create a branch `changelog-YYYY-MM-DD`, commit with `docs: add changelog for YYYY-MM-DD`.\n3. If `gh` is configured, open a PR; otherwise, print the exact commands the user should run.\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- A markdown changelog draft (usually `CHANGELOG.md`), plus an optional PR URL.\n- A quality report (score + findings) from `{baseDir}/scripts/quality_score.py`.\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Validate config: `{baseDir}/scripts/validate_config.py`\n- Render template: `{baseDir}/scripts/render_template.py`\n- Quality scoring: `{baseDir}/scripts/quality_score.py`\n- Default templates:\n  - `{baseDir}/assets/default-changelog.md`\n  - `{baseDir}/assets/weekly-template.md`\n  - `{baseDir}/assets/release-template.md`",
      "parentPlugin": {
        "name": "mattyp-changelog",
        "category": "automation",
        "path": "plugins/automation/mattyp-changelog",
        "version": "0.1.0",
        "description": "Automate changelog generation: fetch recent changes, synthesize release notes, run quality gates, and prepare a PR."
      },
      "filePath": "plugins/automation/mattyp-changelog/skills/changelog-orchestrator/SKILL.md"
    },
    {
      "slug": "checking-hipaa-compliance",
      "name": "checking-hipaa-compliance",
      "description": "Check HIPAA compliance for healthcare data security requirements. Use when auditing healthcare applications. Trigger with 'check HIPAA compliance', 'validate health data security', or 'audit PHI protection'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Hipaa Compliance Checker\n\nThis skill provides automated assistance for hipaa compliance checker tasks.\n\n## Overview\n\nThis skill automates the process of identifying potential HIPAA compliance issues within a software project. By using the hipaa-compliance-checker plugin, it helps developers and security professionals proactively address vulnerabilities and ensure adherence to HIPAA guidelines.\n\n## How It Works\n\n1. **Analyze Request**: Claude identifies the user's intent to check for HIPAA compliance.\n2. **Initiate Plugin**: Claude activates the hipaa-compliance-checker plugin.\n3. **Execute Checks**: The plugin scans the specified codebase, configuration files, or documentation for potential HIPAA violations.\n4. **Generate Report**: The plugin generates a detailed report outlining identified issues and their potential impact on HIPAA compliance.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Evaluate a codebase for HIPAA compliance before deployment.\n- Identify potential HIPAA violations in existing systems.\n- Assess the HIPAA readiness of infrastructure configurations.\n- Verify that documentation adheres to HIPAA guidelines.\n\n## Examples\n\n### Example 1: Checking a codebase for HIPAA compliance\n\nUser request: \"Check HIPAA compliance of the patient data API codebase.\"\n\nThe skill will:\n1. Activate the hipaa-compliance-checker plugin.\n2. Scan the specified API codebase for potential HIPAA violations.\n3. Generate a report listing any identified issues, such as insecure data storage or insufficient access controls.\n\n### Example 2: Assessing infrastructure configuration for HIPAA readiness\n\nUser request: \"Assess the HIPAA readiness of our AWS infrastructure configuration.\"\n\nThe skill will:\n1. Activate the hipaa-compliance-checker plugin.\n2. Analyze the AWS infrastructure configuration files for potential HIPAA violations, such as misconfigured security groups or inadequate encryption.\n3. Generate a report outlining any identified issues and recommendations for remediation.\n\n## Best Practices\n\n- **Specify Target**: Always clearly specify the target (e.g., codebase, configuration file, documentation) for the HIPAA compliance check.\n- **Review Reports**: Carefully review the generated reports to understand the identified issues and their potential impact.\n- **Prioritize Remediation**: Prioritize the remediation of identified issues based on their severity and potential impact on HIPAA compliance.\n\n## Integration\n\nThis skill can be integrated with other security and compliance tools to provide a comprehensive view of a system's security posture. The generated reports can be used as input for vulnerability management systems and security information and event management (SIEM) platforms.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "hipaa-compliance-checker",
        "category": "security",
        "path": "plugins/security/hipaa-compliance-checker",
        "version": "1.0.0",
        "description": "Check HIPAA compliance"
      },
      "filePath": "plugins/security/hipaa-compliance-checker/skills/checking-hipaa-compliance/SKILL.md"
    },
    {
      "slug": "checking-infrastructure-compliance",
      "name": "checking-infrastructure-compliance",
      "description": "Execute use when you need to work with compliance checking. This skill provides compliance monitoring and validation with comprehensive guidance and automation. Trigger with phrases like \"check compliance\", \"validate policies\", or \"audit compliance\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Compliance Checker\n\nThis skill provides automated assistance for compliance checker tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/compliance-checker/`\n\n**Documentation and Guides**: `{baseDir}/docs/compliance-checker/`\n\n**Example Scripts and Code**: `{baseDir}/examples/compliance-checker/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/compliance-checker-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/compliance-checker-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/compliance-checker-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "compliance-checker",
        "category": "devops",
        "path": "plugins/devops/compliance-checker",
        "version": "1.0.0",
        "description": "Check infrastructure compliance (SOC2, HIPAA, PCI-DSS)"
      },
      "filePath": "plugins/devops/compliance-checker/skills/checking-infrastructure-compliance/SKILL.md"
    },
    {
      "slug": "checking-owasp-compliance",
      "name": "checking-owasp-compliance",
      "description": "Check compliance with OWASP Top 10 security risks and best practices. Use when performing comprehensive security audits. Trigger with 'check OWASP compliance', 'audit web security', or 'validate OWASP'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Owasp Compliance Checker\n\nThis skill provides automated assistance for owasp compliance checker tasks.\n\n## Overview\n\nThis skill empowers Claude to assess your project's adherence to the OWASP Top 10 (2021) security guidelines. It automates the process of identifying potential vulnerabilities related to common web application security risks, providing actionable insights to improve your application's security posture.\n\n## How It Works\n\n1. **Initiate Scan**: The skill activates the owasp-compliance-checker plugin upon request.\n2. **Analyze Codebase**: The plugin scans the codebase for potential vulnerabilities related to each OWASP Top 10 category.\n3. **Generate Report**: A detailed report is generated, highlighting compliance gaps and providing specific remediation guidance for each identified issue.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Evaluate your application's security posture against the OWASP Top 10 (2021).\n- Identify potential vulnerabilities related to common web application security risks.\n- Obtain actionable remediation guidance to address identified vulnerabilities.\n- Generate a compliance report for auditing or reporting purposes.\n\n## Examples\n\n### Example 1: Identifying SQL Injection Vulnerabilities\n\nUser request: \"Check OWASP compliance for SQL injection vulnerabilities.\"\n\nThe skill will:\n1. Activate the owasp-compliance-checker plugin.\n2. Scan the codebase for potential SQL injection vulnerabilities.\n3. Generate a report highlighting any identified SQL injection vulnerabilities and providing remediation guidance.\n\n### Example 2: Assessing Overall OWASP Compliance\n\nUser request: \"/owasp\"\n\nThe skill will:\n1. Activate the owasp-compliance-checker plugin.\n2. Scan the entire codebase for vulnerabilities across all OWASP Top 10 categories.\n3. Generate a comprehensive report detailing compliance gaps and remediation steps for each category.\n\n## Best Practices\n\n- **Regular Scanning**: Integrate OWASP compliance checks into your development workflow for continuous security monitoring.\n- **Prioritize Remediation**: Address identified vulnerabilities based on their severity and potential impact.\n- **Stay Updated**: Keep your OWASP compliance checker plugin updated to benefit from the latest vulnerability detection rules and remediation guidance.\n\n## Integration\n\nThis skill can be integrated with other plugins to automate vulnerability remediation or generate comprehensive security reports. For example, it can be used in conjunction with a code modification plugin to automatically apply recommended fixes for identified vulnerabilities.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "owasp-compliance-checker",
        "category": "security",
        "path": "plugins/security/owasp-compliance-checker",
        "version": "1.0.0",
        "description": "Check OWASP Top 10 compliance"
      },
      "filePath": "plugins/security/owasp-compliance-checker/skills/checking-owasp-compliance/SKILL.md"
    },
    {
      "slug": "checking-session-security",
      "name": "checking-session-security",
      "description": "Analyze session management implementations to identify security vulnerabilities in web applications. Use when you need to audit session handling, check for session fixation risks, review session timeout configurations, or validate session ID generation security. Trigger with phrases like \"check session security\", \"audit session management\", \"review session handling\", or \"session fixation vulnerability\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(code-scan:*), Bash(security-check:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Checking Session Security\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Source code accessible in {baseDir}/\n- Session management code locations known (auth modules, middleware)\n- Framework information (Express, Django, Spring, etc.)\n- Configuration files for session settings\n- Write permissions for security report in {baseDir}/security-reports/\n\n## Instructions\n\n1. Review session creation, storage, and transport security controls.\n2. Validate cookie flags, rotation, expiration, and invalidation behavior.\n3. Identify common attack paths (fixation, CSRF, replay) and mitigations.\n4. Provide prioritized fixes with configuration/code examples.\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: Session security report saved to {baseDir}/security-reports/session-security-YYYYMMDD.md\n\n**Report Structure**:\n```\n# Session Security Analysis Report\nAnalysis Date: 2024-01-15\nApplication: Web Portal\nFramework: Express.js\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Session Management Cheat Sheet: https://cheatsheetseries.owasp.org/cheatsheets/Session_Management_Cheat_Sheet.html\n- OWASP Top 10 - Broken Authentication: https://owasp.org/www-project-top-ten/\n- NIST 800-63B Authentication: https://pages.nist.gov/800-63-3/sp800-63b.html\n- PCI-DSS Session Requirements: https://www.pcisecuritystandards.org/\n- Express.js Session Security: https://expressjs.com/en/advanced/best-practice-security.html",
      "parentPlugin": {
        "name": "session-security-checker",
        "category": "security",
        "path": "plugins/security/session-security-checker",
        "version": "1.0.0",
        "description": "Check session security implementation"
      },
      "filePath": "plugins/security/session-security-checker/skills/checking-session-security/SKILL.md"
    },
    {
      "slug": "claude-reflect",
      "name": "claude-reflect",
      "description": "Execute self-learning system that captures corrections during sessions and syncs them to CLAUDE.md. Use when discussing learnings, corrections, or when the user mentions remembering something. Trigger with phrases like \"remember this\", \"don't forget\", \"use X not Y\", or \"actually...\". allowed-tools: Read, Write, Edit, Bash(jq:*), Bash(cat:*) version: 1.4.1 license: MIT author: Bayram Annakov <bayram.annakov@gmail.com>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Bayram Annakov",
      "license": "MIT",
      "content": "# Claude Reflect - Self-Learning System\n\nA two-stage system that helps Claude Code learn from user corrections.\n\n## How It Works\n\n**Stage 1: Capture (Automatic)**\nHooks detect correction patterns (\"no, use X\", \"actually...\", \"use X not Y\") and queue them to `~/.claude/learnings-queue.json`.\n\n**Stage 2: Process (Manual)**\nUser runs `/reflect` to review and apply queued learnings to CLAUDE.md files.\n\n## Available Commands\n\n| Command | Purpose |\n|---------|---------|\n| `/reflect` | Process queued learnings with human review |\n| `/reflect --scan-history` | Scan past sessions for missed learnings |\n| `/reflect --dry-run` | Preview changes without applying |\n| `/skip-reflect` | Discard all queued learnings |\n| `/view-queue` | View pending learnings without processing |\n\n## When to Remind Users\n\nRemind users about `/reflect` when:\n- They complete a feature or meaningful work unit\n- They make corrections you should remember for future sessions\n- They explicitly say \"remember this\" or similar\n- Context is about to compact and queue has items\n\n## Correction Detection Patterns\n\nHigh-confidence corrections:\n- Tool rejections (user stops an action with guidance)\n- \"no, use X\" / \"don't use Y\"\n- \"actually...\" / \"I meant...\"\n- \"use X not Y\" / \"X instead of Y\"\n- \"remember:\" (explicit marker)\n\n## CLAUDE.md Destinations\n\n- `~/.claude/CLAUDE.md` - Global learnings (model names, general patterns)\n- `./CLAUDE.md` - Project-specific learnings (conventions, tools, structure)\n\n## Example Interaction\n\n```\nUser: no, use gpt-5.1 not gpt-5 for reasoning tasks\nClaude: Got it, I'll use gpt-5.1 for reasoning tasks.\n\n[Hook captures this correction to queue]\n\nUser: /reflect\nClaude: Found 1 learning queued. \"Use gpt-5.1 for reasoning tasks\"\n        Scope: global\n        Apply to ~/.claude/CLAUDE.md? [y/n]\n```",
      "parentPlugin": {
        "name": "claude-reflect",
        "category": "community",
        "path": "plugins/community/claude-reflect",
        "version": "1.4.1",
        "description": "Self-learning system for Claude Code that captures corrections and updates CLAUDE.md automatically"
      },
      "filePath": "plugins/community/claude-reflect/SKILL.md"
    },
    {
      "slug": "code-formatter",
      "name": "code-formatter",
      "description": "Execute automatically formats and validates code files using Prettier and other formatting tools. Use when users mention \"format my code\", \"fix formatting\", \"apply code style\", \"check formatting\", \"make code consistent\", or \"clean up code formatting\". Handles JavaScript, TypeScript, JSON, CSS, Markdown, and many other file types. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Code Formatter\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Node.js and npm/npx installed\n- Prettier available globally or locally\n- Write permissions for target files\n- Supported file types in the project\n\n## Instructions\n\n1. Analyze current formatting (`prettier --check`) and identify files to update.\n2. Configure formatting rules (`.prettierrc`, `.editorconfig`) for the project.\n3. Apply formatting (`prettier --write`) to the target files/directories.\n4. Add ignore patterns (`.prettierignore`) for generated/vendor outputs.\n5. Optionally enforce formatting via git hooks (husky/lint-staged).\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- name: Check formatting\n- name: Enforce formatting\n- **ESLint** - Linting and code quality\n- **Stylelint** - CSS/SCSS linting\n- **Markdownlint** - Markdown style checking",
      "parentPlugin": {
        "name": "formatter",
        "category": "examples",
        "path": "plugins/examples/formatter",
        "version": "2.0.1",
        "description": "Comprehensive code formatting plugin with Prettier integration. Use when you need to format code, validate formatting, or maintain consistent code style. Activates with phrases like 'format my code', 'check formatting', or 'apply code style'. Supports JavaScript, TypeScript, JSON, CSS, Markdown, and many other file types with automatic formatting on file operations."
      },
      "filePath": "plugins/examples/formatter/skills/code-formatter/SKILL.md"
    },
    {
      "slug": "collecting-infrastructure-metrics",
      "name": "collecting-infrastructure-metrics",
      "description": "Collect comprehensive infrastructure performance metrics across compute, storage, network, containers, load balancers, and databases. Use when monitoring system performance or troubleshooting infrastructure issues. Trigger with phrases like \"collect infrastructure metrics\", \"monitor server performance\", or \"track system resources\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(metrics:*)",
        "Bash(monitoring:*)",
        "Bash(system:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Infrastructure Metrics Collector\n\nThis skill provides automated assistance for infrastructure metrics collector tasks.\n\n## Overview\n\nThis skill automates the process of setting up infrastructure metrics collection. It identifies key performance indicators (KPIs) across various infrastructure layers, configures agents to collect these metrics, and assists in setting up central aggregation and visualization.\n\n## How It Works\n\n1. **Identify Infrastructure Layers**: Determines the infrastructure layers to monitor (compute, storage, network, containers, load balancers, databases).\n2. **Configure Metrics Collection**: Sets up agents (Prometheus, Datadog, CloudWatch) to collect metrics from the identified layers.\n3. **Aggregate Metrics**: Configures central aggregation of the collected metrics for analysis and visualization.\n4. **Create Dashboards**: Generates infrastructure dashboards for health monitoring, performance analysis, and capacity tracking.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Monitor the performance of your infrastructure.\n- Identify bottlenecks in your system.\n- Set up dashboards for real-time monitoring.\n\n## Examples\n\n### Example 1: Setting up basic monitoring\n\nUser request: \"Collect infrastructure metrics for my web server.\"\n\nThe skill will:\n1. Identify compute, storage, and network layers relevant to the web server.\n2. Configure Prometheus to collect CPU, memory, disk I/O, and network bandwidth metrics.\n\n### Example 2: Troubleshooting database performance\n\nUser request: \"I'm seeing slow database queries. Can you help me monitor the database performance?\"\n\nThe skill will:\n1. Identify the database layer and relevant metrics such as connection pool usage, replication lag, and cache hit rates.\n2. Configure Datadog to collect these metrics and create a dashboard to visualize performance trends.\n\n## Best Practices\n\n- **Agent Selection**: Choose the appropriate agent (Prometheus, Datadog, CloudWatch) based on your existing infrastructure and monitoring tools.\n- **Metric Granularity**: Balance the granularity of metrics collection with the storage and processing overhead. Collect only the essential metrics for your use case.\n- **Alerting**: Configure alerts based on thresholds for key metrics to proactively identify and address performance issues.\n\n## Integration\n\nThis skill can be integrated with other plugins for deployment, configuration management, and alerting to provide a comprehensive infrastructure management solution. For example, it can be used with a deployment plugin to automatically configure metrics collection after deploying new infrastructure.\n\n## Prerequisites\n\n- Access to infrastructure monitoring systems (Prometheus, Datadog, CloudWatch)\n- System permissions for metrics agent installation\n- Network access to monitored infrastructure components\n- Storage for metrics data in {baseDir}/metrics/\n\n## Instructions\n\n1. Identify infrastructure layers to monitor (compute, storage, network, databases)\n2. Select appropriate metrics collection agent based on environment\n3. Configure agent with target endpoints and metric types\n4. Set up central aggregation for collected metrics\n5. Create dashboards for visualization\n6. Configure alerts for critical metrics thresholds\n\n## Output\n\n- Metrics collection configuration files\n- Agent installation and setup scripts\n- Dashboard definitions for infrastructure monitoring\n- Metric export configurations\n- Alert rules for critical thresholds\n\n## Error Handling\n\nIf metrics collection fails:\n- Verify agent installation and permissions\n- Check network connectivity to targets\n- Validate authentication credentials\n- Review firewall and security group rules\n- Confirm metric endpoint availability\n\n## Resources\n\n- Prometheus documentation for metric collection\n- Datadog agent configuration guides\n- AWS CloudWatch metrics reference\n- Infrastructure monitoring best practices",
      "parentPlugin": {
        "name": "infrastructure-metrics-collector",
        "category": "performance",
        "path": "plugins/performance/infrastructure-metrics-collector",
        "version": "1.0.0",
        "description": "Collect comprehensive infrastructure performance metrics"
      },
      "filePath": "plugins/performance/infrastructure-metrics-collector/skills/collecting-infrastructure-metrics/SKILL.md"
    },
    {
      "slug": "comparing-database-schemas",
      "name": "comparing-database-schemas",
      "description": "Process use when you need to work with schema comparison. This skill provides database schema diff and sync with comprehensive guidance and automation. Trigger with phrases like \"compare schemas\", \"diff databases\", or \"sync database schemas\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Diff Tool\n\nThis skill provides automated assistance for database diff tool tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-diff-tool/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-diff-tool/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-diff-tool/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-diff-tool-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-diff-tool-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-diff-tool-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-diff-tool",
        "category": "database",
        "path": "plugins/database/database-diff-tool",
        "version": "1.0.0",
        "description": "Database plugin for database-diff-tool"
      },
      "filePath": "plugins/database/database-diff-tool/skills/comparing-database-schemas/SKILL.md"
    },
    {
      "slug": "configuring-auto-scaling-policies",
      "name": "configuring-auto-scaling-policies",
      "description": "Configure use when you need to work with auto-scaling. This skill provides auto-scaling configuration with comprehensive guidance and automation. Trigger with phrases like \"configure auto-scaling\", \"set up elastic scaling\", or \"implement scaling\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Auto Scaling Configurator\n\nThis skill provides automated assistance for auto scaling configurator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/auto-scaling-configurator/`\n\n**Documentation and Guides**: `{baseDir}/docs/auto-scaling-configurator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/auto-scaling-configurator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/auto-scaling-configurator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/auto-scaling-configurator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/auto-scaling-configurator-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "auto-scaling-configurator",
        "category": "devops",
        "path": "plugins/devops/auto-scaling-configurator",
        "version": "1.0.0",
        "description": "Configure auto-scaling policies for applications and infrastructure"
      },
      "filePath": "plugins/devops/auto-scaling-configurator/skills/configuring-auto-scaling-policies/SKILL.md"
    },
    {
      "slug": "configuring-load-balancers",
      "name": "configuring-load-balancers",
      "description": "Configure use when configuring load balancers including ALB, NLB, Nginx, and HAProxy. Trigger with phrases like \"configure load balancer\", \"create ALB\", \"setup nginx load balancing\", or \"haproxy configuration\". Generates production-ready configurations with health checks, SSL termination, sticky sessions, and traffic distribution rules. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(aws:*), Bash(gcloud:*), Bash(nginx:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Configuring Load Balancers\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Backend servers are identified with IPs or DNS names\n- Load balancer type is determined (ALB, NLB, Nginx, HAProxy)\n- SSL certificates are available if using HTTPS\n- Health check endpoints are defined\n- Understanding of traffic distribution requirements (round-robin, least-connections)\n- Cloud provider CLI installed (if using cloud load balancers)\n\n## Instructions\n\n1. **Select Load Balancer Type**: Choose based on requirements (L4 vs L7, cloud vs on-prem)\n2. **Define Backend Pool**: List backend servers with ports and weights\n3. **Configure Health Checks**: Set check interval, timeout, and healthy threshold\n4. **Set Up SSL/TLS**: Configure certificates and cipher suites\n5. **Define Routing Rules**: Create path-based or host-based routing\n6. **Enable Session Persistence**: Configure sticky sessions if needed\n7. **Add Monitoring**: Set up logging and metrics collection\n8. **Test Configuration**: Validate syntax and test traffic distribution\n\n## Output\n\n**Nginx Configuration:**\n```nginx\n# {baseDir}/nginx/load-balancer.conf\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Nginx documentation: https://nginx.org/en/docs/\n- HAProxy configuration guide: https://www.haproxy.org/\n- AWS ALB documentation: https://docs.aws.amazon.com/elasticloadbalancing/\n- GCP Load Balancing: https://cloud.google.com/load-balancing/docs\n- Example configurations in {baseDir}/lb-examples/",
      "parentPlugin": {
        "name": "load-balancer-configurator",
        "category": "devops",
        "path": "plugins/devops/load-balancer-configurator",
        "version": "1.0.0",
        "description": "Configure load balancers (ALB, NLB, Nginx, HAProxy)"
      },
      "filePath": "plugins/devops/load-balancer-configurator/skills/configuring-load-balancers/SKILL.md"
    },
    {
      "slug": "configuring-service-meshes",
      "name": "configuring-service-meshes",
      "description": "Configure this skill configures service meshes like istio and linkerd for microservices. it generates production-ready configurations, implements best practices, and ensures a security-first approach. use this skill when the user asks to \"configure service ... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Service Mesh Configurator\n\nThis skill provides automated assistance for service mesh configurator tasks.\n\n## Overview\n\nThis skill enables Claude to generate configurations and setup code for service meshes like Istio and Linkerd. It simplifies the process of deploying and managing microservices by automating the configuration of essential service mesh components.\n\n## How It Works\n\n1. **Requirement Gathering**: Claude identifies the specific service mesh (Istio or Linkerd) and infrastructure requirements from the user's request.\n2. **Configuration Generation**: Based on the requirements, Claude generates the necessary configuration files, including YAML manifests and setup scripts.\n3. **Code Delivery**: Claude provides the generated configurations and setup code to the user, ready for deployment.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Configure Istio for a microservices application.\n- Configure Linkerd for a microservices application.\n- Generate service mesh configurations based on specific infrastructure requirements.\n\n## Examples\n\n### Example 1: Setting up Istio\n\nUser request: \"Configure Istio for my Kubernetes microservices deployment with mTLS enabled.\"\n\nThe skill will:\n1. Generate Istio configuration files with mTLS enabled.\n2. Provide the generated YAML manifests and setup instructions.\n\n### Example 2: Configuring Linkerd\n\nUser request: \"Setup Linkerd on my existing microservices cluster, focusing on traffic splitting and observability.\"\n\nThe skill will:\n1. Generate Linkerd configuration files for traffic splitting and observability.\n2. Provide the generated YAML manifests and setup instructions.\n\n## Best Practices\n\n- **Security**: Always prioritize security configurations, such as mTLS, when configuring service meshes.\n- **Observability**: Ensure that the service mesh is configured for comprehensive observability, including metrics, tracing, and logging.\n- **Traffic Management**: Use traffic management features like traffic splitting and canary deployments to manage application updates safely.\n\n## Integration\n\nThis skill can be integrated with other DevOps tools and plugins in the Claude Code ecosystem to automate the deployment and management of microservices applications. For example, it can work with a Kubernetes deployment plugin to automatically deploy the generated configurations.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "service-mesh-configurator",
        "category": "devops",
        "path": "plugins/devops/service-mesh-configurator",
        "version": "1.0.0",
        "description": "Configure service mesh (Istio, Linkerd) for microservices"
      },
      "filePath": "plugins/devops/service-mesh-configurator/skills/configuring-service-meshes/SKILL.md"
    },
    {
      "slug": "creating-alerting-rules",
      "name": "creating-alerting-rules",
      "description": "Execute this skill enables AI assistant to create intelligent alerting rules for proactive performance monitoring. it is triggered when the user requests to \"create alerts\", \"define monitoring rules\", or \"set up alerting\". the skill helps define thresholds, rou... Use when generating or creating new content. Trigger with phrases like 'generate', 'create', or 'scaffold'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Alerting Rule Creator\n\nThis skill provides automated assistance for alerting rule creator tasks.\n\n## Overview\n\nThis skill automates the creation of comprehensive alerting rules, reducing the manual effort required for performance monitoring. It guides you through defining alert categories, setting intelligent thresholds, and configuring routing and escalation policies. The skill also helps generate runbooks and establish alert testing procedures.\n\n## How It Works\n\n1. **Identify Alert Category**: Determines the type of alert to create (e.g., latency, error rate, resource utilization).\n2. **Define Thresholds**: Sets appropriate thresholds to avoid alert fatigue and ensure timely notification of performance issues.\n3. **Configure Routing and Escalation**: Establishes routing policies to direct alerts to the appropriate teams and escalation policies for timely response.\n4. **Generate Runbook**: Creates a basic runbook with steps to diagnose and resolve the alerted issue.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Implement performance monitoring for a new service.\n- Refine existing alerting rules to reduce false positives.\n- Create alerts for specific performance metrics, such as latency or error rate.\n\n## Examples\n\n### Example 1: Setting up Latency Alerts\n\nUser request: \"create latency alerts for the payment service\"\n\nThe skill will:\n1. Prompt for latency thresholds (e.g., warning and critical).\n2. Configure alerts to trigger when latency exceeds defined thresholds.\n\n### Example 2: Creating Error Rate Alerts\n\nUser request: \"set up alerting for error rate increases in the API gateway\"\n\nThe skill will:\n1. Request the baseline error rate and acceptable deviation.\n2. Configure alerts to trigger when the error rate exceeds the defined deviation from the baseline.\n\n## Best Practices\n\n- **Threshold Selection**: Use historical data and statistical analysis to determine appropriate thresholds that minimize false positives and negatives.\n- **Alert Routing**: Route alerts to the appropriate teams or individuals based on the alert category and severity.\n- **Runbook Creation**: Generate or link to detailed runbooks that provide clear instructions for diagnosing and resolving the alerted issue.\n\n## Integration\n\nThis skill can be integrated with other Claude Code plugins to automate incident response workflows. For example, it can trigger automated remediation actions or create tickets in an issue tracking system.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "alerting-rule-creator",
        "category": "performance",
        "path": "plugins/performance/alerting-rule-creator",
        "version": "1.0.0",
        "description": "Create intelligent alerting rules for performance monitoring"
      },
      "filePath": "plugins/performance/alerting-rule-creator/skills/creating-alerting-rules/SKILL.md"
    },
    {
      "slug": "creating-ansible-playbooks",
      "name": "creating-ansible-playbooks",
      "description": "Execute use when you need to work with Ansible automation. This skill provides Ansible playbook creation with comprehensive guidance and automation. Trigger with phrases like \"create Ansible playbook\", \"automate with Ansible\", or \"configure with Ansible\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(ansible:*), Bash(terraform:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Ansible Playbook Creator\n\nThis skill provides automated assistance for ansible playbook creator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/ansible-playbook-creator/`\n\n**Documentation and Guides**: `{baseDir}/docs/ansible-playbook-creator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/ansible-playbook-creator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/ansible-playbook-creator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/ansible-playbook-creator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/ansible-playbook-creator-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "ansible-playbook-creator",
        "category": "devops",
        "path": "plugins/devops/ansible-playbook-creator",
        "version": "1.0.0",
        "description": "Create Ansible playbooks for configuration management"
      },
      "filePath": "plugins/devops/ansible-playbook-creator/skills/creating-ansible-playbooks/SKILL.md"
    },
    {
      "slug": "creating-apm-dashboards",
      "name": "creating-apm-dashboards",
      "description": "Execute this skill enables AI assistant to create application performance monitoring (apm) dashboards. it is triggered when the user requests the creation of a new apm dashboard, monitoring dashboard, or a dashboard for application performance. the skill helps ... Use when generating or creating new content. Trigger with phrases like 'generate', 'create', or 'scaffold'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Apm Dashboard Creator\n\nThis skill provides automated assistance for apm dashboard creator tasks.\n\n## Overview\n\nThis skill automates the creation of Application Performance Monitoring (APM) dashboards, providing a structured approach to visualizing critical application metrics. By defining key performance indicators and generating dashboard configurations, this skill simplifies the process of monitoring application health and performance.\n\n## How It Works\n\n1. **Identify Requirements**: Determine the specific metrics and visualizations needed for the APM dashboard based on the user's request.\n2. **Define Dashboard Components**: Select relevant components such as golden signals (latency, traffic, errors, saturation), request metrics, resource utilization, database metrics, cache metrics, business metrics, and error tracking.\n3. **Generate Configuration**: Create the dashboard configuration file based on the selected components and user preferences.\n4. **Deploy Dashboard**: Deploy the generated configuration to the target monitoring platform (e.g., Grafana, Datadog).\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a new APM dashboard for an application.\n- Define key metrics and visualizations for monitoring application performance.\n- Generate dashboard configurations for Grafana, Datadog, or other monitoring platforms.\n\n## Examples\n\n### Example 1: Creating a Grafana Dashboard\n\nUser request: \"Create a Grafana dashboard for monitoring my web application's performance.\"\n\nThe skill will:\n1. Identify the need for a Grafana dashboard focused on web application performance.\n2. Define dashboard components including request rate, response times, error rates, and resource utilization (CPU, memory).\n3. Generate a Grafana dashboard configuration file with pre-defined visualizations for these metrics.\n\n### Example 2: Setting up a Datadog Dashboard\n\nUser request: \"Set up a Datadog dashboard to track the golden signals for my microservice.\"\n\nThe skill will:\n1. Identify the need for a Datadog dashboard focused on golden signals.\n2. Define dashboard components including latency, traffic, errors, and saturation metrics.\n3. Generate a Datadog dashboard configuration file with pre-defined visualizations for these metrics.\n\n## Best Practices\n\n- **Specificity**: Provide detailed information about the application and metrics to be monitored.\n- **Platform Selection**: Clearly specify the target monitoring platform (Grafana, Datadog, etc.) to ensure compatibility.\n- **Iteration**: Review and refine the generated dashboard configuration to meet specific monitoring needs.\n\n## Integration\n\nThis skill can be integrated with other plugins that manage infrastructure or application deployment to automatically create APM dashboards as part of the deployment process. It can also work with alerting plugins to define alert rules based on the metrics displayed in the generated dashboards.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "apm-dashboard-creator",
        "category": "performance",
        "path": "plugins/performance/apm-dashboard-creator",
        "version": "1.0.0",
        "description": "Create Application Performance Monitoring dashboards"
      },
      "filePath": "plugins/performance/apm-dashboard-creator/skills/creating-apm-dashboards/SKILL.md"
    },
    {
      "slug": "creating-data-visualizations",
      "name": "creating-data-visualizations",
      "description": "Generate plots, charts, and graphs from data with automatic visualization type selection. Use when requesting \"visualization\", \"plot\", \"chart\", or \"graph\". Trigger with phrases like 'generate', 'create', or 'scaffold'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Data Visualization Creator\n\nThis skill provides automated assistance for data visualization creator tasks.\n\n## Overview\n\nThis skill empowers Claude to transform raw data into compelling visual representations. It leverages intelligent automation to select optimal visualization types and generate informative plots, charts, and graphs. This skill helps users understand complex data more easily.\n\n## How It Works\n\n1. **Data Analysis**: Claude analyzes the provided data to understand its structure, type, and distribution.\n2. **Visualization Selection**: Based on the data analysis, Claude selects the most appropriate visualization type (e.g., bar chart, scatter plot, line graph).\n3. **Visualization Generation**: Claude generates the visualization using appropriate libraries and best practices for visual clarity and accuracy.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a visual representation of data.\n- Generate a specific type of plot, chart, or graph (e.g., \"create a bar chart\").\n- Explore data patterns and relationships through visualization.\n\n## Examples\n\n### Example 1: Visualizing Sales Data\n\nUser request: \"Create a bar chart showing sales by region.\"\n\nThe skill will:\n1. Analyze the sales data, identifying regions and corresponding sales figures.\n2. Generate a bar chart with regions on the x-axis and sales on the y-axis.\n\n### Example 2: Plotting Stock Prices\n\nUser request: \"Plot the stock price of AAPL over the last year.\"\n\nThe skill will:\n1. Retrieve historical stock price data for AAPL.\n2. Generate a line graph showing the stock price over time.\n\n## Best Practices\n\n- **Data Clarity**: Ensure the data is clean and well-formatted before requesting a visualization.\n- **Specific Requests**: Be specific about the desired visualization type and any relevant data filters.\n- **Contextual Information**: Provide context about the data and the purpose of the visualization.\n\n## Integration\n\nThis skill can be integrated with other data processing and analysis tools within the Claude Code environment. It can receive data from other skills and provide visualizations for further analysis or reporting.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "data-visualization-creator",
        "category": "ai-ml",
        "path": "plugins/ai-ml/data-visualization-creator",
        "version": "1.0.0",
        "description": "Create data visualizations and plots"
      },
      "filePath": "plugins/ai-ml/data-visualization-creator/skills/creating-data-visualizations/SKILL.md"
    },
    {
      "slug": "creating-github-issues-from-web-research",
      "name": "creating-github-issues-from-web-research",
      "description": "Execute this skill enhances AI assistant's ability to conduct web research and translate findings into actionable github issues. it automates the process of extracting key information from web search results and formatting it into a well-structured issue, ready... Use when managing version control. Trigger with phrases like 'commit', 'branch', or 'git'. allowed-tools: Read, WebFetch, WebSearch, Grep version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins Team <hello@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Web To Github Issue\n\nThis skill provides automated assistance for web to github issue tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for web to github issue tasks.\nThis skill empowers Claude to streamline the research-to-implementation workflow. By integrating web search with GitHub issue creation, Claude can efficiently convert research findings into trackable tasks for development teams.\n\n## How It Works\n\n1. **Web Search**: Claude utilizes its web search capabilities to gather information on the specified topic.\n2. **Information Extraction**: The plugin extracts relevant details, key findings, and supporting evidence from the search results.\n3. **GitHub Issue Creation**: A new GitHub issue is created with a clear title, a summary of the research, key recommendations, and links to the original sources.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Investigate a technical topic and create an implementation ticket.\n- Track security vulnerabilities and generate a security issue with remediation steps.\n- Research competitor features and create a feature request ticket.\n\n## Examples\n\n### Example 1: Researching Security Best Practices\n\nUser request: \"research Docker security best practices and create a ticket in myorg/backend\"\n\nThe skill will:\n1. Search the web for Docker security best practices.\n2. Extract key recommendations, security vulnerabilities, and mitigation strategies.\n3. Create a GitHub issue in the specified repository with a summary of the findings, a checklist of best practices, and links to relevant resources.\n\n### Example 2: Investigating API Rate Limiting\n\nUser request: \"find articles about API rate limiting, create issue with label performance\"\n\nThe skill will:\n1. Search the web for articles and documentation on API rate limiting.\n2. Extract different rate limiting techniques, their pros and cons, and implementation examples.\n3. Create a GitHub issue with the \"performance\" label, summarizing the findings and providing links to the source articles.\n\n## Best Practices\n\n- **Specify Repository**: When creating issues for a specific project, explicitly mention the repository name to ensure the issue is created in the correct location.\n- **Use Labels**: Add relevant labels to the issue to categorize it appropriately and facilitate issue tracking.\n- **Provide Context**: Include sufficient context in your request to guide the web search and ensure the generated issue contains the most relevant information.\n\n## Integration\n\nThis skill seamlessly integrates with Claude's web search Skill and requires authentication with a GitHub account. It can be used in conjunction with other skills to further automate development workflows.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "web-to-github-issue",
        "category": "skill-enhancers",
        "path": "plugins/skill-enhancers/web-to-github-issue",
        "version": "1.0.0",
        "description": "Enhances web_search Skill by automatically creating GitHub issues from research findings"
      },
      "filePath": "plugins/skill-enhancers/web-to-github-issue/skills/creating-github-issues-from-web-research/SKILL.md"
    },
    {
      "slug": "creating-kubernetes-deployments",
      "name": "creating-kubernetes-deployments",
      "description": "Deploy use when generating Kubernetes deployment manifests and services. Trigger with phrases like \"create kubernetes deployment\", \"generate k8s manifest\", \"deploy app to kubernetes\", or \"create service and ingress\". Produces production-ready YAML with health checks, auto-scaling, resource limits, ingress configuration, and TLS termination. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Creating Kubernetes Deployments\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Kubernetes cluster is accessible and kubectl is configured\n- Container image is built and pushed to registry\n- Understanding of application resource requirements\n- Namespace exists or will be created\n- Ingress controller is installed (if using ingress)\n- TLS certificates are available (if using HTTPS)\n\n## Instructions\n\n1. **Gather Requirements**: Application name, image, replicas, ports, environment\n2. **Create Deployment**: Generate YAML with container spec and resource limits\n3. **Add Health Checks**: Configure liveness and readiness probes\n4. **Define Service**: Create ClusterIP, NodePort, or LoadBalancer service\n5. **Configure Ingress**: Set up routing rules and TLS termination\n6. **Add ConfigMaps/Secrets**: Externalize configuration and sensitive data\n7. **Enable Auto-scaling**: Create HorizontalPodAutoscaler if needed\n8. **Apply Manifests**: Use kubectl apply to deploy resources\n\n## Output\n\n**Deployment Manifest:**\n```yaml\n# {baseDir}/k8s/deployment.yaml\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Kubernetes documentation: https://kubernetes.io/docs/\n- kubectl reference: https://kubernetes.io/docs/reference/kubectl/\n- Deployment best practices: https://kubernetes.io/docs/concepts/workloads/\n- Example manifests in {baseDir}/k8s-examples/",
      "parentPlugin": {
        "name": "kubernetes-deployment-creator",
        "category": "devops",
        "path": "plugins/devops/kubernetes-deployment-creator",
        "version": "1.0.0",
        "description": "Create Kubernetes deployments, services, and configurations with best practices"
      },
      "filePath": "plugins/devops/kubernetes-deployment-creator/skills/creating-kubernetes-deployments/SKILL.md"
    },
    {
      "slug": "creating-webhook-handlers",
      "name": "creating-webhook-handlers",
      "description": "Create webhook endpoints with signature verification, retry logic, and payload validation. Use when receiving and processing webhook events. Trigger with phrases like \"create webhook\", \"handle webhook events\", or \"setup webhook handler\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:webhook-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Creating Webhook Handlers\n\n## Overview\n\n\nThis skill provides automated assistance for webhook handler creator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:webhook-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "webhook-handler-creator",
        "category": "api-development",
        "path": "plugins/api-development/webhook-handler-creator",
        "version": "1.0.0",
        "description": "Create secure webhook endpoints with signature verification and retry logic"
      },
      "filePath": "plugins/api-development/webhook-handler-creator/skills/creating-webhook-handlers/SKILL.md"
    },
    {
      "slug": "database-documentation-gen",
      "name": "database-documentation-gen",
      "description": "Process use when you need to work with database documentation. This skill provides automated documentation generation with comprehensive guidance and automation. Trigger with phrases like \"generate docs\", \"document schema\", or \"create database documentation\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Documentation Gen\n\nThis skill provides automated assistance for database documentation gen tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-documentation-gen/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-documentation-gen/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-documentation-gen/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-documentation-gen-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-documentation-gen-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-documentation-gen-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-documentation-gen",
        "category": "database",
        "path": "plugins/database/database-documentation-gen",
        "version": "1.0.0",
        "description": "Database plugin for database-documentation-gen"
      },
      "filePath": "plugins/database/database-documentation-gen/skills/database-documentation-gen/SKILL.md"
    },
    {
      "slug": "deploying-machine-learning-models",
      "name": "deploying-machine-learning-models",
      "description": "Deploy this skill enables AI assistant to deploy machine learning models to production environments. it automates the deployment workflow, implements best practices for serving models, optimizes performance, and handles potential errors. use this skill when th... Use when deploying or managing infrastructure. Trigger with phrases like 'deploy', 'infrastructure', or 'CI/CD'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Model Deployment Helper\n\nThis skill provides automated assistance for model deployment helper tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for model deployment helper tasks.\nThis skill streamlines the process of deploying machine learning models to production, ensuring efficient and reliable model serving. It leverages automated workflows and best practices to simplify the deployment process and optimize performance.\n\n## How It Works\n\n1. **Analyze Requirements**: The skill analyzes the context and user requirements to determine the appropriate deployment strategy.\n2. **Generate Code**: It generates the necessary code for deploying the model, including API endpoints, data validation, and error handling.\n3. **Deploy Model**: The skill deploys the model to the specified production environment.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Deploy a trained machine learning model to a production environment.\n- Serve a model via an API endpoint for real-time predictions.\n- Automate the model deployment process.\n\n## Examples\n\n### Example 1: Deploying a Regression Model\n\nUser request: \"Deploy my regression model trained on the housing dataset.\"\n\nThe skill will:\n1. Analyze the model and data format.\n2. Generate code for a REST API endpoint to serve the model.\n3. Deploy the model to a cloud-based serving platform.\n\n### Example 2: Productionizing a Classification Model\n\nUser request: \"Productionize the classification model I just trained.\"\n\nThe skill will:\n1. Create a Docker container for the model.\n2. Implement data validation and error handling.\n3. Deploy the container to a Kubernetes cluster.\n\n## Best Practices\n\n- **Data Validation**: Implement thorough data validation to ensure the model receives correct inputs.\n- **Error Handling**: Include robust error handling to gracefully manage unexpected issues.\n- **Performance Monitoring**: Set up performance monitoring to track model latency and throughput.\n\n## Integration\n\nThis skill can be integrated with other tools for model training, data preprocessing, and monitoring.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "model-deployment-helper",
        "category": "ai-ml",
        "path": "plugins/ai-ml/model-deployment-helper",
        "version": "1.0.0",
        "description": "Deploy ML models to production"
      },
      "filePath": "plugins/ai-ml/model-deployment-helper/skills/deploying-machine-learning-models/SKILL.md"
    },
    {
      "slug": "deploying-monitoring-stacks",
      "name": "deploying-monitoring-stacks",
      "description": "Monitor use when deploying monitoring stacks including Prometheus, Grafana, and Datadog. Trigger with phrases like \"deploy monitoring stack\", \"setup prometheus\", \"configure grafana\", or \"install datadog agent\". Generates production-ready configurations with metric collection, visualization dashboards, and alerting rules. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(docker:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Monitoring Stack Deployer\n\nThis skill provides automated assistance for monitoring stack deployer tasks.\n\n## Overview\n\nDeploys monitoring stacks (Prometheus/Grafana/Datadog) including collectors, scraping config, dashboards, and alerting rules for production systems.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Target infrastructure is identified (Kubernetes, Docker, bare metal)\n- Metric endpoints are accessible from monitoring platform\n- Storage backend is configured for time-series data\n- Alert notification channels are defined (email, Slack, PagerDuty)\n- Resource requirements are calculated based on scale\n\n## Instructions\n\n1. **Select Platform**: Choose Prometheus/Grafana, Datadog, or hybrid approach\n2. **Deploy Collectors**: Install exporters and agents on monitored systems\n3. **Configure Scraping**: Define metric collection endpoints and intervals\n4. **Set Up Storage**: Configure retention policies and data compaction\n5. **Create Dashboards**: Build visualization panels for key metrics\n6. **Define Alerts**: Create alerting rules with appropriate thresholds\n7. **Test Monitoring**: Verify metrics flow and alert triggering\n\n## Output\n\n**Prometheus + Grafana (Kubernetes):**\n```yaml\n# {baseDir}/monitoring/prometheus.yaml\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-config\ndata:\n  prometheus.yml: |\n    global:\n      scrape_interval: 15s\n      evaluation_interval: 15s\n    scrape_configs:\n      - job_name: 'kubernetes-pods'\n        kubernetes_sd_configs:\n          - role: pod\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus:latest\n        args:\n          - '--config.file=/etc/prometheus/prometheus.yml'\n          - '--storage.tsdb.retention.time=30d'\n        ports:\n        - containerPort: 9090\n```\n\n**Grafana Dashboard Configuration:**\n```json\n{\n  \"dashboard\": {\n    \"title\": \"Application Metrics\",\n    \"panels\": [\n      {\n        \"title\": \"CPU Usage\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(container_cpu_usage_seconds_total[5m])\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n## Error Handling\n\n**Metrics Not Appearing**\n- Error: \"No data points\"\n- Solution: Verify scrape targets are accessible and returning metrics\n\n**High Cardinality**\n- Error: \"Too many time series\"\n- Solution: Reduce label combinations or increase Prometheus resources\n\n**Alert Not Firing**\n- Error: \"Alert condition met but no notification\"\n- Solution: Check Alertmanager configuration and notification channels\n\n**Dashboard Load Failure**\n- Error: \"Failed to load dashboard\"\n- Solution: Verify Grafana datasource configuration and permissions\n\n## Examples\n\n- \"Deploy Prometheus + Grafana on Kubernetes and add alerts for high error rate and latency.\"\n- \"Install Datadog agents across hosts and configure a dashboard for CPU/memory saturation.\"\n\n## Resources\n\n- Prometheus documentation: https://prometheus.io/docs/\n- Grafana documentation: https://grafana.com/docs/\n- Example dashboards in {baseDir}/monitoring-examples/",
      "parentPlugin": {
        "name": "monitoring-stack-deployer",
        "category": "devops",
        "path": "plugins/devops/monitoring-stack-deployer",
        "version": "1.0.0",
        "description": "Deploy monitoring stacks (Prometheus, Grafana, Datadog)"
      },
      "filePath": "plugins/devops/monitoring-stack-deployer/skills/deploying-monitoring-stacks/SKILL.md"
    },
    {
      "slug": "designing-database-schemas",
      "name": "designing-database-schemas",
      "description": "Process use when you need to work with database schema design. This skill provides schema design and migrations with comprehensive guidance and automation. Trigger with phrases like \"design schema\", \"create migration\", or \"model database\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Schema Designer\n\nThis skill provides automated assistance for database schema designer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-schema-designer/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-schema-designer/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-schema-designer/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-schema-designer-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-schema-designer-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-schema-designer-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-schema-designer",
        "category": "database",
        "path": "plugins/database/database-schema-designer",
        "version": "1.0.0",
        "description": "Design and visualize database schemas with normalization guidance, relationship mapping, and ERD generation"
      },
      "filePath": "plugins/database/database-schema-designer/skills/designing-database-schemas/SKILL.md"
    },
    {
      "slug": "detecting-data-anomalies",
      "name": "detecting-data-anomalies",
      "description": "Process identify anomalies and outliers in datasets using machine learning algorithms. Use when analyzing data for unusual patterns, outliers, or unexpected deviations from normal behavior. Trigger with phrases like \"detect anomalies\", \"find outliers\", or \"identify unusual patterns\". allowed-tools: Read, Bash(python:*), Grep, Glob version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Detecting Data Anomalies\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Dataset in accessible format (CSV, JSON, or database)\n- Python environment with scikit-learn or similar ML libraries\n- Understanding of data distribution and expected patterns\n- Sufficient data volume for statistical significance\n- Knowledge of domain-specific normal behavior\n- Data preprocessing capabilities for cleaning and scaling\n\n## Instructions\n\n1. Load dataset using Read tool\n2. Inspect data structure and identify relevant features\n3. Clean data by handling missing values and inconsistencies\n4. Normalize or scale features as appropriate for algorithm\n5. Split temporal data if time-series analysis is needed\n1. Apply selected algorithm using Bash tool\n2. Generate anomaly scores for each data point\n3. Classify points as normal or anomalous based on threshold\n4. Extract characteristics of identified anomalies\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Total data points analyzed\n- Number of anomalies detected\n- Contamination rate (percentage of anomalies)\n- Algorithm used and configuration parameters\n- Confidence scores for detected anomalies\n- Record identifier and timestamp (if applicable)\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Isolation Forest documentation and implementation examples\n- One-Class SVM for novelty detection\n- Local Outlier Factor (LOF) for density-based detection\n- Autoencoder-based anomaly detection for deep learning approaches\n- scikit-learn anomaly detection module",
      "parentPlugin": {
        "name": "anomaly-detection-system",
        "category": "ai-ml",
        "path": "plugins/ai-ml/anomaly-detection-system",
        "version": "1.0.0",
        "description": "Detect anomalies and outliers in data"
      },
      "filePath": "plugins/ai-ml/anomaly-detection-system/skills/detecting-data-anomalies/SKILL.md"
    },
    {
      "slug": "detecting-database-deadlocks",
      "name": "detecting-database-deadlocks",
      "description": "Process use when you need to work with deadlock detection. This skill provides deadlock detection and resolution with comprehensive guidance and automation. Trigger with phrases like \"detect deadlocks\", \"resolve deadlocks\", or \"prevent deadlocks\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Deadlock Detector\n\nThis skill provides automated assistance for database deadlock detector tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-deadlock-detector/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-deadlock-detector/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-deadlock-detector/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-deadlock-detector-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-deadlock-detector-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-deadlock-detector-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-deadlock-detector",
        "category": "database",
        "path": "plugins/database/database-deadlock-detector",
        "version": "1.0.0",
        "description": "Database plugin for database-deadlock-detector"
      },
      "filePath": "plugins/database/database-deadlock-detector/skills/detecting-database-deadlocks/SKILL.md"
    },
    {
      "slug": "detecting-infrastructure-drift",
      "name": "detecting-infrastructure-drift",
      "description": "Execute use when detecting infrastructure drift from desired state. Trigger with phrases like \"check for drift\", \"infrastructure drift detection\", \"compare actual vs desired state\", or \"detect configuration changes\". Identifies discrepancies between current infrastructure and IaC definitions using terraform plan, cloudformation drift detection, or manual comparison. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(terraform:*), Bash(aws:*), Bash(gcloud:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Infrastructure Drift Detector\n\nThis skill provides automated assistance for infrastructure drift detector tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Infrastructure as Code (IaC) files are up to date in {baseDir}\n- Cloud provider CLI is installed and authenticated\n- IaC tool (Terraform/CloudFormation/Pulumi) is installed\n- Remote state storage is configured and accessible\n- Appropriate read permissions for infrastructure resources\n\n## Instructions\n\n1. **Identify IaC Tool**: Determine if using Terraform, CloudFormation, Pulumi, or ARM\n2. **Fetch Current State**: Retrieve actual infrastructure state from cloud provider\n3. **Load Desired State**: Read IaC configuration from {baseDir}/terraform or equivalent\n4. **Compare States**: Execute drift detection command for the IaC platform\n5. **Analyze Differences**: Identify added, modified, or removed resources\n6. **Generate Report**: Create detailed report of drift with affected resources\n7. **Suggest Remediation**: Provide commands to resolve drift (apply or import)\n8. **Document Findings**: Save drift report to {baseDir}/drift-reports/\n\n## Output\n\nGenerates drift detection reports:\n\n**Terraform Drift Report:**\n```\nDrift Detection Report - 2025-12-10 10:30:00\n==============================================\n\nResources with Drift: 3\n\n1. aws_instance.web_server\n   Status: Modified\n   Drift: instance_type changed from \"t3.micro\" to \"t3.small\"\n   Action: Update IaC to match or revert instance type\n\n2. aws_s3_bucket.assets\n   Status: Modified\n   Drift: versioning_enabled changed from true to false\n   Action: Re-enable versioning or update IaC\n\n3. aws_iam_role.lambda_exec\n   Status: Deleted\n   Drift: Role no longer exists in AWS\n   Action: terraform apply to recreate\n\nRemediation Command:\nterraform plan -out=drift-fix.tfplan\nterraform apply drift-fix.tfplan\n```\n\n**CloudFormation Drift:**\n```yaml\nStackName: production-vpc\nDriftStatus: DRIFTED\nResources:\n  - LogicalResourceId: VPC\n    ResourceType: AWS::EC2::VPC\n    DriftStatus: IN_SYNC\n  - LogicalResourceId: PublicSubnet\n    ResourceType: AWS::EC2::Subnet\n    DriftStatus: MODIFIED\n    PropertyDifferences:\n      - PropertyPath: /Tags\n        ExpectedValue: [{Key: Env, Value: prod}]\n        ActualValue: [{Key: Env, Value: production}]\n```\n\n## Error Handling\n\nCommon issues and solutions:\n\n**State Lock Error**\n- Error: \"Error acquiring state lock\"\n- Solution: Ensure no other terraform process is running, or force-unlock if safe\n\n**Authentication Failure**\n- Error: \"Unable to authenticate to cloud provider\"\n- Solution: Refresh credentials with `aws configure` or `gcloud auth login`\n\n**Missing State File**\n- Error: \"No state file found\"\n- Solution: Initialize terraform with `terraform init` or specify remote backend\n\n**Permission Denied**\n- Error: \"Access denied reading resource\"\n- Solution: Grant read-only IAM permissions to service account\n\n**State Version Mismatch**\n- Error: \"State file version too new\"\n- Solution: Upgrade Terraform version or use compatible state version\n\n## Resources\n\n- Terraform drift documentation: https://www.terraform.io/docs/cli/state/\n- AWS CloudFormation drift detection: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/detect-drift-stack.html\n- Drift remediation best practices in {baseDir}/docs/drift-remediation.md\n- Automated drift detection scripts in {baseDir}/scripts/drift-check.sh\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "infrastructure-drift-detector",
        "category": "devops",
        "path": "plugins/devops/infrastructure-drift-detector",
        "version": "1.0.0",
        "description": "Detect infrastructure drift from desired state"
      },
      "filePath": "plugins/devops/infrastructure-drift-detector/skills/detecting-infrastructure-drift/SKILL.md"
    },
    {
      "slug": "detecting-memory-leaks",
      "name": "detecting-memory-leaks",
      "description": "Detect potential memory leaks and analyze memory usage patterns in code. Use when troubleshooting performance issues related to memory growth or identifying leak sources. Trigger with phrases like \"detect memory leaks\", \"analyze memory usage\", or \"find memory issues\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(profiling:*)",
        "Bash(memory:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Memory Leak Detector\n\nThis skill provides automated assistance for memory leak detector tasks.\n\n## Overview\n\nThis skill helps you identify and resolve memory leaks in your code. By analyzing your code for common memory leak patterns, it can help you improve the performance and stability of your application.\n\n## How It Works\n\n1. **Initiate Analysis**: The user requests memory leak detection.\n2. **Code Analysis**: The plugin analyzes the codebase for potential memory leak patterns.\n3. **Report Generation**: The plugin generates a report detailing potential memory leaks and recommended fixes.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Detect potential memory leaks in your application.\n- Analyze memory usage patterns to identify performance bottlenecks.\n- Troubleshoot performance issues related to memory leaks.\n\n## Examples\n\n### Example 1: Identifying Event Listener Leaks\n\nUser request: \"detect memory leaks in my event handling code\"\n\nThe skill will:\n1. Analyze the code for unremoved event listeners.\n2. Generate a report highlighting potential event listener leaks and suggesting how to properly remove them.\n\n### Example 2: Analyzing Cache Growth\n\nUser request: \"analyze memory usage to find excessive cache growth\"\n\nThe skill will:\n1. Analyze cache implementations for unbounded growth.\n2. Identify caches that are not properly managed and recommend strategies for limiting their size.\n\n## Best Practices\n\n- **Code Review**: Always review the reported potential leaks to ensure they are genuine issues.\n- **Regular Analysis**: Incorporate memory leak detection into your regular development workflow.\n- **Targeted Analysis**: Focus your analysis on specific areas of your code that are known to be memory-intensive.\n\n## Integration\n\nThis skill can be used in conjunction with other performance analysis tools to provide a comprehensive view of application performance.\n\n## Prerequisites\n\n- Access to application source code in {baseDir}/\n- Memory profiling tools (valgrind, heapdump, etc.)\n- Understanding of application memory architecture\n- Runtime environment for testing\n\n## Instructions\n\n1. Analyze code for common memory leak patterns\n2. Identify unremoved event listeners and callbacks\n3. Check for unbounded cache growth\n4. Review closure usage and retained references\n5. Generate report with leak locations and severity\n6. Provide remediation recommendations\n\n## Output\n\n- Memory leak detection report with file locations\n- Pattern analysis for event listeners and caches\n- Memory usage trends and growth patterns\n- Code snippets highlighting potential leaks\n- Recommended fixes with code examples\n\n## Error Handling\n\nIf memory leak detection fails:\n- Verify code file access permissions\n- Check profiling tool installation\n- Validate code syntax and structure\n- Ensure sufficient memory for analysis\n- Review runtime environment configuration\n\n## Resources\n\n- Memory profiling tool documentation\n- Memory leak detection best practices\n- JavaScript/Node.js memory management guides\n- Performance optimization resources",
      "parentPlugin": {
        "name": "memory-leak-detector",
        "category": "performance",
        "path": "plugins/performance/memory-leak-detector",
        "version": "1.0.0",
        "description": "Detect memory leaks and analyze memory usage patterns"
      },
      "filePath": "plugins/performance/memory-leak-detector/skills/detecting-memory-leaks/SKILL.md"
    },
    {
      "slug": "detecting-performance-bottlenecks",
      "name": "detecting-performance-bottlenecks",
      "description": "Execute this skill enables AI assistant to detect and resolve performance bottlenecks in applications. it analyzes cpu, memory, i/o, and database performance to identify areas of concern. use this skill when you need to diagnose slow application performance, op... Use when optimizing performance. Trigger with phrases like 'optimize', 'performance', or 'speed up'. allowed-tools: Read, Bash(cmd:*), Grep, Glob version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Bottleneck Detector\n\nThis skill provides automated assistance for bottleneck detector tasks.\n\n## Overview\n\nThis skill empowers Claude to identify and address performance bottlenecks across different layers of an application. By pinpointing performance issues in CPU, memory, I/O, and database operations, it assists in optimizing resource utilization and improving overall application speed and responsiveness.\n\n## How It Works\n\n1. **Architecture Analysis**: Claude analyzes the application's architecture and data flow to understand potential bottlenecks.\n2. **Bottleneck Identification**: The plugin identifies bottlenecks across CPU, memory, I/O, database, lock contention, and resource exhaustion.\n3. **Remediation Suggestions**: Claude provides remediation strategies with code examples to resolve the identified bottlenecks.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Diagnose slow application performance.\n- Optimize resource usage (CPU, memory, I/O, database).\n- Proactively prevent performance issues.\n\n## Examples\n\n### Example 1: Diagnosing Slow Database Queries\n\nUser request: \"detect bottlenecks in my database queries\"\n\nThe skill will:\n1. Analyze database query performance and identify slow-running queries.\n2. Suggest optimizations like indexing or query rewriting to improve database performance.\n\n### Example 2: Identifying Memory Leaks\n\nUser request: \"analyze performance and find memory leaks\"\n\nThe skill will:\n1. Profile memory usage patterns to identify potential memory leaks.\n2. Provide code examples and recommendations to fix the memory leaks.\n\n## Best Practices\n\n- **Comprehensive Analysis**: Always analyze all potential bottleneck areas (CPU, memory, I/O, database) for a complete picture.\n- **Prioritize by Severity**: Focus on addressing the most severe bottlenecks first for maximum impact.\n- **Test Thoroughly**: After implementing remediation strategies, thoroughly test the application to ensure the bottlenecks are resolved and no new issues are introduced.\n\n## Integration\n\nThis skill can be used in conjunction with code generation plugins to automatically implement the suggested remediation strategies. It also integrates with monitoring and logging tools to provide real-time performance data.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "bottleneck-detector",
        "category": "performance",
        "path": "plugins/performance/bottleneck-detector",
        "version": "1.0.0",
        "description": "Detect and resolve performance bottlenecks"
      },
      "filePath": "plugins/performance/bottleneck-detector/skills/detecting-performance-bottlenecks/SKILL.md"
    },
    {
      "slug": "detecting-performance-regressions",
      "name": "detecting-performance-regressions",
      "description": "Automatically detect performance regressions in CI/CD pipelines by comparing metrics against baselines. Use when validating builds or analyzing performance trends. Trigger with phrases like \"detect performance regression\", \"compare performance metrics\", or \"analyze performance degradation\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(ci:*)",
        "Bash(metrics:*)",
        "Bash(testing:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Performance Regression Detector\n\nThis skill provides automated assistance for performance regression detector tasks.\n\n## Overview\n\nThis skill automates the detection of performance regressions within a CI/CD pipeline. It utilizes various methods, including baseline comparison, statistical analysis, and threshold violation checks, to identify performance degradation. The skill provides insights into potential performance bottlenecks and helps maintain application performance.\n\n## How It Works\n\n1. **Analyze Performance Data**: The plugin gathers performance metrics from the CI/CD environment.\n2. **Detect Regressions**: It employs methods like baseline comparison, statistical analysis, and threshold checks to detect regressions.\n3. **Report Findings**: The plugin generates a report summarizing the detected performance regressions and their potential impact.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify performance regressions in a CI/CD pipeline.\n- Analyze performance metrics for potential degradation.\n- Compare current performance against historical baselines.\n\n## Examples\n\n### Example 1: Identifying a Response Time Regression\n\nUser request: \"Detect performance regressions in the latest build. Specifically, check for increases in response time.\"\n\nThe skill will:\n1. Analyze response time metrics from the latest build.\n2. Compare the response times against a historical baseline.\n3. Report any statistically significant increases in response time that exceed a defined threshold.\n\n### Example 2: Detecting Throughput Degradation\n\nUser request: \"Analyze throughput for performance regressions after the recent code merge.\"\n\nThe skill will:\n1. Gather throughput data (requests per second) from the post-merge CI/CD run.\n2. Compare the throughput to pre-merge values, looking for statistically significant drops.\n3. Generate a report highlighting any throughput degradation, indicating a potential performance regression.\n\n## Best Practices\n\n- **Define Baselines**: Establish clear and representative performance baselines for accurate comparison.\n- **Set Thresholds**: Configure appropriate thresholds for identifying significant performance regressions.\n- **Monitor Key Metrics**: Focus on monitoring critical performance metrics relevant to the application's behavior.\n\n## Integration\n\nThis skill can be integrated with other CI/CD tools to automatically trigger regression detection upon new builds or code merges. It can also be combined with reporting plugins to generate detailed performance reports.\n\n## Prerequisites\n\n- Historical performance baselines in {baseDir}/performance/baselines/\n- Access to CI/CD performance metrics\n- Statistical analysis tools\n- Defined regression thresholds\n\n## Instructions\n\n1. Collect performance metrics from current build\n2. Load historical baseline data\n3. Apply statistical analysis to detect significant changes\n4. Check for threshold violations\n5. Identify specific regressed metrics\n6. Generate regression report with root cause analysis\n\n## Output\n\n- Performance regression detection report\n- Statistical comparison with baselines\n- List of regressed metrics with severity\n- Visualization of performance trends\n- Recommendations for investigation\n\n## Error Handling\n\nIf regression detection fails:\n- Verify baseline data availability\n- Check metrics collection configuration\n- Validate statistical analysis parameters\n- Ensure threshold definitions are valid\n- Review CI/CD integration setup\n\n## Resources\n\n- Statistical process control for performance testing\n- CI/CD performance testing best practices\n- Regression detection algorithms\n- Performance monitoring strategies",
      "parentPlugin": {
        "name": "performance-regression-detector",
        "category": "performance",
        "path": "plugins/performance/performance-regression-detector",
        "version": "1.0.0",
        "description": "Detect performance regressions in CI/CD pipeline"
      },
      "filePath": "plugins/performance/performance-regression-detector/skills/detecting-performance-regressions/SKILL.md"
    },
    {
      "slug": "detecting-sql-injection-vulnerabilities",
      "name": "detecting-sql-injection-vulnerabilities",
      "description": "Detect and analyze SQL injection vulnerabilities in application code and database queries. Use when you need to scan code for SQL injection risks, review query construction, validate input sanitization, or implement secure query patterns. Trigger with phrases like \"detect SQL injection\", \"scan for SQLi vulnerabilities\", \"review database queries\", or \"check SQL security\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(code-scan:*), Bash(security-test:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Detecting Sql Injection Vulnerabilities\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Application source code accessible in {baseDir}/\n- Database query files and ORM configurations available\n- Framework information (Django, Rails, Express, Spring, etc.)\n- Write permissions for security reports in {baseDir}/security-reports/\n\n## Instructions\n\n1. Identify input surfaces and data flows into database queries.\n2. Review query construction and parameterization patterns.\n3. Flag injection vectors and document impact.\n4. Recommend fixes (parameterized queries, ORM patterns, validation) and tests.\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: SQL injection vulnerability report saved to {baseDir}/security-reports/sqli-scan-YYYYMMDD.md\n\n**Report Structure**:\n```\n# SQL Injection Vulnerability Report\nScan Date: 2024-01-15\nApplication: E-commerce Platform\nFramework: Django 4.2\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- SQL Injection Prevention Cheat Sheet: https://cheatsheetseries.owasp.org/cheatsheets/SQL_Injection_Prevention_Cheat_Sheet.html\n- OWASP Top 10 - Injection: https://owasp.org/www-project-top-ten/\n- CWE-89: SQL Injection: https://cwe.mitre.org/data/definitions/89.html\n- CAPEC-66: SQL Injection: https://capec.mitre.org/data/definitions/66.html\n- Django Security: https://docs.djangoproject.com/en/stable/topics/security/",
      "parentPlugin": {
        "name": "sql-injection-detector",
        "category": "security",
        "path": "plugins/security/sql-injection-detector",
        "version": "1.0.0",
        "description": "Detect SQL injection vulnerabilities"
      },
      "filePath": "plugins/security/sql-injection-detector/skills/detecting-sql-injection-vulnerabilities/SKILL.md"
    },
    {
      "slug": "emitting-api-events",
      "name": "emitting-api-events",
      "description": "Build event-driven APIs with webhooks, Server-Sent Events, and real-time notifications. Use when building event-driven API architectures. Trigger with phrases like \"add webhooks\", \"implement events\", or \"create event-driven API\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:events-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Emitting Api Events\n\n## Overview\n\n\nThis skill provides automated assistance for api event emitter tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:events-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-event-emitter",
        "category": "api-development",
        "path": "plugins/api-development/api-event-emitter",
        "version": "1.0.0",
        "description": "Implement event-driven APIs with message queues and event streaming"
      },
      "filePath": "plugins/api-development/api-event-emitter/skills/emitting-api-events/SKILL.md"
    },
    {
      "slug": "encrypting-and-decrypting-data",
      "name": "encrypting-and-decrypting-data",
      "description": "Validate encryption implementations and cryptographic practices. Use when reviewing data security measures. Trigger with 'check encryption', 'validate crypto', or 'review security keys'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Encryption Tool\n\nThis skill provides automated assistance for encryption tool tasks.\n\n## Overview\n\nThis skill empowers Claude to handle data encryption and decryption tasks seamlessly. It leverages the encryption-tool plugin to provide a secure way to protect sensitive information, ensuring confidentiality and integrity.\n\n## How It Works\n\n1. **Identify Encryption/Decryption Request**: Claude analyzes the user's request to determine whether encryption or decryption is required.\n2. **Select Encryption Method**: Claude prompts the user to specify the desired encryption algorithm (e.g., AES, RSA). If not specified, a default secure method is chosen.\n3. **Execute Encryption/Decryption**: Claude utilizes the encryption-tool plugin to perform the encryption or decryption operation on the provided data or file.\n4. **Return Encrypted/Decrypted Data**: Claude presents the encrypted or decrypted data to the user, or saves the result to a file as requested.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Encrypt sensitive data before storage or transmission.\n- Decrypt previously encrypted data for access or processing.\n- Generate encrypted files for secure archiving.\n\n## Examples\n\n### Example 1: Encrypting a Text File\n\nUser request: \"Encrypt the file 'sensitive_data.txt' using AES.\"\n\nThe skill will:\n1. Activate the encryption-tool plugin.\n2. Encrypt the contents of 'sensitive_data.txt' using AES encryption.\n3. Save the encrypted data to a new file (e.g., 'sensitive_data.txt.enc').\n\n### Example 2: Decrypting an Encrypted File\n\nUser request: \"Decrypt the file 'confidential.txt.enc'.\"\n\nThe skill will:\n1. Activate the encryption-tool plugin.\n2. Decrypt the contents of 'confidential.txt.enc' using the appropriate decryption key (assumed to be available or prompted for).\n3. Save the decrypted data to a new file (e.g., 'confidential.txt').\n\n## Best Practices\n\n- **Key Management**: Always store encryption keys securely and avoid hardcoding them in scripts.\n- **Algorithm Selection**: Choose encryption algorithms based on the sensitivity of the data and the required security level. Consider industry best practices and compliance requirements.\n- **Data Integrity**: Implement mechanisms to verify the integrity of encrypted data to detect tampering or corruption.\n\n## Integration\n\nThis skill can be integrated with other Claude Code plugins, such as file management tools, to automate the encryption and decryption of files during data processing workflows. It can also be combined with security auditing tools to ensure compliance with security policies.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "encryption-tool",
        "category": "security",
        "path": "plugins/security/encryption-tool",
        "version": "1.0.0",
        "description": "Encrypt and decrypt data with various algorithms"
      },
      "filePath": "plugins/security/encryption-tool/skills/encrypting-and-decrypting-data/SKILL.md"
    },
    {
      "slug": "engineering-features-for-machine-learning",
      "name": "engineering-features-for-machine-learning",
      "description": "Execute create, select, and transform features to improve machine learning model performance. Handles feature scaling, encoding, and importance analysis. Use when asked to \"engineer features\" or \"select features\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Feature Engineering Toolkit\n\nThis skill provides automated assistance for feature engineering toolkit tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for feature engineering toolkit tasks.\nThis skill enables Claude to leverage the feature-engineering-toolkit plugin to enhance machine learning models. It automates the process of creating new features, selecting the most relevant ones, and transforming existing features to better suit the model's needs. Use this skill to improve the accuracy, efficiency, and interpretability of machine learning models.\n\n## How It Works\n\n1. **Analyzing Requirements**: Claude analyzes the user's request and identifies the specific feature engineering task required.\n2. **Generating Code**: Claude generates Python code using the feature-engineering-toolkit plugin to perform the requested task. This includes data validation and error handling.\n3. **Executing Task**: The generated code is executed, creating, selecting, or transforming features as requested.\n4. **Providing Insights**: Claude provides performance metrics and insights related to the feature engineering process, such as the importance of newly created features or the impact of transformations on model performance.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create new features from existing data to improve model accuracy.\n- Select the most relevant features from a dataset to reduce model complexity and improve efficiency.\n- Transform features to better suit the assumptions of a machine learning model (e.g., scaling, normalization, encoding).\n\n## Examples\n\n### Example 1: Improving Model Accuracy\n\nUser request: \"Create new features from the existing 'age' and 'income' columns to improve the accuracy of a customer churn prediction model.\"\n\nThe skill will:\n1. Generate code to create interaction terms between 'age' and 'income' (e.g., age * income, age / income).\n2. Execute the code and evaluate the impact of the new features on model performance.\n\n### Example 2: Reducing Model Complexity\n\nUser request: \"Select the top 10 most important features from the dataset to reduce the complexity of a fraud detection model.\"\n\nThe skill will:\n1. Generate code to calculate feature importance using a suitable method (e.g., Random Forest, SelectKBest).\n2. Execute the code and select the top 10 features based on their importance scores.\n\n## Best Practices\n\n- **Data Validation**: Always validate the input data to ensure it is clean and consistent before performing feature engineering.\n- **Feature Scaling**: Scale numerical features to prevent features with larger ranges from dominating the model.\n- **Encoding Categorical Features**: Encode categorical features appropriately (e.g., one-hot encoding, label encoding) to make them suitable for machine learning models.\n\n## Integration\n\nThis skill integrates with the feature-engineering-toolkit plugin, providing a seamless way to create, select, and transform features for machine learning models. It can be used in conjunction with other Claude Code skills to build complete machine learning pipelines.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "feature-engineering-toolkit",
        "category": "ai-ml",
        "path": "plugins/ai-ml/feature-engineering-toolkit",
        "version": "1.0.0",
        "description": "Feature creation, selection, and transformation tools"
      },
      "filePath": "plugins/ai-ml/feature-engineering-toolkit/skills/engineering-features-for-machine-learning/SKILL.md"
    },
    {
      "slug": "evaluating-machine-learning-models",
      "name": "evaluating-machine-learning-models",
      "description": "Build this skill allows AI assistant to evaluate machine learning models using a comprehensive suite of metrics. it should be used when the user requests model performance analysis, validation, or testing. AI assistant can use this skill to assess model accuracy, p... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Model Evaluation Suite\n\nThis skill provides automated assistance for model evaluation suite tasks.\n\n## Overview\n\nThis skill empowers Claude to perform thorough evaluations of machine learning models, providing detailed performance insights. It leverages the `model-evaluation-suite` plugin to generate a range of metrics, enabling informed decisions about model selection and optimization.\n\n## How It Works\n\n1. **Analyzing Context**: Claude analyzes the user's request to identify the model to be evaluated and any specific metrics of interest.\n2. **Executing Evaluation**: Claude uses the `/eval-model` command to initiate the model evaluation process within the `model-evaluation-suite` plugin.\n3. **Presenting Results**: Claude presents the generated metrics and insights to the user, highlighting key performance indicators and potential areas for improvement.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Assess the performance of a machine learning model.\n- Compare the performance of multiple models.\n- Identify areas where a model can be improved.\n- Validate a model's performance before deployment.\n\n## Examples\n\n### Example 1: Evaluating Model Accuracy\n\nUser request: \"Evaluate the accuracy of my image classification model.\"\n\nThe skill will:\n1. Invoke the `/eval-model` command.\n2. Analyze the model's performance on a held-out dataset.\n3. Report the accuracy score and other relevant metrics.\n\n### Example 2: Comparing Model Performance\n\nUser request: \"Compare the F1-score of model A and model B.\"\n\nThe skill will:\n1. Invoke the `/eval-model` command for both models.\n2. Extract the F1-score from the evaluation results.\n3. Present a comparison of the F1-scores for model A and model B.\n\n## Best Practices\n\n- **Specify Metrics**: Clearly define the specific metrics of interest for the evaluation.\n- **Data Validation**: Ensure the data used for evaluation is representative of the real-world data the model will encounter.\n- **Interpret Results**: Provide context and interpretation of the evaluation results to facilitate informed decision-making.\n\n## Integration\n\nThis skill integrates seamlessly with the `model-evaluation-suite` plugin, providing a comprehensive solution for model evaluation within the Claude Code environment. It can be combined with other skills to build automated machine learning workflows.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "model-evaluation-suite",
        "category": "ai-ml",
        "path": "plugins/ai-ml/model-evaluation-suite",
        "version": "1.0.0",
        "description": "Comprehensive model evaluation with multiple metrics"
      },
      "filePath": "plugins/ai-ml/model-evaluation-suite/skills/evaluating-machine-learning-models/SKILL.md"
    },
    {
      "slug": "excel-dcf-modeler",
      "name": "excel-dcf-modeler",
      "description": "Build discounted cash flow (DCF) valuation models in Excel. Use when creating DCF models, calculating enterprise value, or valuing companies. Trigger with phrases like 'excel dcf', 'build dcf model', 'calculate enterprise value'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "ClaudeCodePlugins <plugins@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Excel DCF Modeler\n\n## Overview\n\nCreates professional DCF valuation models following investment banking standards with WACC calculations and sensitivity analysis.\n\n## Prerequisites\n\n- Excel or compatible spreadsheet software\n- Historical financial data for target company\n- Industry comparables for WACC estimation\n\n## Instructions\n\n1. Create assumptions sheet with revenue growth, margins, WACC, and terminal growth rate\n2. Build free cash flow projections (5-year forecast)\n3. Calculate terminal value using Gordon Growth Model\n4. Discount cash flows and terminal value to present value\n5. Sum to get enterprise value, subtract net debt for equity value\n6. Add sensitivity tables for key assumptions\n\n## Output\n\n- Complete 4-sheet DCF model with assumptions, projections, valuation, and sensitivity\n- Enterprise value and equity value per share\n- Sensitivity analysis on WACC and terminal growth rate\n\n## Error Handling\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| #DIV/0! in terminal value | WACC equals terminal growth | Terminal growth must be less than WACC |\n| Negative FCF | High CapEx or WC needs | Review assumptions, may need different model |\n| Unrealistic EV | Extreme growth assumptions | Benchmark against industry comparables |\n\n## Examples\n\n**Example: Value a SaaS Company**\nRequest: \"Create a DCF model for a $50M ARR SaaS company growing 30%\"\nResult: 4-sheet model with 5-year projections, 12% WACC, 3% terminal growth, sensitivity tables\n\n**Example: M&A Valuation**\nRequest: \"DCF analysis for acquisition target\"\nResult: Model with synergy adjustments, scenario analysis, and per-share valuation\n\n## Resources\n\n- [Damodaran Online DCF Resources](https://pages.stern.nyu.edu/~adamodar/)\n- [WSO DCF Modeling Guide](https://www.wallstreetoasis.com/)\n- `{baseDir}/references/dcf-formulas.md` for Excel formula templates",
      "parentPlugin": {
        "name": "excel-analyst-pro",
        "category": "business-tools",
        "path": "plugins/business-tools/excel-analyst-pro",
        "version": "1.0.0",
        "description": "Professional financial modeling toolkit for Claude Code with auto-invoked Skills and Excel MCP integration. Build DCF models, LBO analysis, variance reports, and pivot tables using natural language."
      },
      "filePath": "plugins/business-tools/excel-analyst-pro/skills/excel-dcf-modeler/SKILL.md"
    },
    {
      "slug": "excel-lbo-modeler",
      "name": "excel-lbo-modeler",
      "description": "Build leveraged buyout (LBO) models in Excel with debt schedules and IRR analysis. Use when structuring LBO transactions or analyzing PE returns. Trigger with phrases like 'excel lbo', 'build lbo model', 'calculate pe returns'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "ClaudeCodePlugins <plugins@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Excel LBO Modeler\n\n## Overview\n\nCreates leveraged buyout models with debt structuring, amortization schedules, and sponsor returns analysis for private equity transactions.\n\n## Prerequisites\n\n- Excel or compatible spreadsheet software\n- Target company financial data\n- Debt term sheet parameters\n- Entry/exit multiple assumptions\n\n## Instructions\n\n1. Set up transaction structure (purchase price, debt/equity split)\n2. Build debt schedules for each tranche (senior, mezzanine, etc.)\n3. Create operating projections with debt service\n4. Calculate cash flow available for debt paydown\n5. Model exit scenarios and calculate IRR/MOIC\n\n## Output\n\n- Complete LBO model with sources & uses, debt schedules, and returns\n- IRR and MOIC at various exit multiples and years\n- Sensitivity tables for entry/exit multiple and leverage\n\n## Error Handling\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Negative cash flow | Debt service exceeds EBITDA | Reduce leverage or restructure debt terms |\n| IRR #NUM! | No valid solution | Check exit value exceeds equity contribution |\n| Circular reference | Cash sweep tied to interest | Enable iterative calculation |\n\n## Examples\n\n**Example: Mid-Market LBO**\nRequest: \"Build an LBO model for a $100M EBITDA company at 8x entry\"\nResult: 60% senior / 40% equity structure, 5-year model, IRR analysis at 7x-10x exits\n\n**Example: Add-On Acquisition**\nRequest: \"Model a bolt-on acquisition with synergies\"\nResult: Integrated model with synergy phase-in and accretion analysis\n\n## Resources\n\n- [Macabacus LBO Modeling](https://macabacus.com/)\n- [WSO PE Interview Prep](https://www.wallstreetoasis.com/)\n- `{baseDir}/references/lbo-formulas.md` for debt schedule templates",
      "parentPlugin": {
        "name": "excel-analyst-pro",
        "category": "business-tools",
        "path": "plugins/business-tools/excel-analyst-pro",
        "version": "1.0.0",
        "description": "Professional financial modeling toolkit for Claude Code with auto-invoked Skills and Excel MCP integration. Build DCF models, LBO analysis, variance reports, and pivot tables using natural language."
      },
      "filePath": "plugins/business-tools/excel-analyst-pro/skills/excel-lbo-modeler/SKILL.md"
    },
    {
      "slug": "excel-pivot-wizard",
      "name": "excel-pivot-wizard",
      "description": "Create advanced Excel pivot tables with calculated fields and slicers. Use when building data summaries or creating interactive dashboards. Trigger with phrases like 'excel pivot', 'create pivot table', 'data summary'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "ClaudeCodePlugins <plugins@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Excel Pivot Wizard\n\n## Overview\n\nCreates advanced pivot tables with calculated fields, slicers, and dynamic dashboards for data analysis and reporting.\n\n## Prerequisites\n\n- Excel or compatible spreadsheet software\n- Tabular data with headers\n- Clear understanding of analysis dimensions and measures\n\n## Instructions\n\n1. Verify source data is in tabular format with headers\n2. Create pivot table from data range\n3. Configure rows, columns, values, and filters\n4. Add calculated fields for custom metrics\n5. Insert slicers for interactive filtering\n6. Format and style for presentation\n\n## Output\n\n- Configured pivot table with appropriate aggregations\n- Calculated fields for derived metrics\n- Interactive slicers for filtering\n- Dashboard-ready formatting\n\n## Error Handling\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Field not found | Changed source data | Refresh data connection |\n| Calculated field error | Invalid formula | Check field names match exactly |\n| Slicer not updating | Disconnected report | Reconnect slicer to pivot |\n\n## Examples\n\n**Example: Sales Dashboard**\nRequest: \"Create a pivot summarizing sales by region and product\"\nResult: Pivot with region rows, product columns, revenue values, and date slicer\n\n**Example: Financial Analysis**\nRequest: \"Build a pivot showing monthly trends by cost center\"\nResult: Time-series pivot with calculated YoY growth fields\n\n## Resources\n\n- [Microsoft Pivot Table Guide](https://support.microsoft.com/)\n- `{baseDir}/references/pivot-formulas.md` for calculated field syntax",
      "parentPlugin": {
        "name": "excel-analyst-pro",
        "category": "business-tools",
        "path": "plugins/business-tools/excel-analyst-pro",
        "version": "1.0.0",
        "description": "Professional financial modeling toolkit for Claude Code with auto-invoked Skills and Excel MCP integration. Build DCF models, LBO analysis, variance reports, and pivot tables using natural language."
      },
      "filePath": "plugins/business-tools/excel-analyst-pro/skills/excel-pivot-wizard/SKILL.md"
    },
    {
      "slug": "excel-variance-analyzer",
      "name": "excel-variance-analyzer",
      "description": "Analyze budget vs actual variances in Excel with drill-down and root cause analysis. Use when performing variance analysis or explaining budget differences. Trigger with phrases like 'excel variance', 'analyze budget variance', 'actual vs budget'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "ClaudeCodePlugins <plugins@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Excel Variance Analyzer\n\n## Overview\n\nPerforms comprehensive budget vs actual variance analysis with automated drill-down, root cause identification, and executive reporting.\n\n## Prerequisites\n\n- Excel or compatible spreadsheet software\n- Budget data by period and category\n- Actual results for comparison\n- Cost center or department structure\n\n## Instructions\n\n1. Import budget and actual data into comparison template\n2. Calculate absolute and percentage variances\n3. Apply materiality thresholds for flagging\n4. Create drill-down by category, period, or cost center\n5. Generate variance waterfall chart for executive reporting\n\n## Output\n\n- Variance summary with favorable/unfavorable indicators\n- Materiality-filtered exception report\n- Waterfall chart showing budget-to-actual bridge\n- Drill-down by category or cost center\n\n## Error Handling\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Missing periods | Data gaps | Fill with zeros or interpolate |\n| Percentage calc error | Zero budget | Use IF to handle div/0 |\n| Misaligned categories | Changed chart of accounts | Create mapping table |\n\n## Examples\n\n**Example: Monthly P&L Variance**\nRequest: \"Analyze why we missed budget by $500K this month\"\nResult: Variance waterfall showing revenue shortfall offset by OPEX savings\n\n**Example: Department Budget Review**\nRequest: \"Which departments are over budget YTD?\"\nResult: Ranked list by variance magnitude with drill-down to line items\n\n## Resources\n\n- [FP&A Best Practices](https://www.fpanda.org/)\n- `{baseDir}/references/variance-formulas.md` for calculation templates",
      "parentPlugin": {
        "name": "excel-analyst-pro",
        "category": "business-tools",
        "path": "plugins/business-tools/excel-analyst-pro",
        "version": "1.0.0",
        "description": "Professional financial modeling toolkit for Claude Code with auto-invoked Skills and Excel MCP integration. Build DCF models, LBO analysis, variance reports, and pivot tables using natural language."
      },
      "filePath": "plugins/business-tools/excel-analyst-pro/skills/excel-variance-analyzer/SKILL.md"
    },
    {
      "slug": "explaining-machine-learning-models",
      "name": "explaining-machine-learning-models",
      "description": "Build this skill enables AI assistant to provide interpretability and explainability for machine learning models. it is triggered when the user requests explanations for model predictions, insights into feature importance, or help understanding model behavior... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Model Explainability Tool\n\nThis skill provides automated assistance for model explainability tool tasks.\n\n## Overview\n\nThis skill empowers Claude to analyze and explain machine learning models. It helps users understand why a model makes certain predictions, identify the most influential features, and gain insights into the model's overall behavior.\n\n## How It Works\n\n1. **Analyze Context**: Claude analyzes the user's request and the available model data.\n2. **Select Explanation Technique**: Claude chooses the most appropriate explanation technique (e.g., SHAP, LIME) based on the model type and the user's needs.\n3. **Generate Explanations**: Claude uses the selected technique to generate explanations for model predictions.\n4. **Present Results**: Claude presents the explanations in a clear and concise format, highlighting key insights and feature importances.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Understand why a machine learning model made a specific prediction.\n- Identify the most important features influencing a model's output.\n- Debug model performance issues by identifying unexpected feature interactions.\n- Communicate model insights to non-technical stakeholders.\n- Ensure fairness and transparency in model predictions.\n\n## Examples\n\n### Example 1: Understanding Loan Application Decisions\n\nUser request: \"Explain why this loan application was rejected.\"\n\nThe skill will:\n1. Analyze the loan application data and the model's prediction.\n2. Calculate SHAP values to determine the contribution of each feature to the rejection decision.\n3. Present the results, highlighting the features that most strongly influenced the outcome, such as credit score or debt-to-income ratio.\n\n### Example 2: Identifying Key Factors in Customer Churn\n\nUser request: \"Interpret the customer churn model and identify the most important factors.\"\n\nThe skill will:\n1. Analyze the customer churn model and its predictions.\n2. Use LIME to generate local explanations for individual customer churn predictions.\n3. Aggregate the LIME explanations to identify the most important features driving churn, such as customer tenure or service usage.\n\n## Best Practices\n\n- **Model Type**: Choose the explanation technique that is most appropriate for the model type (e.g., tree-based models, neural networks).\n- **Data Preprocessing**: Ensure that the data used for explanation is properly preprocessed and aligned with the model's input format.\n- **Visualization**: Use visualizations to effectively communicate model insights and feature importances.\n\n## Integration\n\nThis skill integrates with other data analysis and visualization plugins to provide a comprehensive model understanding workflow. It can be used in conjunction with data cleaning and preprocessing plugins to ensure data quality and with visualization tools to present the explanation results in an informative way.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "model-explainability-tool",
        "category": "ai-ml",
        "path": "plugins/ai-ml/model-explainability-tool",
        "version": "1.0.0",
        "description": "Model interpretability and explainability"
      },
      "filePath": "plugins/ai-ml/model-explainability-tool/skills/explaining-machine-learning-models/SKILL.md"
    },
    {
      "slug": "exploring-blockchain-data",
      "name": "exploring-blockchain-data",
      "description": "Process query and analyze blockchain data including blocks, transactions, and smart contracts. Use when querying blockchain data and transactions. Trigger with phrases like \"explore blockchain\", \"query transactions\", or \"check on-chain data\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:explorer-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Exploring Blockchain Data\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:explorer-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "blockchain-explorer-cli",
        "category": "crypto",
        "path": "plugins/crypto/blockchain-explorer-cli",
        "version": "1.0.0",
        "description": "Command-line blockchain explorer for transactions, addresses, and contracts"
      },
      "filePath": "plugins/crypto/blockchain-explorer-cli/skills/exploring-blockchain-data/SKILL.md"
    },
    {
      "slug": "fairdb-backup-manager",
      "name": "fairdb-backup-manager",
      "description": "Manage use when you need to work with backup and recovery. This skill provides backup automation and disaster recovery with comprehensive guidance and automation. Trigger with phrases like \"create backups\", \"automate backups\", or \"implement disaster recovery\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(tar:*), Bash(rsync:*), Bash(aws:s3:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Fairdb Backup Manager\n\nThis skill provides automated assistance for fairdb backup manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/fairdb-backup-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/fairdb-backup-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/fairdb-backup-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/fairdb-backup-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/fairdb-backup-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/fairdb-backup-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "fairdb-operations-kit",
        "category": "devops",
        "path": "plugins/devops/fairdb-operations-kit",
        "version": "1.0.0",
        "description": "Complete operations kit for FairDB PostgreSQL as a Service - VPS setup, PostgreSQL management, customer provisioning, monitoring, and backup automation"
      },
      "filePath": "plugins/devops/fairdb-operations-kit/skills/fairdb-backup-manager/SKILL.md"
    },
    {
      "slug": "finding-arbitrage-opportunities",
      "name": "finding-arbitrage-opportunities",
      "description": "Detect profitable arbitrage opportunities across exchanges and DEXs in real-time. Use when discovering profitable arbitrage across exchanges. Trigger with phrases like \"find arbitrage\", \"scan for arb opportunities\", or \"check arbitrage\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:arbitrage-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Finding Arbitrage Opportunities\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:arbitrage-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "arbitrage-opportunity-finder",
        "category": "crypto",
        "path": "plugins/crypto/arbitrage-opportunity-finder",
        "version": "1.0.0",
        "description": "Find and analyze arbitrage opportunities across exchanges and DeFi protocols"
      },
      "filePath": "plugins/crypto/arbitrage-opportunity-finder/skills/finding-arbitrage-opportunities/SKILL.md"
    },
    {
      "slug": "finding-security-misconfigurations",
      "name": "finding-security-misconfigurations",
      "description": "Configure identify security misconfigurations in infrastructure-as-code, application settings, and system configurations. Use when you need to audit Terraform/CloudFormation templates, check application config files, validate system security settings, or ensure compliance with security best practices. Trigger with phrases like \"find security misconfigurations\", \"audit infrastructure security\", \"check config security\", or \"scan for misconfigured settings\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(config-scan:*), Bash(iac-check:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Finding Security Misconfigurations\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Configuration files accessible in {baseDir}/ (Terraform, CloudFormation, YAML, JSON)\n- Infrastructure-as-code files (.tf, .yaml, .json, .template)\n- Application configuration files (application.yml, config.json, .env.example)\n- System configuration exports available\n- Write permissions for findings report in {baseDir}/security-findings/\n\n## Instructions\n\n1. Identify the target system/service and gather current configuration.\n2. Compare settings against baseline hardening guidance.\n3. Flag risky defaults, drift, and missing controls with severity.\n4. Provide a minimal-change remediation plan and verification steps.\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: Security misconfigurations report saved to {baseDir}/security-findings/misconfig-YYYYMMDD.md\n\n**Report Structure**:\n```\n# Security Misconfiguration Findings\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CIS Benchmarks: https://www.cisecurity.org/cis-benchmarks/\n- OWASP Configuration Guide: https://cheatsheetseries.owasp.org/cheatsheets/Infrastructure_as_Code_Security_Cheatsheet.html\n- Cloud Security Alliance: https://cloudsecurityalliance.org/\n- tfsec (Terraform): https://github.com/aquasecurity/tfsec\n- Checkov (Multi-cloud): https://www.checkov.io/",
      "parentPlugin": {
        "name": "security-misconfiguration-finder",
        "category": "security",
        "path": "plugins/security/security-misconfiguration-finder",
        "version": "1.0.0",
        "description": "Find security misconfigurations"
      },
      "filePath": "plugins/security/security-misconfiguration-finder/skills/finding-security-misconfigurations/SKILL.md"
    },
    {
      "slug": "firebase-vertex-ai",
      "name": "firebase-vertex-ai",
      "description": "Execute firebase platform expert with Vertex AI Gemini integration for Authentication, Firestore, Storage, Functions, Hosting, and AI-powered features. Use when asked to \"setup firebase\", \"deploy to firebase\", or \"integrate vertex ai with firebase\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Firebase Vertex AI\n\nOperate Firebase projects end-to-end (Auth, Firestore, Functions, Hosting) and integrate Gemini/Vertex AI safely for AI-powered features.\n\n## Overview\n\nUse this skill to design, implement, and deploy Firebase applications that call Vertex AI/Gemini from Cloud Functions (or other GCP services) with secure secrets handling, least-privilege IAM, and production-ready observability.\n\n## Prerequisites\n\n- Node.js runtime and Firebase CLI access for the target project\n- A Firebase project (billing enabled for Functions/Vertex AI as needed)\n- Vertex AI API enabled and permissions to call Gemini/Vertex AI from your backend\n- Secrets managed via env vars or Secret Manager (never in client code)\n\n## Instructions\n\n1. Initialize Firebase (or validate an existing repo): Hosting/Functions/Firestore as required.\n2. Implement backend integration:\n   - add a Cloud Function/HTTP endpoint that calls Gemini/Vertex AI\n   - validate inputs and return structured responses\n3. Configure data and security:\n   - Firestore rules + indexes\n   - Storage rules (if applicable)\n   - Auth providers and authorization checks\n4. Deploy and verify:\n   - deploy Functions/Hosting\n   - run smoke tests against deployed endpoints\n5. Add ops guardrails:\n   - logging/metrics\n   - alerting for error spikes\n   - basic cost controls (budgets/quotas) where appropriate\n\n## Output\n\n- A deployable Firebase project structure (configs + Functions/Hosting as needed)\n- Secure backend code that calls Gemini/Vertex AI (with secrets handled correctly)\n- Firestore/Storage rules and index guidance\n- A verification checklist (local + deployed) and CI-ready commands\n\n## Error Handling\n\n- Auth failures: identify the principal and missing permission/role; fix with least privilege.\n- Billing/API issues: detect which API or quota is blocking and provide remediation steps.\n- Firestore rule/index problems: provide minimal repro queries and rule fixes.\n- Vertex AI call failures: surface model/region mismatches and add retries/backoff for transient errors.\n\n## Examples\n\n**Example: Gemini-backed chat API on Firebase**\n- Request: “Deploy Hosting + a Function that powers a Gemini chat endpoint.”\n- Result: `/api/chat` function, Secret Manager wiring, and smoke tests.\n\n**Example: Firestore-powered RAG**\n- Request: “Build a RAG flow that embeds docs and answers with citations.”\n- Result: ingestion plan, embedding + index strategy, and evaluation prompts.\n\n## Resources\n\n- Full detailed guide (kept for reference): `{baseDir}/references/SKILL.full.md`\n- Firebase docs: https://firebase.google.com/docs\n- Cloud Functions for Firebase: https://firebase.google.com/docs/functions\n- Vertex AI docs: https://cloud.google.com/vertex-ai/docs",
      "parentPlugin": {
        "name": "jeremy-firebase",
        "category": "community",
        "path": "plugins/community/jeremy-firebase",
        "version": "1.0.0",
        "description": "Firebase platform expert for Firestore, Auth, Functions, and Vertex AI integration"
      },
      "filePath": "plugins/community/jeremy-firebase/skills/firebase-vertex-ai/SKILL.md"
    },
    {
      "slug": "firestore-operations-manager",
      "name": "firestore-operations-manager",
      "description": "Manage Firebase/Firestore operations including CRUD, queries, batch processing, and index/rule guidance. Use when you need to create/update/query Firestore documents, run batch writes, troubleshoot missing indexes, or plan migrations. Trigger with phrases like \"firestore operations\", \"create firestore document\", \"batch write\", \"missing index\", or \"fix firestore query\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Firestore Operations Manager\n\nOperate Firestore safely in production: schema-aware CRUD, query/index tuning, batch processing, and guardrails for permissions and cost.\n\n## Overview\n\nUse this skill to design Firestore data access patterns and implement changes with the right indexes, security rules, and operational checks (emulator tests, monitoring, and rollback plans).\n\n## Prerequisites\n\n- A Firebase project with Firestore enabled (or a local emulator setup)\n- A clear collection/document schema (or permission to propose one)\n- Credentials for the target environment (service account / ADC) and a plan for secrets\n\n## Instructions\n\n1. Identify the operation: create/update/delete/query/batch/migration.\n2. Confirm schema expectations and security rules constraints.\n3. Implement the change (or propose a patch) using safe patterns:\n   - prefer batched writes/transactions where consistency matters\n   - add pagination for large queries\n4. Check indexes:\n   - detect required composite indexes and provide `firestore.indexes.json` updates\n5. Validate:\n   - run emulator tests or a minimal smoke query\n   - confirm cost/perf implications for the query pattern\n\n## Output\n\n- Code changes or snippets for the requested Firestore operation\n- Index recommendations (and config updates when needed)\n- A validation checklist (emulator commands and production smoke tests)\n\n## Error Handling\n\n- Permission denied: identify the rule/role blocking the operation and propose least-privilege changes.\n- Missing index: provide the exact composite index needed for the query.\n- Hotspot/latency issues: propose sharding, pagination, or query redesign.\n\n## Examples\n\n**Example: Fix a failing query**\n- Request: “This query needs a composite index—what do I add?”\n- Result: the exact index definition and a safer query pattern if needed.\n\n**Example: Batch migration**\n- Request: “Backfill a new field across 100k docs.”\n- Result: batched write strategy, checkpoints, and rollback guidance.\n\n## Resources\n\n- Full detailed guide (kept for reference): `{baseDir}/references/SKILL.full.md`\n- Firestore docs: https://firebase.google.com/docs/firestore\n- Firestore indexes: https://firebase.google.com/docs/firestore/query-data/indexing",
      "parentPlugin": {
        "name": "jeremy-firestore",
        "category": "community",
        "path": "plugins/community/jeremy-firestore",
        "version": "1.0.0",
        "description": "Firestore database specialist for schema design, queries, and real-time sync"
      },
      "filePath": "plugins/community/jeremy-firestore/skills/firestore-operations-manager/SKILL.md"
    },
    {
      "slug": "forecasting-time-series-data",
      "name": "forecasting-time-series-data",
      "description": "Process this skill enables AI assistant to forecast future values based on historical time series data. it analyzes time-dependent data to identify trends, seasonality, and other patterns. use this skill when the user asks to predict future values of a time ser... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Time Series Forecaster\n\nThis skill provides automated assistance for time series forecaster tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for time series forecaster tasks.\nThis skill empowers Claude to perform time series forecasting, providing insights into future trends and patterns. It automates the process of data analysis, model selection, and prediction generation, delivering valuable information for decision-making.\n\n## How It Works\n\n1. **Data Analysis**: Claude analyzes the provided time series data, identifying key characteristics such as trends, seasonality, and autocorrelation.\n2. **Model Selection**: Based on the data characteristics, Claude selects an appropriate forecasting model (e.g., ARIMA, Prophet).\n3. **Prediction Generation**: The selected model is trained on the historical data, and future values are predicted along with confidence intervals.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Forecast future sales based on past sales data.\n- Predict website traffic for the next month.\n- Analyze trends in stock prices over the past year.\n\n## Examples\n\n### Example 1: Forecasting Sales\n\nUser request: \"Forecast sales for the next quarter based on the past 3 years of monthly sales data.\"\n\nThe skill will:\n1. Analyze the historical sales data to identify trends and seasonality.\n2. Select and train a suitable forecasting model (e.g., ARIMA or Prophet).\n3. Generate a forecast of sales for the next quarter, including confidence intervals.\n\n### Example 2: Predicting Website Traffic\n\nUser request: \"Predict weekly website traffic for the next month based on the last 6 months of data.\"\n\nThe skill will:\n1. Analyze the website traffic data to identify patterns and seasonality.\n2. Choose an appropriate time series forecasting model.\n3. Generate a forecast of weekly website traffic for the next month.\n\n## Best Practices\n\n- **Data Quality**: Ensure the time series data is clean, complete, and accurate for optimal forecasting results.\n- **Model Selection**: Choose a forecasting model appropriate for the characteristics of the data (e.g., ARIMA for stationary data, Prophet for data with strong seasonality).\n- **Evaluation**: Evaluate the performance of the forecasting model using appropriate metrics (e.g., Mean Absolute Error, Root Mean Squared Error).\n\n## Integration\n\nThis skill can be integrated with other data analysis and visualization tools within the Claude Code ecosystem to provide a comprehensive solution for time series analysis and forecasting.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "time-series-forecaster",
        "category": "ai-ml",
        "path": "plugins/ai-ml/time-series-forecaster",
        "version": "1.0.0",
        "description": "Time series forecasting and analysis"
      },
      "filePath": "plugins/ai-ml/time-series-forecaster/skills/forecasting-time-series-data/SKILL.md"
    },
    {
      "slug": "fuzzing-apis",
      "name": "fuzzing-apis",
      "description": "Configure perform API fuzzing to discover edge cases, crashes, and security vulnerabilities. Use when performing specialized testing. Trigger with phrases like \"fuzz the API\", \"run fuzzing tests\", or \"discover edge cases\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:fuzz-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Api Fuzzer\n\nThis skill provides automated assistance for api fuzzer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:fuzz-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for api fuzzer tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-fuzzer",
        "category": "testing",
        "path": "plugins/testing/api-fuzzer",
        "version": "1.0.0",
        "description": "Fuzz testing for APIs with malformed inputs, edge cases, and security vulnerability detection"
      },
      "filePath": "plugins/testing/api-fuzzer/skills/fuzzing-apis/SKILL.md"
    },
    {
      "slug": "gastown",
      "name": "gastown",
      "description": "Manage multi-agent orchestrator for Claude Code. Use when user mentions gastown, gas town, gt commands, bd commands, convoys, polecats, crew, rigs, slinging work, multi-agent coordination, beads, hooks, molecules, workflows, the witness, the mayor, the refinery, the deacon, dogs, escalation, or wants to run multiple AI agents on projects simultaneously. Handles installation, workspace setup, work tracking, agent lifecycle, crash recovery, and all gt/bd CLI operations. Trigger with phrases like \"gas town\", \"gt sling\", \"fire up the engine\". allowed-tools: Read, Write, Edit, Bash(cmd:*), Grep, Glob, WebFetch version: 1.0.0 license: Apache-2.0 author: Numman Ali <numman.ali@gmail.com>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Numman Ali <numman.ali@gmail.com>",
      "license": "MIT",
      "content": "# Gastown\n\n## Overview\n\nGas Town is a multi-agent orchestration system for Claude Code that enables parallel AI workers to execute tasks simultaneously. It provides work tracking through beads, agent lifecycle management via polecats and crew, and automated code merging through the Refinery.\n\n## Prerequisites\n\n- Go 1.21+ installed for CLI tools (`gt` and `bd`)\n- Git configured with SSH or HTTPS access\n- Terminal access for running commands\n- Sufficient disk space for workspace (~100MB for ~/gt)\n- GitHub account for repository integration (optional)\n\n## Instructions\n\n1. Install Gas Town CLI tools (gt and bd) using Go\n2. Create your workshop directory at ~/gt\n3. Run diagnostics with gt doctor and bd doctor\n4. Add a project as a rig using gt rig add\n5. Create work items as beads using bd create\n6. Sling work to agents using gt sling\n7. Monitor progress with gt status and gt peek\n8. Let the Refinery merge completed work\n\nThe Cognition Engine. Track work with convoys; sling to agents.\n\n## Output\n\n- Executed gt and bd commands with results reported to user\n- Engine status reports showing system health and worker states\n- Work tracking updates (beads created, assigned, completed)\n- Polecat and crew lifecycle events (spawn, completion, termination)\n- Diagnostic results from gt doctor and bd doctor\n- Merge pipeline status from Refinery operations\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources",
      "parentPlugin": {
        "name": "gastown",
        "category": "community",
        "path": "plugins/community/gastown",
        "version": "1.0.0",
        "description": "Multi-agent orchestrator for Claude Code. Track work with convoys, sling to polecats. The Cognition Engine for AI-powered software factories."
      },
      "filePath": "plugins/community/gastown/skills/gastown/SKILL.md"
    },
    {
      "slug": "gcp-examples-expert",
      "name": "gcp-examples-expert",
      "description": "Generate production-ready Google Cloud code examples from official repositories including ADK samples, Genkit templates, Vertex AI notebooks, and Gemini patterns. Use when asked to \"show ADK example\" or \"provide GCP starter kit\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Gcp Examples Expert\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "jeremy-gcp-starter-examples",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-gcp-starter-examples",
        "version": "1.0.0",
        "description": "Google Cloud starter kits and example code aggregator with ADK samples"
      },
      "filePath": "plugins/ai-ml/jeremy-gcp-starter-examples/skills/gcp-examples-expert/SKILL.md"
    },
    {
      "slug": "generating-api-contracts",
      "name": "generating-api-contracts",
      "description": "Generate API contracts and OpenAPI specifications from code or design documents. Use when documenting API contracts and specifications. Trigger with phrases like \"generate API contract\", \"create OpenAPI spec\", or \"document API contract\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:contract-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Generating Api Contracts\n\n## Overview\n\n\nThis skill provides automated assistance for api contract generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:contract-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-contract-generator",
        "category": "api-development",
        "path": "plugins/api-development/api-contract-generator",
        "version": "1.0.0",
        "description": "Generate API contracts for consumer-driven contract testing"
      },
      "filePath": "plugins/api-development/api-contract-generator/skills/generating-api-contracts/SKILL.md"
    },
    {
      "slug": "generating-api-docs",
      "name": "generating-api-docs",
      "description": "Create comprehensive API documentation with examples, authentication guides, and SDKs. Use when creating comprehensive API documentation. Trigger with phrases like \"generate API docs\", \"create API documentation\", or \"document the API\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:docs-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Generating Api Docs\n\n## Overview\n\n\nThis skill provides automated assistance for api documentation generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:docs-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-documentation-generator",
        "category": "api-development",
        "path": "plugins/api-development/api-documentation-generator",
        "version": "1.0.0",
        "description": "Generate comprehensive API documentation from OpenAPI/Swagger specs"
      },
      "filePath": "plugins/api-development/api-documentation-generator/skills/generating-api-docs/SKILL.md"
    },
    {
      "slug": "generating-api-sdks",
      "name": "generating-api-sdks",
      "description": "Generate client SDKs in multiple languages from OpenAPI specifications. Use when generating client libraries for API consumption. Trigger with phrases like \"generate SDK\", \"create client library\", or \"build API SDK\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:sdk-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Generating Api Sdks\n\n## Overview\n\n\nThis skill provides automated assistance for api sdk generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:sdk-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-sdk-generator",
        "category": "api-development",
        "path": "plugins/api-development/api-sdk-generator",
        "version": "1.0.0",
        "description": "Generate client SDKs from OpenAPI specs for multiple languages"
      },
      "filePath": "plugins/api-development/api-sdk-generator/skills/generating-api-sdks/SKILL.md"
    },
    {
      "slug": "generating-compliance-reports",
      "name": "generating-compliance-reports",
      "description": "Generate comprehensive compliance reports for security standards. Use when creating compliance documentation. Trigger with 'generate compliance report', 'compliance status', or 'audit compliance'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Compliance Report Generator\n\nThis skill provides automated assistance for compliance report generator tasks.\n\n## Overview\n\nThis skill empowers Claude to create detailed compliance reports, saving time and ensuring accuracy in documenting security practices. It automates the process of gathering information and formatting it into a standardized report, making compliance audits easier and more efficient.\n\n## How It Works\n\n1. **Identify Report Type**: Claude analyzes the user's request to determine the required compliance standard (e.g., PCI DSS, HIPAA).\n2. **Gather Data**: The plugin collects relevant data from the system or prompts the user for necessary information.\n3. **Generate Report**: The plugin formats the collected data into a comprehensive compliance report, including necessary sections and documentation.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Generate a report for a specific compliance standard (e.g., \"generate a HIPAA compliance report\").\n- Create a security audit report.\n- Document adherence to a security policy.\n- Prepare for a compliance audit.\n\n## Examples\n\n### Example 1: Generating a PCI DSS Compliance Report\n\nUser request: \"Generate a PCI DSS compliance report for our e-commerce platform.\"\n\nThe skill will:\n1. Activate the compliance-report-generator plugin.\n2. Prompt the user for information about their e-commerce platform's security controls and processes.\n3. Generate a detailed PCI DSS compliance report based on the provided information.\n\n### Example 2: Creating a HIPAA Compliance Report\n\nUser request: \"Create a HIPAA compliance report to demonstrate our adherence to privacy regulations.\"\n\nThe skill will:\n1. Activate the compliance-report-generator plugin.\n2. Guide the user through a series of questions related to HIPAA requirements.\n3. Compile the answers into a structured HIPAA compliance report.\n\n## Best Practices\n\n- **Specificity**: Be specific about the compliance standard you need a report for (e.g., \"SOC 2 report\").\n- **Completeness**: Provide all the necessary information requested by the plugin to ensure a comprehensive and accurate report.\n- **Review**: Always review the generated report to ensure its accuracy and completeness before submitting it for an audit.\n\n## Integration\n\nThis skill can be integrated with other plugins that provide security assessment or vulnerability scanning capabilities. The results from those plugins can be incorporated into the compliance reports generated by this skill, providing a more comprehensive view of the organization's security posture.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "compliance-report-generator",
        "category": "security",
        "path": "plugins/security/compliance-report-generator",
        "version": "1.0.0",
        "description": "Generate compliance reports"
      },
      "filePath": "plugins/security/compliance-report-generator/skills/generating-compliance-reports/SKILL.md"
    },
    {
      "slug": "generating-conventional-commits",
      "name": "generating-conventional-commits",
      "description": "Execute generates conventional commit messages using AI. It analyzes code changes and suggests a commit message adhering to the conventional commits specification. Use this skill when you need help writing clear, standardized commit messages, especially a... Use when managing version control. Trigger with phrases like 'commit', 'branch', or 'git'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Devops Automation Pack\n\nThis skill provides automated assistance for devops automation pack tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for devops automation pack tasks.\nThis skill helps you create well-formatted, informative commit messages that follow the conventional commits standard, improving collaboration and automation in your Git workflow. It saves you time and ensures consistency across your project.\n\n## How It Works\n\n1. **Analyze Changes**: The skill analyzes the staged changes in your Git repository.\n2. **Generate Suggestion**: It uses AI to generate a commit message based on the analyzed changes, adhering to the conventional commits format (e.g., `feat: add new feature`, `fix: correct bug`).\n3. **Present to User**: The generated commit message is presented to you for review and acceptance.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a commit message after making code changes.\n- Ensure your commit messages follow the conventional commits standard.\n- Save time writing commit messages manually.\n\n## Examples\n\n### Example 1: Adding a New Feature\n\nUser request: \"Generate a commit message for these changes.\"\n\nThe skill will:\n1. Analyze the staged changes related to a new feature.\n2. Generate a commit message like `feat: Implement user authentication`.\n\n### Example 2: Fixing a Bug\n\nUser request: \"Create a commit for the bug fix.\"\n\nThe skill will:\n1. Analyze the staged changes related to a bug fix.\n2. Generate a commit message like `fix: Resolve issue with incorrect password reset`.\n\n## Best Practices\n\n- **Stage Changes**: Ensure all relevant changes are staged before using the skill.\n- **Review Carefully**: Always review the generated commit message before accepting it.\n- **Customize if Needed**: Feel free to customize the generated message to provide more context.\n\n## Integration\n\nThis skill integrates with your Git workflow, providing a convenient way to generate commit messages directly within Claude Code. It complements other Git-related skills in the DevOps Automation Pack, such as `/branch-create` and `/pr-create`.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "devops-automation-pack",
        "category": "packages",
        "path": "plugins/packages/devops-automation-pack",
        "version": "1.0.0",
        "description": "25 professional DevOps plugins for CI/CD, deployment, Docker, Kubernetes, and infrastructure automation. Save 20+ hours of manual work."
      },
      "filePath": "plugins/packages/devops-automation-pack/skills/generating-conventional-commits/SKILL.md"
    },
    {
      "slug": "generating-database-seed-data",
      "name": "generating-database-seed-data",
      "description": "Process this skill enables AI assistant to generate realistic test data and database seed scripts for development and testing environments. it uses faker libraries to create realistic data, maintains relational integrity, and allows configurable data volumes. u... Use when working with databases or data models. Trigger with phrases like 'database', 'query', or 'schema'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Data Seeder Generator\n\nThis skill provides automated assistance for data seeder generator tasks.\n\n## Overview\n\nThis skill automates the creation of database seed scripts, populating your database with realistic and consistent test data. It leverages Faker libraries to generate diverse and believable data, ensuring relational integrity and configurable data volumes.\n\n## How It Works\n\n1. **Analyze Schema**: Claude analyzes the database schema to understand table structures and relationships.\n2. **Generate Data**: Using Faker libraries, Claude generates realistic data for each table, respecting data types and constraints.\n3. **Maintain Relationships**: Claude ensures foreign key relationships are maintained, creating consistent and valid data across tables.\n4. **Create Seed Script**: Claude generates a database seed script (e.g., SQL, JavaScript) containing the generated data.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Populate a development database with realistic data.\n- Create a seed script for automated database setup.\n- Generate test data for application testing.\n- Demonstrate an application with pre-populated data.\n\n## Examples\n\n### Example 1: Populating a User Database\n\nUser request: \"Create a seed script to populate my users table with 50 realistic users.\"\n\nThe skill will:\n1. Analyze the 'users' table schema (name, email, password, etc.).\n2. Generate 50 sets of realistic user data using Faker libraries.\n3. Create a SQL seed script to insert the generated user data into the 'users' table.\n\n### Example 2: Seeding a Blog Database\n\nUser request: \"Generate test data for my blog database, including posts, comments, and users.\"\n\nThe skill will:\n1. Analyze the 'posts', 'comments', and 'users' table schemas and their relationships.\n2. Generate realistic data for each table, ensuring foreign key relationships are maintained (e.g., comments linked to posts, posts linked to users).\n3. Create a seed script (e.g., JavaScript with TypeORM) to insert the generated data into the database.\n\n## Best Practices\n\n- **Data Volume**: Start with a small data volume and gradually increase it to avoid performance issues.\n- **Data Consistency**: Ensure the Faker libraries used are appropriate for the data types and formats required by your database.\n- **Idempotency**: Design your seed scripts to be idempotent, so they can be run multiple times without causing errors or duplicate data.\n\n## Integration\n\nThis skill integrates well with database migration tools and frameworks, allowing you to automate the entire database setup process, including schema creation and data seeding. It can also be used in conjunction with testing frameworks to generate realistic test data for automated testing.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "data-seeder-generator",
        "category": "database",
        "path": "plugins/database/data-seeder-generator",
        "version": "1.0.0",
        "description": "Generate realistic test data and database seed scripts for development and testing environments"
      },
      "filePath": "plugins/database/data-seeder-generator/skills/generating-database-seed-data/SKILL.md"
    },
    {
      "slug": "generating-docker-compose-files",
      "name": "generating-docker-compose-files",
      "description": "Execute use when you need to work with Docker Compose. This skill provides Docker Compose file generation with comprehensive guidance and automation. Trigger with phrases like \"generate docker-compose\", \"create compose file\", or \"configure multi-container app\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(docker:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Docker Compose Generator\n\nThis skill provides automated assistance for docker compose generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/docker-compose-generator/`\n\n**Documentation and Guides**: `{baseDir}/docs/docker-compose-generator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/docker-compose-generator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/docker-compose-generator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/docker-compose-generator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/docker-compose-generator-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "docker-compose-generator",
        "category": "devops",
        "path": "plugins/devops/docker-compose-generator",
        "version": "1.0.0",
        "description": "Generate Docker Compose configurations for multi-container applications with best practices"
      },
      "filePath": "plugins/devops/docker-compose-generator/skills/generating-docker-compose-files/SKILL.md"
    },
    {
      "slug": "generating-grpc-services",
      "name": "generating-grpc-services",
      "description": "Generate gRPC service definitions, stubs, and implementations from Protocol Buffers. Use when creating high-performance gRPC services. Trigger with phrases like \"generate gRPC service\", \"create gRPC API\", or \"build gRPC server\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:grpc-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Generating Grpc Services\n\n## Overview\n\n\nThis skill provides automated assistance for grpc service generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:grpc-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "grpc-service-generator",
        "category": "api-development",
        "path": "plugins/api-development/grpc-service-generator",
        "version": "1.0.0",
        "description": "Generate gRPC services with Protocol Buffers and streaming support"
      },
      "filePath": "plugins/api-development/grpc-service-generator/skills/generating-grpc-services/SKILL.md"
    },
    {
      "slug": "generating-helm-charts",
      "name": "generating-helm-charts",
      "description": "Execute use when generating Helm charts for Kubernetes applications. Trigger with phrases like \"create Helm chart\", \"generate chart for app\", \"package Kubernetes deployment\", or \"helm template\". Produces production-ready charts with Chart.yaml, values.yaml, templates, and best practices for multi-environment deployments. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(helm:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Helm Chart Generator\n\nThis skill provides automated assistance for helm chart generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Helm 3+ is installed on the system\n- Kubernetes cluster access is configured\n- Application container images are available\n- Understanding of application resource requirements\n- Chart repository access (if publishing)\n\n## Instructions\n\n1. **Gather Requirements**: Identify application type, dependencies, configuration needs\n2. **Create Chart Structure**: Generate Chart.yaml with metadata and version info\n3. **Define Values**: Create values.yaml with configurable parameters and defaults\n4. **Build Templates**: Generate deployment, service, ingress, and configmap templates\n5. **Add Helpers**: Create _helpers.tpl for reusable template functions\n6. **Configure Resources**: Set resource limits, security contexts, and health checks\n7. **Test Chart**: Validate with `helm lint` and `helm template` commands\n8. **Document Usage**: Add README with installation instructions and configuration options\n\n## Output\n\nGenerates complete Helm chart structure:\n\n```\n{baseDir}/helm-charts/app-name/\n├── Chart.yaml          # Chart metadata\n├── values.yaml         # Default configuration\n├── templates/\n│   ├── deployment.yaml\n│   ├── service.yaml\n│   ├── ingress.yaml\n│   ├── configmap.yaml\n│   ├── _helpers.tpl    # Template helpers\n│   └── NOTES.txt       # Post-install notes\n├── charts/             # Dependencies\n└── README.md\n```\n\n**Example Chart.yaml:**\n```yaml\napiVersion: v2\nname: my-app\ndescription: Production-ready application chart\ntype: application\nversion: 1.0.0\nappVersion: \"1.0.0\"\n```\n\n**Example values.yaml:**\n```yaml\nreplicaCount: 3\nimage:\n  repository: registry/app\n  tag: \"1.0.0\"\n  pullPolicy: IfNotPresent\nresources:\n  limits:\n    cpu: 500m\n    memory: 512Mi\n  requests:\n    cpu: 250m\n    memory: 256Mi\n```\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Chart Validation Errors**\n- Error: \"Chart.yaml: version is required\"\n- Solution: Ensure Chart.yaml contains valid apiVersion, name, and version fields\n\n**Template Rendering Failures**\n- Error: \"parse error in deployment.yaml\"\n- Solution: Validate template syntax with `helm template` and check Go template formatting\n\n**Missing Dependencies**\n- Error: \"dependency not found\"\n- Solution: Run `helm dependency update` in chart directory\n\n**Values Override Issues**\n- Error: \"failed to render values\"\n- Solution: Check values.yaml syntax and ensure proper YAML indentation\n\n**Installation Failures**\n- Error: \"release failed: timed out waiting for condition\"\n- Solution: Increase timeout or check pod logs for application startup issues\n\n## Resources\n\n- Helm documentation: https://helm.sh/docs/\n- Chart best practices guide: https://helm.sh/docs/chart_best_practices/\n- Template function reference: https://helm.sh/docs/chart_template_guide/\n- Example charts repository: https://github.com/helm/charts\n- Chart testing guide in {baseDir}/docs/helm-testing.md\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "helm-chart-generator",
        "category": "devops",
        "path": "plugins/devops/helm-chart-generator",
        "version": "1.0.0",
        "description": "Generate Helm charts for Kubernetes applications"
      },
      "filePath": "plugins/devops/helm-chart-generator/skills/generating-helm-charts/SKILL.md"
    },
    {
      "slug": "generating-infrastructure-as-code",
      "name": "generating-infrastructure-as-code",
      "description": "Execute use when generating infrastructure as code configurations. Trigger with phrases like \"create Terraform config\", \"generate CloudFormation template\", \"write Pulumi code\", or \"IaC for AWS/GCP/Azure\". Produces production-ready code for Terraform, CloudFormation, Pulumi, ARM templates, and CDK across multiple cloud providers. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(terraform:*), Bash(aws:*), Bash(gcloud:*), Bash(az:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Infrastructure As Code Generator\n\nThis skill provides automated assistance for infrastructure as code generator tasks.\n\n## Overview\n\nGenerates production-ready IaC (Terraform/CloudFormation/Pulumi/etc.) with modular structure, variables, outputs, and deployment guidance for common cloud stacks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Target cloud provider CLI is installed (aws-cli, gcloud, az)\n- IaC tool is installed (Terraform, Pulumi, AWS CDK)\n- Cloud credentials are configured locally\n- Understanding of target infrastructure architecture\n- Version control system for IaC storage\n\n## Instructions\n\n1. **Identify Platform**: Determine IaC tool (Terraform, CloudFormation, Pulumi, ARM, CDK)\n2. **Define Resources**: Specify cloud resources needed (compute, network, storage, database)\n3. **Establish Structure**: Create modular file structure for maintainability\n4. **Generate Code**: Write IaC configurations with proper syntax and formatting\n5. **Add Variables**: Define input variables for environment-specific values\n6. **Configure Outputs**: Specify outputs for resource references and integrations\n7. **Implement State**: Set up remote state storage for team collaboration\n8. **Document Usage**: Add README with deployment instructions and prerequisites\n\n## Output\n\nGenerates infrastructure as code files:\n\n**Terraform Example:**\n```hcl\n# {baseDir}/terraform/main.tf\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\nterraform {\n  required_version = \">= 1.0\"\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nresource \"aws_vpc\" \"main\" {\n  cidr_block = var.vpc_cidr\n  enable_dns_hostnames = true\n\n  tags = {\n    Name = \"${var.project}-vpc\"\n    Environment = var.environment\n  }\n}\n```\n\n**CloudFormation Example:**\n```yaml\n# {baseDir}/cloudformation/template.yaml\nAWSTemplateFormatVersion: '2010-09-09'\nDescription: Production VPC infrastructure\n\nParameters:\n  VpcCidr:\n    Type: String\n    Default: 10.0.0.0/16\n\nResources:\n  VPC:\n    Type: AWS::EC2::VPC\n    Properties:\n      CidrBlock: !Ref VpcCidr\n      EnableDnsHostnames: true\n```\n\n**Pulumi Example:**\n```typescript\n// {baseDir}/pulumi/index.ts\nimport * as aws from \"@pulumi/aws\";\n\nconst vpc = new aws.ec2.Vpc(\"main\", {\n    cidrBlock: \"10.0.0.0/16\",\n    enableDnsHostnames: true,\n    tags: {\n        Name: \"production-vpc\"\n    }\n});\n\nexport const vpcId = vpc.id;\n```\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Syntax Errors**\n- Error: \"Invalid resource syntax in configuration\"\n- Solution: Validate syntax with `terraform validate` or respective tool linter\n\n**Provider Authentication**\n- Error: \"Unable to authenticate with cloud provider\"\n- Solution: Configure credentials via environment variables or CLI login\n\n**Resource Conflicts**\n- Error: \"Resource already exists\"\n- Solution: Import existing resources or use data sources instead of creating new ones\n\n**State Lock Issues**\n- Error: \"Error acquiring state lock\"\n- Solution: Ensure no other process is running, or force unlock if safe\n\n**Dependency Errors**\n- Error: \"Resource depends on resource that does not exist\"\n- Solution: Check resource references and ensure proper dependency ordering\n\n## Examples\n\n- \"Generate Terraform for a VPC + private subnets + NAT + EKS cluster on AWS.\"\n- \"Create a minimal CloudFormation template for an S3 bucket with encryption and public access blocked.\"\n\n## Resources\n\n- Terraform documentation: https://www.terraform.io/docs/\n- AWS CloudFormation guide: https://docs.aws.amazon.com/cloudformation/\n- Pulumi documentation: https://www.pulumi.com/docs/\n- Azure ARM templates: https://docs.microsoft.com/azure/azure-resource-manager/\n- IaC best practices guide in {baseDir}/docs/iac-standards.md",
      "parentPlugin": {
        "name": "infrastructure-as-code-generator",
        "category": "devops",
        "path": "plugins/devops/infrastructure-as-code-generator",
        "version": "1.0.0",
        "description": "Generate Infrastructure as Code for Terraform, CloudFormation, Pulumi, and more"
      },
      "filePath": "plugins/devops/infrastructure-as-code-generator/skills/generating-infrastructure-as-code/SKILL.md"
    },
    {
      "slug": "generating-orm-code",
      "name": "generating-orm-code",
      "description": "Execute use when you need to work with ORM code generation. This skill provides ORM model and code generation with comprehensive guidance and automation. Trigger with phrases like \"generate ORM models\", \"create entity classes\", or \"scaffold database models\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Orm Code Generator\n\nThis skill provides automated assistance for orm code generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/orm-code-generator/`\n\n**Documentation and Guides**: `{baseDir}/docs/orm-code-generator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/orm-code-generator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/orm-code-generator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/orm-code-generator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/orm-code-generator-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "orm-code-generator",
        "category": "database",
        "path": "plugins/database/orm-code-generator",
        "version": "1.0.0",
        "description": "Generate ORM models from database schemas or create database schemas from models for TypeORM, Prisma, Sequelize, SQLAlchemy, and more"
      },
      "filePath": "plugins/database/orm-code-generator/skills/generating-orm-code/SKILL.md"
    },
    {
      "slug": "generating-rest-apis",
      "name": "generating-rest-apis",
      "description": "Generate complete REST API implementations from OpenAPI specifications or database schemas. Use when generating RESTful API implementations. Trigger with phrases like \"generate REST API\", \"create RESTful API\", or \"build REST endpoints\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:rest-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Generating Rest Apis\n\n## Overview\n\n\nThis skill provides automated assistance for rest api generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:rest-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "rest-api-generator",
        "category": "api-development",
        "path": "plugins/api-development/rest-api-generator",
        "version": "1.0.0",
        "description": "Generate RESTful APIs from schemas with proper routing, validation, and documentation"
      },
      "filePath": "plugins/api-development/rest-api-generator/skills/generating-rest-apis/SKILL.md"
    },
    {
      "slug": "generating-security-audit-reports",
      "name": "generating-security-audit-reports",
      "description": "Generate comprehensive security audit reports for applications and systems. Use when you need to assess security posture, identify vulnerabilities, evaluate compliance status, or create formal security documentation. Trigger with phrases like \"create security audit report\", \"generate security assessment\", \"audit security posture\", or \"PCI-DSS compliance report\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(security-scan:*), Bash(report-gen:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Generating Security Audit Reports\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Security scan data or logs are available in {baseDir}/security/\n- Access to application configuration files\n- Security tool outputs (e.g., vulnerability scanners, SAST/DAST results)\n- Compliance framework documentation (if applicable)\n- Write permissions for generating report files\n\n## Instructions\n\n1. Collect available security signals (scanner outputs, configs, logs).\n2. Analyze findings and map to risk + compliance requirements.\n3. Generate a report with prioritized remediation guidance.\n4. Format outputs (Markdown/HTML/PDF) and include evidence links.\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: Comprehensive security audit report saved to {baseDir}/reports/security-audit-YYYYMMDD.md\n\n**Report Structure**:\n```\n# Security Audit Report - [System Name]\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- OWASP Top 10: https://owasp.org/www-project-top-ten/\n- CWE Top 25: https://cwe.mitre.org/top25/\n- NIST Cybersecurity Framework: https://www.nist.gov/cyberframework\n- PCI-DSS Requirements: https://www.pcisecuritystandards.org/\n- GDPR Compliance Checklist: https://gdpr.eu/checklist/",
      "parentPlugin": {
        "name": "security-audit-reporter",
        "category": "security",
        "path": "plugins/security/security-audit-reporter",
        "version": "1.0.0",
        "description": "Generate comprehensive security audit reports"
      },
      "filePath": "plugins/security/security-audit-reporter/skills/generating-security-audit-reports/SKILL.md"
    },
    {
      "slug": "generating-smart-commits",
      "name": "generating-smart-commits",
      "description": "Execute use when generating conventional commit messages from staged git changes. Trigger with phrases like \"create commit message\", \"generate smart commit\", \"/commit-smart\", or \"/gc\". Automatically analyzes changes to determine commit type (feat, fix, docs), identifies breaking changes, and formats according to conventional commit standards. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(git:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Git Commit Smart\n\nThis skill provides automated assistance for git commit smart tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Git repository is initialized in {baseDir}\n- Changes are staged using `git add`\n- User has permission to create commits\n- Git user name and email are configured\n\n## Instructions\n\n1. **Analyze Staged Changes**: Examine git diff output to understand modifications\n2. **Determine Commit Type**: Classify changes as feat, fix, docs, style, refactor, test, or chore\n3. **Identify Scope**: Extract affected module or component from file paths\n4. **Detect Breaking Changes**: Look for API changes, removed features, or incompatible modifications\n5. **Format Message**: Construct message following pattern: `type(scope): description`\n6. **Present for Review**: Show generated message and ask for confirmation before committing\n\n## Output\n\nGenerates conventional commit messages in this format:\n\n```\ntype(scope): brief description\n\n- Detailed explanation of changes\n- Why the change was necessary\n- Impact on existing functionality\n\nBREAKING CHANGE: description if applicable\n```\n\nExamples:\n- `feat(auth): implement JWT authentication middleware`\n- `fix(api): resolve null pointer exception in user endpoint`\n- `docs(readme): update installation instructions`\n\n## Error Handling\n\nCommon issues and solutions:\n\n**No Staged Changes**\n- Error: \"No changes staged for commit\"\n- Solution: Stage files using `git add <files>` before generating commit message\n\n**Git Not Initialized**\n- Error: \"Not a git repository\"\n- Solution: Initialize git with `git init` or navigate to repository root\n\n**Uncommitted Changes**\n- Warning: \"Unstaged changes detected\"\n- Solution: Stage relevant changes or use `git stash` for unrelated modifications\n\n**Invalid Commit Format**\n- Error: \"Generated message doesn't follow conventional format\"\n- Solution: Review and manually adjust type, scope, or description\n\n## Resources\n\n- Conventional Commits specification: https://www.conventionalcommits.org/\n- Git commit best practices documentation\n- Repository commit history for style consistency\n- Project-specific commit guidelines in {baseDir}/000-docs/007-DR-GUID-contributing.md\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "git-commit-smart",
        "category": "devops",
        "path": "plugins/devops/git-commit-smart",
        "version": "1.0.0",
        "description": "AI-powered conventional commit message generator with smart analysis"
      },
      "filePath": "plugins/devops/git-commit-smart/skills/generating-smart-commits/SKILL.md"
    },
    {
      "slug": "generating-stored-procedures",
      "name": "generating-stored-procedures",
      "description": "Execute use when you need to work with stored procedure generation. This skill provides stored procedure code generation with comprehensive guidance and automation. Trigger with phrases like \"generate stored procedures\", \"create database functions\", or \"write SQL procedures\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Stored Procedure Generator\n\nThis skill provides automated assistance for stored procedure generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/stored-procedure-generator/`\n\n**Documentation and Guides**: `{baseDir}/docs/stored-procedure-generator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/stored-procedure-generator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/stored-procedure-generator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/stored-procedure-generator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/stored-procedure-generator-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "stored-procedure-generator",
        "category": "database",
        "path": "plugins/database/stored-procedure-generator",
        "version": "1.0.0",
        "description": "Database plugin for stored-procedure-generator"
      },
      "filePath": "plugins/database/stored-procedure-generator/skills/generating-stored-procedures/SKILL.md"
    },
    {
      "slug": "generating-test-data",
      "name": "generating-test-data",
      "description": "Generate realistic test data including edge cases and boundary conditions. Use when creating realistic fixtures or edge case test data. Trigger with phrases like \"generate test data\", \"create fixtures\", or \"setup test database\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:data-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Test Data Generator\n\nThis skill provides automated assistance for test data generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:data-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for test data generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "test-data-generator",
        "category": "testing",
        "path": "plugins/testing/test-data-generator",
        "version": "1.0.0",
        "description": "Generate realistic test data including users, products, orders, and custom schemas for comprehensive testing"
      },
      "filePath": "plugins/testing/test-data-generator/skills/generating-test-data/SKILL.md"
    },
    {
      "slug": "generating-test-doubles",
      "name": "generating-test-doubles",
      "description": "Generate mocks, stubs, spies, and fakes for dependency isolation. Use when creating mocks, stubs, or test isolation fixtures. Trigger with phrases like \"generate mocks\", \"create test doubles\", or \"setup stubs\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:doubles-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Test Doubles Generator\n\nThis skill provides automated assistance for test doubles generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:doubles-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for test doubles generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "test-doubles-generator",
        "category": "testing",
        "path": "plugins/testing/test-doubles-generator",
        "version": "1.0.0",
        "description": "Generate mocks, stubs, spies, and fakes for unit testing with Jest, Sinon, and test frameworks"
      },
      "filePath": "plugins/testing/test-doubles-generator/skills/generating-test-doubles/SKILL.md"
    },
    {
      "slug": "generating-test-reports",
      "name": "generating-test-reports",
      "description": "Generate comprehensive test reports with metrics, coverage, and visualizations. Use when performing specialized testing. Trigger with phrases like \"generate test report\", \"create test documentation\", or \"show test metrics\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:report-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Test Report Generator\n\nThis skill provides automated assistance for test report generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:report-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for test report generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "test-report-generator",
        "category": "testing",
        "path": "plugins/testing/test-report-generator",
        "version": "1.0.0",
        "description": "Generate comprehensive test reports with coverage, trends, and stakeholder-friendly formats"
      },
      "filePath": "plugins/testing/test-report-generator/skills/generating-test-reports/SKILL.md"
    },
    {
      "slug": "generating-trading-signals",
      "name": "generating-trading-signals",
      "description": "Generate trading signals using technical indicators and on-chain metrics. Use when receiving trading signals and alerts. Trigger with phrases like \"get trading signals\", \"check indicators\", or \"analyze signals\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:signals-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Generating Trading Signals\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:signals-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "crypto-signal-generator",
        "category": "crypto",
        "path": "plugins/crypto/crypto-signal-generator",
        "version": "1.0.0",
        "description": "Generate trading signals from technical indicators and market analysis"
      },
      "filePath": "plugins/crypto/crypto-signal-generator/skills/generating-trading-signals/SKILL.md"
    },
    {
      "slug": "generating-unit-tests",
      "name": "generating-unit-tests",
      "description": "Test automatically generate comprehensive unit tests from source code covering happy paths, edge cases, and error conditions. Use when creating test coverage for functions, classes, or modules. Trigger with phrases like \"generate unit tests\", \"create tests for\", or \"add test coverage\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:unit-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Unit Test Generator\n\nThis skill provides automated assistance for unit test generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Source code files requiring test coverage\n- Testing framework installed (Jest, Mocha, pytest, JUnit, etc.)\n- Understanding of code dependencies and external services to mock\n- Test directory structure established (e.g., `tests/`, `__tests__/`, `spec/`)\n- Package configuration updated with test scripts\n\n## Instructions\n\n### Step 1: Analyze Source Code\nExamine code structure and identify test requirements:\n1. Use Read tool to load source files from {baseDir}/src/\n2. Identify all functions, classes, and methods requiring tests\n3. Document function signatures, parameters, return types, and side effects\n4. Note external dependencies requiring mocking or stubbing\n\n### Step 2: Determine Testing Framework\nSelect appropriate testing framework based on language:\n- JavaScript/TypeScript: Jest, Mocha, Jasmine, Vitest\n- Python: pytest, unittest, nose2\n- Java: JUnit 5, TestNG\n- Go: testing package with testify assertions\n- Ruby: RSpec, Minitest\n\n### Step 3: Generate Test Cases\nCreate comprehensive test suite covering:\n1. Happy path tests with valid inputs and expected outputs\n2. Edge case tests with boundary values (empty arrays, null, zero, max values)\n3. Error condition tests with invalid inputs\n4. Mock external dependencies (databases, APIs, file systems)\n5. Setup and teardown fixtures for test isolation\n\n### Step 4: Write Test File\nGenerate test file in {baseDir}/tests/ with structure:\n- Import statements for code under test and testing framework\n- Mock declarations for external dependencies\n- Describe/context blocks grouping related tests\n- Individual test cases with arrange-act-assert pattern\n- Cleanup logic in afterEach/tearDown hooks\n\n## Output\n\nThe skill generates complete test files:\n\n### Test File Structure\n```javascript\n// Example Jest test file\nimport { validator } from '../src/utils/validator';\n\ndescribe('Validator', () => {\n  describe('validateEmail', () => {\n    it('should accept valid email addresses', () => {\n      expect(validator.validateEmail('test@example.com')).toBe(true);\n    });\n\n    it('should reject invalid email formats', () => {\n      expect(validator.validateEmail('invalid-email')).toBe(false);\n    });\n\n    it('should handle null and undefined', () => {\n      expect(validator.validateEmail(null)).toBe(false);\n      expect(validator.validateEmail(undefined)).toBe(false);\n    });\n  });\n});\n```\n\n### Coverage Metrics\n- Line coverage percentage (target: 80%+)\n- Branch coverage showing tested conditional paths\n- Function coverage ensuring all exports are tested\n- Statement coverage for comprehensive validation\n\n### Mock Implementations\nGenerated mocks for:\n- Database connections and queries\n- HTTP requests to external APIs\n- File system operations (read/write)\n- Environment variables and configuration\n- Time-dependent functions (Date.now(), setTimeout)\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Module Import Errors**\n- Error: Cannot find module or dependencies\n- Solution: Install missing packages; verify import paths match project structure; check TypeScript configuration\n\n**Mock Setup Failures**\n- Error: Mock not properly intercepting calls\n- Solution: Ensure mocks are defined before imports; use proper mocking syntax for framework; clear mocks between tests\n\n**Async Test Timeouts**\n- Error: Test exceeded timeout before completing\n- Solution: Increase timeout for slow operations; ensure async/await or done callbacks are used correctly; check for unresolved promises\n\n**Test Isolation Issues**\n- Error: Tests pass individually but fail when run together\n- Solution: Add proper cleanup in afterEach hooks; avoid shared mutable state; reset mocks between tests\n\n## Resources\n\n### Testing Frameworks\n- Jest documentation for JavaScript testing\n- pytest documentation for Python testing\n- JUnit 5 User Guide for Java testing\n- Go testing package and testify library\n\n### Best Practices\n- Follow AAA pattern (Arrange, Act, Assert) for test structure\n- Write tests before fixing bugs (test-driven bug fixing)\n- Use descriptive test names that explain the scenario\n- Keep tests independent and avoid test interdependencies\n- Mock external dependencies for unit test isolation\n- Aim for 80%+ code coverage on critical paths\n\n## Overview\n\n\nThis skill provides automated assistance for unit test generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "unit-test-generator",
        "category": "testing",
        "path": "plugins/testing/unit-test-generator",
        "version": "1.0.0",
        "description": "Automatically generate comprehensive unit tests from source code with multiple testing framework support"
      },
      "filePath": "plugins/testing/unit-test-generator/skills/generating-unit-tests/SKILL.md"
    },
    {
      "slug": "genkit-infra-expert",
      "name": "genkit-infra-expert",
      "description": "Execute use when deploying Genkit applications to production with Terraform. Trigger with phrases like \"deploy genkit terraform\", \"provision genkit infrastructure\", \"firebase functions terraform\", \"cloud run deployment\", or \"genkit production infrastructure\". Provisions Firebase Functions, Cloud Run services, GKE clusters, monitoring dashboards, and CI/CD for AI workflows. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(terraform:*), Bash(gcloud:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Genkit Infra Expert\n\n## Overview\n\nDeploy Genkit applications to production with Terraform (Firebase Functions, Cloud Run, or GKE) with secure secrets handling and observability. Use this skill to choose a target, generate the Terraform baseline, wire up Secret Manager, and provide a validation checklist for your Genkit flows.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Google Cloud project with Firebase enabled\n- Terraform 1.0+ installed\n- gcloud and firebase CLI authenticated\n- Genkit application built and containerized\n- API keys for Gemini or other AI models\n- Understanding of Genkit flows and deployment options\n\n## Instructions\n\n1. **Choose Deployment Target**: Firebase Functions, Cloud Run, or GKE\n2. **Configure Terraform Backend**: Set up remote state in GCS\n3. **Define Variables**: Project ID, region, Genkit app configuration\n4. **Provision Compute**: Deploy functions or containers\n5. **Configure Secrets**: Store API keys in Secret Manager\n6. **Set Up Monitoring**: Create dashboards for token usage and latency\n7. **Enable Auto-scaling**: Configure min/max instances\n8. **Validate Deployment**: Test Genkit flows via HTTP endpoints\n\n## Output\n\n\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Genkit Deployment: https://genkit.dev/docs/deployment\n- Firebase Terraform: https://registry.terraform.io/providers/hashicorp/google/latest\n- Genkit examples in {baseDir}/genkit-examples/",
      "parentPlugin": {
        "name": "jeremy-genkit-terraform",
        "category": "devops",
        "path": "plugins/devops/jeremy-genkit-terraform",
        "version": "1.0.0",
        "description": "Terraform modules for Firebase Genkit infrastructure and deployments"
      },
      "filePath": "plugins/devops/jeremy-genkit-terraform/skills/genkit-infra-expert/SKILL.md"
    },
    {
      "slug": "genkit-production-expert",
      "name": "genkit-production-expert",
      "description": "Build production Firebase Genkit applications including RAG systems, multi-step flows, and tool calling for Node.js/Python/Go. Deploy to Firebase Functions or Cloud Run with AI monitoring. Use when asked to \"create genkit flow\" or \"implement RAG\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Genkit Production Expert\n\n## Overview\n\n\nThis skill provides automated assistance for genkit production expert tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "jeremy-genkit-pro",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-genkit-pro",
        "version": "1.0.0",
        "description": "Firebase Genkit expert for production-ready AI workflows with RAG and tool calling"
      },
      "filePath": "plugins/ai-ml/jeremy-genkit-pro/skills/genkit-production-expert/SKILL.md"
    },
    {
      "slug": "gh-actions-validator",
      "name": "gh-actions-validator",
      "description": "Validate use when validating GitHub Actions workflows for Google Cloud and Vertex AI deployments. Trigger with phrases like \"validate github actions\", \"setup workload identity federation\", \"github actions security\", \"deploy agent with ci/cd\", or \"automate vertex ai deployment\". Enforces Workload Identity Federation (WIF), validates OIDC permissions, ensures least privilege IAM, and implements security best practices. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(git:*), Bash(gcloud:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Gh Actions Validator\n\n## Overview\n\nValidate and harden GitHub Actions workflows that deploy to Google Cloud (especially Vertex AI) using Workload Identity Federation (OIDC) instead of long-lived service account keys. Use this to audit existing workflows, propose a secure replacement, and add CI checks that prevent common credential and permission mistakes.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- GitHub repository with Actions enabled\n- Google Cloud project with billing enabled\n- gcloud CLI authenticated with admin permissions\n- Understanding of Workload Identity Federation concepts\n- GitHub repository secrets configured\n- Appropriate IAM roles for CI/CD automation\n\n## Instructions\n\n1. **Audit Existing Workflows**: Scan .github/workflows/ for security issues\n2. **Validate WIF Usage**: Ensure no JSON service account keys are used\n3. **Check OIDC Permissions**: Verify id-token: write is present\n4. **Review IAM Roles**: Confirm least privilege (no owner/editor roles)\n5. **Add Security Scans**: Include secret detection and vulnerability scanning\n6. **Validate Deployments**: Add post-deployment health checks\n7. **Configure Monitoring**: Set up alerts for deployment failures\n8. **Document WIF Setup**: Provide one-time WIF configuration commands\n\n## Output\n\n      - uses: actions/checkout@v4\n      - name: Authenticate to GCP (WIF)\n      - name: Deploy to Vertex AI\n            --project=${{ secrets.GCP_PROJECT_ID }} \\\n            --region=us-central1\n      - name: Validate Deployment\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Workload Identity Federation: https://cloud.google.com/iam/docs/workload-identity-federation\n- GitHub OIDC: https://docs.github.com/en/actions/deployment/security-hardening-your-deployments\n- Vertex AI Agent Engine: https://cloud.google.com/vertex-ai/docs/agent-engine\n- google-github-actions/auth: https://github.com/google-github-actions/auth\n- WIF setup guide in {baseDir}/docs/wif-setup.md",
      "parentPlugin": {
        "name": "jeremy-github-actions-gcp",
        "category": "devops",
        "path": "plugins/devops/jeremy-github-actions-gcp",
        "version": "1.0.0",
        "description": "GitHub Actions CI/CD workflows for Google Cloud and Vertex AI deployments"
      },
      "filePath": "plugins/devops/jeremy-github-actions-gcp/skills/gh-actions-validator/SKILL.md"
    },
    {
      "slug": "google-cloud-agent-sdk-master",
      "name": "google-cloud-agent-sdk-master",
      "description": "Execute automatic activation for all google cloud agent development kit (adk) Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, WebFetch, WebSearch, Grep version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Google Cloud Agent SDK Master\n\nMaster Google’s Agent Development Kit (ADK) patterns for building and deploying production-grade agents with clear tool contracts, validation, and operational guardrails.\n\n## Overview\n\nUse this skill to quickly answer “how do I do X with Google ADK?” and to produce a safe, production-oriented plan (structure, patterns, deployment, verification) rather than ad-hoc snippets.\n\n## Examples\n\n**Example: Pick the right ADK pattern**\n- Request: “Should this be a single agent or a multi-agent orchestrator?”\n- Output: an architecture recommendation with tradeoffs, plus a minimal scaffold plan.\n\n## Prerequisites\n\n- The target environment (local-only vs Vertex AI Agent Engine)\n- The agent’s core job, expected inputs/outputs, and required tools\n- Any constraints (latency, cost, compliance/security)\n\n## Instructions\n\n1. Clarify requirements and choose an ADK architecture (single vs multi-agent; orchestration pattern).\n2. Define tool interfaces (inputs, outputs, and error contracts) and how secrets are managed.\n3. Provide an implementation plan with a minimal scaffold and incremental milestones.\n4. Add validation: smoke prompts, regression tests, and deployment verification steps.\n\n## Output\n\n- A recommended ADK architecture and scaffold layout\n- A checklist of commands to validate locally and in CI\n- Optional: deployment steps and post-deploy health checks\n\n## Error Handling\n\n- If documentation conflicts, prefer the latest canonical standards in `000-docs/6767-*`.\n- If an API feature is unavailable in a region/version, propose a compatible alternative.\n\n## Resources\n\n- Full detailed guide (kept for reference): `{baseDir}/references/SKILL.full.md`\n- ADK / Agent Engine docs: https://cloud.google.com/vertex-ai/docs/agent-engine\n- Canonical repo standards: `000-docs/6767-a-SPEC-DR-STND-claude-code-plugins-standard.md`",
      "parentPlugin": {
        "name": "004-jeremy-google-cloud-agent-sdk",
        "category": "productivity",
        "path": "plugins/productivity/004-jeremy-google-cloud-agent-sdk",
        "version": "1.0.0",
        "description": "Google Cloud Agent Development Kit (ADK) and Agent Starter Pack mastery - build containerized multi-agent systems with production-ready templates, deploy to Cloud Run/GKE/Agent Engine, RAG agents, ReAct agents, and multi-agent orchestration."
      },
      "filePath": "plugins/productivity/004-jeremy-google-cloud-agent-sdk/skills/google-cloud-agent-sdk-master/SKILL.md"
    },
    {
      "slug": "handling-api-errors",
      "name": "handling-api-errors",
      "description": "Implement standardized error handling with proper HTTP status codes and error responses. Use when implementing standardized error handling. Trigger with phrases like \"add error handling\", \"standardize errors\", or \"implement error responses\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:error-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Handling Api Errors\n\n## Overview\n\n\nThis skill provides automated assistance for api error handler tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:error-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-error-handler",
        "category": "api-development",
        "path": "plugins/api-development/api-error-handler",
        "version": "1.0.0",
        "description": "Implement standardized error handling with proper HTTP status codes"
      },
      "filePath": "plugins/api-development/api-error-handler/skills/handling-api-errors/SKILL.md"
    },
    {
      "slug": "implementing-backup-strategies",
      "name": "implementing-backup-strategies",
      "description": "Execute use when you need to work with backup and recovery. This skill provides backup automation and disaster recovery with comprehensive guidance and automation. Trigger with phrases like \"create backups\", \"automate backups\", or \"implement disaster recovery\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(tar:*), Bash(rsync:*), Bash(aws:s3:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Backup Strategy Implementor\n\nThis skill provides automated assistance for backup strategy implementor tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/backup-strategy-implementor/`\n\n**Documentation and Guides**: `{baseDir}/docs/backup-strategy-implementor/`\n\n**Example Scripts and Code**: `{baseDir}/examples/backup-strategy-implementor/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/backup-strategy-implementor-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/backup-strategy-implementor-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/backup-strategy-implementor-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "backup-strategy-implementor",
        "category": "devops",
        "path": "plugins/devops/backup-strategy-implementor",
        "version": "1.0.0",
        "description": "Implement backup strategies for databases and applications"
      },
      "filePath": "plugins/devops/backup-strategy-implementor/skills/implementing-backup-strategies/SKILL.md"
    },
    {
      "slug": "implementing-database-audit-logging",
      "name": "implementing-database-audit-logging",
      "description": "Process use when you need to track database changes for compliance and security monitoring. This skill implements audit logging using triggers, application-level logging, CDC, or native logs. Trigger with phrases like \"implement database audit logging\", \"add audit trails\", \"track database changes\", or \"monitor database activity for compliance\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Audit Logger\n\nThis skill provides automated assistance for database audit logger tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Database credentials with CREATE TABLE and CREATE TRIGGER permissions\n- Understanding of compliance requirements (GDPR, HIPAA, SOX, PCI-DSS)\n- Sufficient storage for audit logs (estimate 10-30% of data size)\n- Decision on audit log retention period\n- Access to database documentation for table schemas\n- Monitoring tools configured for audit log analysis\n\n## Instructions\n\n### Step 1: Define Audit Requirements\n1. Identify tables requiring audit logging based on compliance needs\n2. Determine events to audit (INSERT, UPDATE, DELETE, SELECT for sensitive data)\n3. Define which columns contain sensitive data requiring audit\n4. Document retention requirements for audit logs\n5. Identify users/roles whose actions need auditing\n\n### Step 2: Choose Audit Strategy\n1. **Trigger-Based Auditing**: Best for comprehensive row-level tracking\n   - Pros: Automatic, no application changes, captures all changes\n   - Cons: Performance overhead, complex trigger maintenance\n2. **Application-Level Auditing**: Best for selective auditing\n   - Pros: Flexible, lower database overhead, easier debugging\n   - Cons: Requires application changes, can miss direct database changes\n3. **Change Data Capture (CDC)**: Best for real-time streaming\n   - Pros: Minimal performance impact, real-time analysis, external processing\n   - Cons: Complex setup, requires CDC infrastructure\n4. **Native Database Logs**: Best for general monitoring\n   - Pros: No setup, captures everything, built-in\n   - Cons: High volume, limited retention, difficult to query\n\n### Step 3: Design Audit Table Schema\n1. Create audit log table with these core columns:\n   - audit_id (primary key), table_name, action (INSERT/UPDATE/DELETE)\n   - record_id (reference to audited record), old_values (JSON), new_values (JSON)\n   - changed_by (user), changed_at (timestamp), client_ip, application_context\n2. Add indexes on table_name, changed_at, changed_by for query performance\n3. Partition audit table by date for efficient archival\n4. Configure tablespace for audit logs separate from primary data\n\n### Step 4: Implement Audit Mechanism\n1. For trigger-based: Create AFTER INSERT/UPDATE/DELETE triggers on each table\n2. Capture old and new row values as JSON in trigger body\n3. Record user context (CURRENT_USER, application user, IP address)\n4. Handle trigger failures gracefully (log but don't block operations)\n5. Test triggers with sample data modifications\n\n### Step 5: Configure Audit Log Management\n1. Set up automated archival of old audit logs to cold storage\n2. Implement audit log analysis queries for common compliance reports\n3. Create alerts for suspicious activities (bulk deletes, off-hours changes)\n4. Document audit log query procedures for compliance auditors\n5. Schedule periodic audit log reviews with security team\n\n### Step 6: Validate Audit Implementation\n1. Perform test operations on audited tables\n2. Verify audit log entries are created with complete data\n3. Test audit log queries for performance\n4. Confirm audit logs cannot be modified by regular users\n5. Document audit implementation for compliance documentation\n\n## Output\n\nThis skill produces:\n\n**Audit Table Schema**: SQL DDL for audit log table with proper indexes and partitioning\n\n**Audit Triggers**: Database triggers for automatic audit log population on data changes\n\n**Audit Log Queries**: Pre-built SQL queries for compliance reports and change tracking\n\n**Implementation Documentation**: Configuration details, trigger logic, and maintenance procedures\n\n**Compliance Report Templates**: SQL queries for GDPR access logs, SOX change reports, etc.\n\n## Error Handling\n\n**Trigger Performance Issues**:\n- Audit only critical tables, not all tables\n- Use asynchronous audit logging with queue systems\n- Batch audit log inserts instead of individual inserts\n- Monitor trigger execution time and optimize trigger logic\n\n**Audit Table Growth**:\n- Implement automated archival of audit logs older than retention period\n- Partition audit table by month or quarter\n- Compress old audit log partitions\n- Move historical audit logs to cheaper storage tiers\n\n**Missing Audit Context**:\n- Set application context in database session before operations\n- Use database session variables to pass user identity\n- Implement connection pooling with session initialization\n- Log application user separately from database user\n\n**Permission Issues**:\n- Ensure audit log table is writable by trigger execution context\n- Grant INSERT on audit table to all database users\n- Protect audit table from modifications (no UPDATE/DELETE grants)\n- Use separate schema for audit tables with restricted access\n\n## Resources\n\n**Audit Table Templates**:\n- PostgreSQL audit trigger: `{baseDir}/templates/postgresql-audit-trigger.sql`\n- MySQL audit trigger: `{baseDir}/templates/mysql-audit-trigger.sql`\n- Audit table schema: `{baseDir}/templates/audit-table-schema.sql`\n\n**Compliance Report Queries**: `{baseDir}/queries/compliance-reports/`\n- GDPR data access report\n- SOX change audit report\n- User activity summary\n- Suspicious activity detection\n\n**Audit Strategy Guide**: `{baseDir}/docs/audit-strategy-selection.md`\n**Performance Tuning**: `{baseDir}/docs/audit-performance-optimization.md`\n**Archival Procedures**: `{baseDir}/scripts/audit-archival.sh`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-audit-logger",
        "category": "database",
        "path": "plugins/database/database-audit-logger",
        "version": "1.0.0",
        "description": "Database plugin for database-audit-logger"
      },
      "filePath": "plugins/database/database-audit-logger/skills/implementing-database-audit-logging/SKILL.md"
    },
    {
      "slug": "implementing-database-caching",
      "name": "implementing-database-caching",
      "description": "Process use when you need to implement multi-tier caching to improve database performance. This skill sets up Redis, in-memory caching, and CDN layers to reduce database load. Trigger with phrases like \"implement database caching\", \"add Redis cache layer\", \"improve query performance with caching\", or \"reduce database load\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(redis-cli:*), Bash(docker:redis:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Cache Layer\n\nThis skill provides automated assistance for database cache layer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Redis server available or ability to deploy Redis container\n- Understanding of application data access patterns and hotspots\n- Knowledge of which queries/data benefit most from caching\n- Monitoring tools to measure cache hit rates and performance\n- Development environment for testing caching implementation\n- Understanding of cache invalidation requirements for data consistency\n\n## Instructions\n\n### Step 1: Analyze Caching Requirements\n1. Profile database queries to identify slow or frequently executed queries\n2. Determine which data is read-heavy vs write-heavy\n3. Identify data that can tolerate eventual consistency\n4. Calculate expected cache size and Redis memory requirements\n5. Document current database load and target performance metrics\n\n### Step 2: Choose Caching Strategy\n1. **Cache-Aside (Lazy Loading)**: Application checks cache first, loads from DB on miss\n   - Best for: Read-heavy workloads, unpredictable access patterns\n   - Pros: Only caches requested data, simple to implement\n   - Cons: Cache misses incur database hit, stale data possible\n2. **Write-Through**: Application writes to cache and database simultaneously\n   - Best for: Write-heavy workloads needing consistency\n   - Pros: Cache always consistent, no stale data\n   - Cons: Write latency, unnecessary caching of rarely-read data\n3. **Write-Behind (Write-Back)**: Application writes to cache, async writes to database\n   - Best for: High write throughput requirements\n   - Pros: Low write latency, batched database writes\n   - Cons: Risk of data loss, complexity in implementation\n\n### Step 3: Design Cache Architecture\n1. Set up Redis as distributed cache layer (L2 cache)\n2. Implement in-memory LRU cache in application (L1 cache)\n3. Configure CDN for static assets (images, CSS, JS)\n4. Design cache key naming convention (e.g., `user:123:profile`)\n5. Define TTL (Time To Live) for different data types\n\n### Step 4: Implement Caching Code\n1. Add Redis client library to application dependencies\n2. Create cache wrapper functions (get, set, delete, invalidate)\n3. Modify database query code to check cache before DB query\n4. Implement cache population on cache miss\n5. Add error handling for cache failures (fail gracefully to database)\n\n### Step 5: Configure Cache Invalidation\n1. Implement TTL-based expiration for time-sensitive data\n2. Add explicit cache invalidation on data updates/deletes\n3. Use cache tags or patterns for bulk invalidation\n4. Implement cache warming for critical data after deployments\n5. Set up cache stampede prevention (lock/queue on miss)\n\n### Step 6: Monitor and Optimize\n1. Track cache hit rate, miss rate, and eviction rate\n2. Monitor Redis memory usage and eviction policy\n3. Analyze query performance improvements\n4. Adjust TTLs based on data update frequency\n5. Identify and cache additional hot data\n\n## Output\n\nThis skill produces:\n\n**Redis Configuration**: Docker Compose or config files for Redis deployment with appropriate memory and eviction settings\n\n**Caching Code**: Application code implementing cache-aside, write-through, or write-behind patterns\n\n**Cache Key Schema**: Documentation of cache key naming conventions and TTL settings\n\n**Monitoring Dashboards**: Metrics for cache hit rates, memory usage, and performance improvements\n\n**Cache Invalidation Logic**: Code for explicit and implicit cache invalidation on data changes\n\n## Error Handling\n\n**Cache Connection Failures**:\n- Implement circuit breaker pattern to prevent cascading failures\n- Fall back to database when cache is unavailable\n- Log cache connection errors for monitoring\n- Retry cache connections with exponential backoff\n- Consider read-replica or cache cluster for high availability\n\n**Cache Stampede**:\n- Implement probabilistic early expiration (PER) for TTLs\n- Use distributed locks (Redis SETNX) to prevent concurrent cache population\n- Queue cache refresh requests instead of parallel execution\n- Add jitter to TTLs to spread expiration times\n- Use stale-while-revalidate pattern for acceptable delays\n\n**Stale Data Issues**:\n- Implement versioning in cache keys (e.g., `user:123:v2`)\n- Use cache tags for related data invalidation\n- Set aggressive TTLs for frequently changing data\n- Implement active cache invalidation on data updates\n- Monitor data consistency between cache and database\n\n**Memory Pressure**:\n- Configure Redis eviction policy (allkeys-lru recommended)\n- Monitor Redis memory usage and set max memory limits\n- Implement tiered caching (hot data in Redis, warm data in DB)\n- Reduce TTLs for less critical data\n- Scale Redis horizontally with cluster mode\n\n## Resources\n\n**Redis Configuration Templates**:\n- Docker Compose: `{baseDir}/docker/redis-compose.yml`\n- Redis config: `{baseDir}/config/redis.conf`\n- Cluster config: `{baseDir}/config/redis-cluster.conf`\n\n**Caching Code Examples**: `{baseDir}/examples/caching/`\n- Cache-aside pattern (Node.js, Python, Java)\n- Write-through pattern\n- Cache invalidation strategies\n- Distributed locking\n\n**Cache Key Design Guide**: `{baseDir}/docs/cache-key-design.md`\n**Performance Tuning**: `{baseDir}/docs/cache-performance-tuning.md`\n**Monitoring Setup**: `{baseDir}/monitoring/redis-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-cache-layer",
        "category": "database",
        "path": "plugins/database/database-cache-layer",
        "version": "1.0.0",
        "description": "Database plugin for database-cache-layer"
      },
      "filePath": "plugins/database/database-cache-layer/skills/implementing-database-caching/SKILL.md"
    },
    {
      "slug": "implementing-real-user-monitoring",
      "name": "implementing-real-user-monitoring",
      "description": "Implement Real User Monitoring (RUM) to capture actual user performance data including Core Web Vitals and page load times. Use when setting up user experience monitoring or tracking custom performance events. Trigger with phrases like \"setup RUM\", \"track Core Web Vitals\", or \"monitor real user performance\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(npm:*)",
        "Bash(rum:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Real User Monitoring\n\nThis skill provides automated assistance for real user monitoring tasks.\n\n## Overview\n\nThis skill streamlines the process of setting up Real User Monitoring (RUM) for web applications. It guides you through the essential steps of choosing a platform, defining metrics, and implementing the tracking code to capture valuable user experience data.\n\n## How It Works\n\n1. **Platform Selection**: Helps you consider available RUM platforms (e.g., Google Analytics, Datadog RUM, New Relic).\n2. **Instrumentation Design**: Guides you in defining the key performance metrics to track, including Core Web Vitals and custom events.\n3. **Tracking Code Implementation**: Assists in implementing the necessary JavaScript code to collect and transmit performance data.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Implement Real User Monitoring on a website or web application.\n- Track Core Web Vitals (LCP, FID, CLS) to improve user experience.\n- Monitor page load times (FCP, TTI, TTFB) for performance optimization.\n\n## Examples\n\n### Example 1: Setting up RUM for a new website\n\nUser request: \"setup RUM for my new website\"\n\nThe skill will:\n1. Guide the user through selecting a RUM platform.\n2. Provide code snippets for implementing basic tracking.\n\n### Example 2: Tracking custom performance metrics\n\nUser request: \"I want to track how long it takes users to complete a purchase\"\n\nThe skill will:\n1. Help define a custom performance metric for purchase completion time.\n2. Generate JavaScript code to track the metric.\n\n## Best Practices\n\n- **Privacy Compliance**: Ensure compliance with privacy regulations (e.g., GDPR, CCPA) when collecting user data.\n- **Sampling**: Implement sampling to reduce data volume and impact on performance.\n- **Error Handling**: Implement robust error handling to prevent tracking code from breaking the website.\n\n## Integration\n\nThis skill can be used in conjunction with other monitoring and analytics tools to provide a comprehensive view of application performance.\n\n## Prerequisites\n\n- Access to web application frontend code in {baseDir}/\n- RUM platform account (Google Analytics, Datadog, New Relic)\n- Understanding of Core Web Vitals metrics\n- Privacy compliance documentation (GDPR, CCPA)\n\n## Instructions\n\n1. Select appropriate RUM platform for requirements\n2. Define key metrics to track (Core Web Vitals, custom events)\n3. Implement tracking code in application frontend\n4. Configure data sampling and privacy settings\n5. Set up dashboards for metric visualization\n6. Define alerts for performance degradation\n\n## Output\n\n- RUM implementation code snippets\n- Platform configuration documentation\n- Custom event tracking examples\n- Dashboard definitions for key metrics\n- Privacy compliance checklist\n\n## Error Handling\n\nIf RUM implementation fails:\n- Verify platform API credentials\n- Check JavaScript bundle integration\n- Validate metric collection permissions\n- Review privacy consent configuration\n- Ensure network connectivity for data transmission\n\n## Resources\n\n- Core Web Vitals measurement guide\n- RUM platform documentation\n- Privacy compliance best practices\n- Performance monitoring strategies",
      "parentPlugin": {
        "name": "real-user-monitoring",
        "category": "performance",
        "path": "plugins/performance/real-user-monitoring",
        "version": "1.0.0",
        "description": "Implement Real User Monitoring for actual performance data"
      },
      "filePath": "plugins/performance/real-user-monitoring/skills/implementing-real-user-monitoring/SKILL.md"
    },
    {
      "slug": "integrating-secrets-managers",
      "name": "integrating-secrets-managers",
      "description": "Manage this skill enables AI assistant to seamlessly integrate with various secrets managers like hashicorp vault and aws secrets manager. it generates configurations and setup code, ensuring best practices for secure credential management. use this skill when... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Secrets Manager Integrator\n\nThis skill provides automated assistance for secrets manager integrator tasks.\n\n## Overview\n\nThis skill empowers Claude to automate the integration of secrets managers into your infrastructure. It generates the necessary configuration files and setup code, ensuring a secure and efficient workflow for managing sensitive credentials.\n\n## How It Works\n\n1. **Identify Requirements**: Claude analyzes the user's request to determine the specific secrets manager and desired configurations.\n2. **Generate Configuration**: Based on the identified requirements, Claude generates the appropriate configuration files (e.g., Vault policies, AWS IAM roles) and setup code.\n3. **Provide Instructions**: Claude provides clear instructions on how to deploy and configure the generated code and integrate it into the existing infrastructure.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Integrate HashiCorp Vault into your infrastructure.\n- Set up AWS Secrets Manager for secure credential storage.\n- Generate configuration files for managing secrets.\n- Implement best practices for secrets management.\n\n## Examples\n\n### Example 1: Integrating Vault with a Kubernetes Cluster\n\nUser request: \"Integrate Vault with my Kubernetes cluster for managing database credentials.\"\n\nThe skill will:\n1. Generate Vault policies for accessing database credentials.\n2. Create Kubernetes service accounts with appropriate annotations for Vault integration.\n3. Provide instructions for deploying the Vault agent injector to the Kubernetes cluster.\n\n### Example 2: Setting up AWS Secrets Manager for API Keys\n\nUser request: \"Set up AWS Secrets Manager to securely store API keys for my application.\"\n\nThe skill will:\n1. Generate an IAM role with permissions to access AWS Secrets Manager.\n2. Create a Secrets Manager secret containing the API keys.\n3. Provide code snippets for retrieving the API keys from Secrets Manager within the application.\n\n## Best Practices\n\n- **Least Privilege**: Generate configurations that grant only the necessary permissions for accessing secrets.\n- **Secure Storage**: Ensure that secrets are stored securely within the chosen secrets manager.\n- **Regular Rotation**: Implement a strategy for regularly rotating secrets to minimize the impact of potential breaches.\n\n## Integration\n\nThis skill can be used in conjunction with other skills for deploying applications, configuring infrastructure, and automating DevOps workflows. It provides a secure foundation for managing sensitive information across your entire infrastructure.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "secrets-manager-integrator",
        "category": "devops",
        "path": "plugins/devops/secrets-manager-integrator",
        "version": "1.0.0",
        "description": "Integrate with secrets managers (Vault, AWS Secrets Manager, etc)"
      },
      "filePath": "plugins/devops/secrets-manager-integrator/skills/integrating-secrets-managers/SKILL.md"
    },
    {
      "slug": "load-testing-apis",
      "name": "load-testing-apis",
      "description": "Execute comprehensive load and stress testing to validate API performance and scalability. Use when validating API performance under load. Trigger with phrases like \"load test the API\", \"stress test API\", or \"benchmark API performance\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:load-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Load Testing Apis\n\n## Overview\n\n\nThis skill provides automated assistance for api load tester tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:load-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-load-tester",
        "category": "api-development",
        "path": "plugins/api-development/api-load-tester",
        "version": "1.0.0",
        "description": "Load test APIs with k6, Gatling, or Artillery"
      },
      "filePath": "plugins/api-development/api-load-tester/skills/load-testing-apis/SKILL.md"
    },
    {
      "slug": "logging-api-requests",
      "name": "logging-api-requests",
      "description": "Monitor and log API requests with correlation IDs, performance metrics, and security audit trails. Use when auditing API requests and responses. Trigger with phrases like \"log API requests\", \"add API logging\", or \"track API calls\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:log-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Logging Api Requests\n\n## Overview\n\n\nThis skill provides automated assistance for api request logger tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:log-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-request-logger",
        "category": "api-development",
        "path": "plugins/api-development/api-request-logger",
        "version": "1.0.0",
        "description": "Log API requests with structured logging and correlation IDs"
      },
      "filePath": "plugins/api-development/api-request-logger/skills/logging-api-requests/SKILL.md"
    },
    {
      "slug": "managing-api-cache",
      "name": "managing-api-cache",
      "description": "Implement intelligent API response caching with Redis, Memcached, and CDN integration. Use when optimizing API performance with caching. Trigger with phrases like \"add caching\", \"optimize API performance\", or \"implement cache layer\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:cache-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Managing Api Cache\n\n## Overview\n\n\nThis skill provides automated assistance for api cache manager tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:cache-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-cache-manager",
        "category": "api-development",
        "path": "plugins/api-development/api-cache-manager",
        "version": "1.0.0",
        "description": "Implement caching strategies with Redis, CDN, and HTTP headers"
      },
      "filePath": "plugins/api-development/api-cache-manager/skills/managing-api-cache/SKILL.md"
    },
    {
      "slug": "managing-autonomous-development",
      "name": "managing-autonomous-development",
      "description": "Execute enables AI assistant to manage sugar's autonomous development workflows. it allows AI assistant to create tasks, view the status of the system, review pending tasks, and start autonomous execution mode. use this skill when the user asks to create a new develo... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "",
      "license": "MIT",
      "content": "# Sugar\n\nThis skill provides automated assistance for sugar tasks.\n\n## Overview\n\nThis skill empowers Claude to orchestrate and monitor autonomous development processes within the Sugar environment. It provides a set of commands to create, manage, and execute tasks, ensuring efficient and automated software development workflows.\n\n## How It Works\n\n1. **Command Recognition**: Claude identifies the appropriate Sugar command (e.g., `/sugar-task`, `/sugar-status`, `/sugar-review`, `/sugar-run`).\n2. **Parameter Extraction**: Claude extracts relevant parameters from the user's request, such as task type, priority, and execution flags.\n3. **Execution**: Claude executes the corresponding Sugar command with the extracted parameters, interacting with the Sugar plugin.\n4. **Response Generation**: Claude presents the results of the command execution to the user in a clear and informative manner.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a new development task with specific requirements.\n- Check the current status of the Sugar system and task queue.\n- Review and manage pending tasks in the queue.\n- Start or manage the autonomous execution mode.\n\n## Examples\n\n### Example 1: Creating a New Feature Task\n\nUser request: \"/sugar-task Implement user authentication --type feature --priority 4\"\n\nThe skill will:\n1. Parse the request and identify the command as `/sugar-task` with parameters \"Implement user authentication\", `--type feature`, and `--priority 4`.\n2. Execute the `sugar` command to create a new task with the specified parameters.\n3. Confirm the successful creation of the task to the user.\n\n### Example 2: Checking System Status\n\nUser request: \"/sugar-status\"\n\nThe skill will:\n1. Identify the command as `/sugar-status`.\n2. Execute the `sugar` command to retrieve the system status.\n3. Display the system status, including task queue information, to the user.\n\n## Best Practices\n\n- **Clarity**: Always confirm the parameters before executing a command to ensure accuracy.\n- **Safety**: When using `/sugar-run`, strongly advise the user to use `--dry-run --once` first.\n- **Validation**: Recommend validating the Sugar configuration before starting autonomous mode.\n\n## Integration\n\nThis skill integrates directly with the Sugar plugin, leveraging its command-line interface to manage autonomous development workflows. It can be combined with other skills to provide a more comprehensive development experience.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "sugar",
        "category": "devops",
        "path": "plugins/devops/sugar",
        "version": "unknown",
        "description": ""
      },
      "filePath": "plugins/devops/sugar/skills/managing-autonomous-development/SKILL.md"
    },
    {
      "slug": "managing-container-registries",
      "name": "managing-container-registries",
      "description": "Execute use when you need to work with containerization. This skill provides container management and orchestration with comprehensive guidance and automation. Trigger with phrases like \"containerize app\", \"manage containers\", or \"orchestrate deployment\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(docker:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Container Registry Manager\n\nThis skill provides automated assistance for container registry manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/container-registry-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/container-registry-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/container-registry-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/container-registry-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/container-registry-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/container-registry-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "container-registry-manager",
        "category": "devops",
        "path": "plugins/devops/container-registry-manager",
        "version": "1.0.0",
        "description": "Manage container registries (ECR, GCR, Harbor)"
      },
      "filePath": "plugins/devops/container-registry-manager/skills/managing-container-registries/SKILL.md"
    },
    {
      "slug": "managing-database-migrations",
      "name": "managing-database-migrations",
      "description": "Process use when you need to work with database migrations. This skill provides schema migration management with comprehensive guidance and automation. Trigger with phrases like \"create migration\", \"run migrations\", or \"manage schema versions\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Migration Manager\n\nThis skill provides automated assistance for database migration manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-migration-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-migration-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-migration-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-migration-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-migration-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-migration-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-migration-manager",
        "category": "database",
        "path": "plugins/database/database-migration-manager",
        "version": "1.0.0",
        "description": "Manage database migrations with version control, rollback capabilities, and automated schema evolution tracking"
      },
      "filePath": "plugins/database/database-migration-manager/skills/managing-database-migrations/SKILL.md"
    },
    {
      "slug": "managing-database-partitions",
      "name": "managing-database-partitions",
      "description": "Process use when you need to work with database partitioning. This skill provides table partitioning strategies with comprehensive guidance and automation. Trigger with phrases like \"partition tables\", \"implement partitioning\", or \"optimize large tables\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Partition Manager\n\nThis skill provides automated assistance for database partition manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-partition-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-partition-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-partition-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-partition-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-partition-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-partition-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-partition-manager",
        "category": "database",
        "path": "plugins/database/database-partition-manager",
        "version": "1.0.0",
        "description": "Database plugin for database-partition-manager"
      },
      "filePath": "plugins/database/database-partition-manager/skills/managing-database-partitions/SKILL.md"
    },
    {
      "slug": "managing-database-recovery",
      "name": "managing-database-recovery",
      "description": "Process use when you need to work with database operations. This skill provides database management and optimization with comprehensive guidance and automation. Trigger with phrases like \"manage database\", \"optimize database\", or \"configure database\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(tar:*), Bash(rsync:*), Bash(aws:s3:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Recovery Manager\n\nThis skill provides automated assistance for database recovery manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-recovery-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-recovery-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-recovery-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-recovery-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-recovery-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-recovery-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-recovery-manager",
        "category": "database",
        "path": "plugins/database/database-recovery-manager",
        "version": "1.0.0",
        "description": "Database plugin for database-recovery-manager"
      },
      "filePath": "plugins/database/database-recovery-manager/skills/managing-database-recovery/SKILL.md"
    },
    {
      "slug": "managing-database-replication",
      "name": "managing-database-replication",
      "description": "Process use when you need to work with database scalability. This skill provides replication and sharding with comprehensive guidance and automation. Trigger with phrases like \"set up replication\", \"implement sharding\", or \"scale database\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Replication Manager\n\nThis skill provides automated assistance for database replication manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-replication-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-replication-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-replication-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-replication-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-replication-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-replication-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-replication-manager",
        "category": "database",
        "path": "plugins/database/database-replication-manager",
        "version": "1.0.0",
        "description": "Manage database replication, failover, and high availability configurations"
      },
      "filePath": "plugins/database/database-replication-manager/skills/managing-database-replication/SKILL.md"
    },
    {
      "slug": "managing-database-sharding",
      "name": "managing-database-sharding",
      "description": "Process use when you need to work with database sharding. This skill provides horizontal sharding strategies with comprehensive guidance and automation. Trigger with phrases like \"implement sharding\", \"shard database\", or \"distribute data\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Sharding Manager\n\nThis skill provides automated assistance for database sharding manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-sharding-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-sharding-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-sharding-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-sharding-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-sharding-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-sharding-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-sharding-manager",
        "category": "database",
        "path": "plugins/database/database-sharding-manager",
        "version": "1.0.0",
        "description": "Database plugin for database-sharding-manager"
      },
      "filePath": "plugins/database/database-sharding-manager/skills/managing-database-sharding/SKILL.md"
    },
    {
      "slug": "managing-database-tests",
      "name": "managing-database-tests",
      "description": "Test database testing including fixtures, transactions, and rollback management. Use when performing specialized testing. Trigger with phrases like \"test the database\", \"run database tests\", or \"validate data integrity\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:db-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Database Test Manager\n\nThis skill provides automated assistance for database test manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:db-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for database test manager tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-test-manager",
        "category": "testing",
        "path": "plugins/testing/database-test-manager",
        "version": "1.0.0",
        "description": "Database testing utilities with test data setup, transaction rollback, and schema validation"
      },
      "filePath": "plugins/testing/database-test-manager/skills/managing-database-tests/SKILL.md"
    },
    {
      "slug": "managing-deployment-rollbacks",
      "name": "managing-deployment-rollbacks",
      "description": "Deploy use when you need to work with deployment and CI/CD. This skill provides deployment automation and orchestration with comprehensive guidance and automation. Trigger with phrases like \"deploy application\", \"create pipeline\", or \"automate deployment\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(git:*), Bash(docker:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Deployment Rollback Manager\n\nThis skill provides automated assistance for deployment rollback manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/deployment-rollback-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/deployment-rollback-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/deployment-rollback-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/deployment-rollback-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/deployment-rollback-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/deployment-rollback-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "deployment-rollback-manager",
        "category": "devops",
        "path": "plugins/devops/deployment-rollback-manager",
        "version": "1.0.0",
        "description": "Manage and execute deployment rollbacks with safety checks"
      },
      "filePath": "plugins/devops/deployment-rollback-manager/skills/managing-deployment-rollbacks/SKILL.md"
    },
    {
      "slug": "managing-environment-configurations",
      "name": "managing-environment-configurations",
      "description": "Implement environment and configuration management with comprehensive guidance and automation. Use when you need to work with environment configuration. Trigger with phrases like \"manage environments\", \"configure environments\", or \"sync configurations\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Environment Config Manager\n\nThis skill provides automated assistance for environment config manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/environment-config-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/environment-config-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/environment-config-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/environment-config-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/environment-config-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/environment-config-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "environment-config-manager",
        "category": "devops",
        "path": "plugins/devops/environment-config-manager",
        "version": "1.0.0",
        "description": "Manage environment configurations and secrets across deployments"
      },
      "filePath": "plugins/devops/environment-config-manager/skills/managing-environment-configurations/SKILL.md"
    },
    {
      "slug": "managing-network-policies",
      "name": "managing-network-policies",
      "description": "Execute use when managing Kubernetes network policies and firewall rules. Trigger with phrases like \"create network policy\", \"configure firewall rules\", \"restrict pod communication\", or \"setup ingress/egress rules\". Generates Kubernetes NetworkPolicy manifests following least privilege and zero-trust principles. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Network Policy Manager\n\nThis skill provides automated assistance for network policy manager tasks.\n\n## Overview\n\nCreates Kubernetes NetworkPolicy manifests to enforce least-privilege ingress/egress between pods and namespaces, and helps validate connectivity after changes.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Kubernetes cluster has network policy support enabled\n- Network plugin supports policies (Calico, Cilium, Weave)\n- Pod labels are properly defined for policy selectors\n- Understanding of application communication patterns\n- Namespace isolation strategy is defined\n\n## Instructions\n\n1. **Identify Requirements**: Determine which pods need to communicate\n2. **Define Selectors**: Use pod/namespace labels for policy targeting\n3. **Configure Ingress**: Specify allowed incoming traffic sources and ports\n4. **Configure Egress**: Define allowed outgoing traffic destinations\n5. **Test Policies**: Verify connectivity works as expected\n6. **Monitor Denials**: Check for blocked traffic in network plugin logs\n7. **Iterate**: Refine policies based on application behavior\n\n## Output\n\n**Network Policy Examples:**\n```yaml\n# {baseDir}/network-policies/allow-frontend-to-backend.yaml\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-to-backend\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n    - Ingress\n  ingress:\n    - from:\n      - podSelector:\n          matchLabels:\n            app: frontend\n      ports:\n      - protocol: TCP\n        port: 8080\n---\n# Deny all ingress by default\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-ingress\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n    - Ingress\n```\n\n**Egress Policy:**\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-external-api\nspec:\n  podSelector:\n    matchLabels:\n      app: api-client\n  policyTypes:\n    - Egress\n  egress:\n    - to:\n      - namespaceSelector:\n          matchLabels:\n            name: external-services\n      ports:\n      - protocol: TCP\n        port: 443\n```\n\n## Error Handling\n\n**Policy Not Applied**\n- Error: Traffic still blocked/allowed contrary to policy\n- Solution: Verify network plugin supports policies and policy is applied to correct namespace\n\n**DNS Resolution Fails**\n- Error: Pods cannot resolve DNS after applying policy\n- Solution: Add egress rule allowing DNS traffic to kube-dns/coredns\n\n**No Communication After Policy**\n- Error: All traffic blocked unexpectedly\n- Solution: Check for default-deny policies and ensure explicit allow rules exist\n\n**Label Mismatch**\n- Error: Policy not targeting intended pods\n- Solution: Verify pod labels match policy selectors using `kubectl get pods --show-labels`\n\n## Examples\n\n- \"Restrict namespace `prod` so only the ingress controller can reach the web pods on 443.\"\n- \"Create egress rules that allow the API to talk only to Postgres and Redis.\"\n\n## Resources\n\n- Kubernetes NetworkPolicy: https://kubernetes.io/docs/concepts/services-networking/network-policies/\n- Calico documentation: https://docs.projectcalico.org/\n- Example policies in {baseDir}/network-policy-examples/",
      "parentPlugin": {
        "name": "network-policy-manager",
        "category": "devops",
        "path": "plugins/devops/network-policy-manager",
        "version": "1.0.0",
        "description": "Manage Kubernetes network policies and firewall rules"
      },
      "filePath": "plugins/devops/network-policy-manager/skills/managing-network-policies/SKILL.md"
    },
    {
      "slug": "managing-snapshot-tests",
      "name": "managing-snapshot-tests",
      "description": "Create and validate component snapshots for UI regression testing. Use when performing specialized testing. Trigger with phrases like \"update snapshots\", \"test UI snapshots\", or \"validate component snapshots\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:snapshot-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Snapshot Test Manager\n\nThis skill provides automated assistance for snapshot test manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:snapshot-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for snapshot test manager tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "snapshot-test-manager",
        "category": "testing",
        "path": "plugins/testing/snapshot-test-manager",
        "version": "1.0.0",
        "description": "Manage and update snapshot tests with intelligent diff analysis and selective updates"
      },
      "filePath": "plugins/testing/snapshot-test-manager/skills/managing-snapshot-tests/SKILL.md"
    },
    {
      "slug": "managing-ssltls-certificates",
      "name": "managing-ssltls-certificates",
      "description": "Execute this skill enables AI assistant to manage and monitor ssl/tls certificates using the ssl-certificate-manager plugin. it is activated when the user requests actions related to ssl certificates, such as checking certificate expiry, renewing certificates, ... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Bash(cmd:*), Grep, Glob version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Ssl Certificate Manager\n\nThis skill provides automated assistance for ssl certificate manager tasks.\n\n## Overview\n\nThis skill empowers Claude to seamlessly interact with the ssl-certificate-manager plugin, facilitating efficient management and monitoring of SSL/TLS certificates. It allows for quick checks of certificate expiry dates, automated renewal processes, and comprehensive listings of installed certificates.\n\n## How It Works\n\n1. **Identify Intent**: Claude analyzes the user's request for keywords related to SSL/TLS certificate management.\n2. **Plugin Activation**: The ssl-certificate-manager plugin is automatically activated.\n3. **Command Execution**: Based on the user's request, Claude executes the appropriate command within the plugin (e.g., checking expiry, renewing certificate, listing certificates).\n4. **Result Presentation**: Claude presents the results of the command execution to the user in a clear and concise format.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Check the expiry date of an SSL/TLS certificate.\n- Renew an SSL/TLS certificate.\n- List all installed SSL/TLS certificates.\n- Investigate SSL/TLS certificate issues.\n\n## Examples\n\n### Example 1: Checking Certificate Expiry\n\nUser request: \"Check the expiry date of my SSL certificate for example.com\"\n\nThe skill will:\n1. Activate the ssl-certificate-manager plugin.\n2. Execute the command to check the expiry date for the specified domain.\n3. Display the expiry date to the user.\n\n### Example 2: Renewing a Certificate\n\nUser request: \"Renew my SSL certificate for api.example.org\"\n\nThe skill will:\n1. Activate the ssl-certificate-manager plugin.\n2. Execute the command to renew the SSL certificate for the specified domain.\n3. Confirm the renewal process to the user.\n\n## Best Practices\n\n- **Specificity**: Provide the full domain name when requesting certificate checks or renewals.\n- **Context**: If encountering errors, provide the full error message to Claude for better troubleshooting.\n- **Verification**: After renewal, always verify the new certificate is correctly installed and functioning.\n\n## Integration\n\nThis skill can be used in conjunction with other security-related plugins to provide a comprehensive security overview. For example, it can be integrated with vulnerability scanning tools to identify potential weaknesses related to outdated or misconfigured certificates.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "ssl-certificate-manager",
        "category": "security",
        "path": "plugins/security/ssl-certificate-manager",
        "version": "1.0.0",
        "description": "Manage and monitor SSL/TLS certificates"
      },
      "filePath": "plugins/security/ssl-certificate-manager/skills/managing-ssltls-certificates/SKILL.md"
    },
    {
      "slug": "managing-test-environments",
      "name": "managing-test-environments",
      "description": "Test provision and manage isolated test environments with configuration and data. Use when performing specialized testing. Trigger with phrases like \"manage test environment\", \"provision test env\", or \"setup test infrastructure\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:env-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Test Environment Manager\n\nThis skill provides automated assistance for test environment manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:env-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for test environment manager tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "test-environment-manager",
        "category": "testing",
        "path": "plugins/testing/test-environment-manager",
        "version": "1.0.0",
        "description": "Manage test environments with Docker Compose, Testcontainers, and environment isolation"
      },
      "filePath": "plugins/testing/test-environment-manager/skills/managing-test-environments/SKILL.md"
    },
    {
      "slug": "memory",
      "name": "memory",
      "description": "Execute extract and use project memories from previous sessions for context-aware assistance. Use when recalling past decisions, checking project conventions, or understanding user preferences. Trigger with phrases like \"remember when\", \"like before\", or \"what was our decision about\". allowed-tools: Read, Write version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "yldrmahmet",
      "license": "MIT",
      "content": "# Memory\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Project memory file at `{baseDir}/.memories/project_memory.json`\n- Read permissions for the memory storage location\n- Understanding that memories persist across sessions\n- Knowledge of slash commands for manual memory management\n\n## Instructions\n\n1. Locate memory file using Read tool\n2. Parse JSON structure containing memory entries\n3. Identify relevant memories based on current context\n4. Extract applicable decisions, conventions, or preferences\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Memories applied automatically without announcement\n- Decisions informed by historical context\n- Consistent behavior aligned with past choices\n- Natural incorporation of established patterns\n- List of all stored memories with timestamps\n- Confirmation of newly added memories\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- `/remember [text]` - Add new memory to manual_memories array\n- `/forget [text]` - Remove matching memory from storage\n- `/memories` - Display all currently stored memories\n- Apply memories silently without announcing to user\n- Current explicit requests always override stored memories",
      "parentPlugin": {
        "name": "claude-never-forgets",
        "category": "community",
        "path": "plugins/community/claude-never-forgets",
        "version": "1.0.0",
        "description": "Persistent memory across sessions. Learns preferences, conventions, and corrections automatically."
      },
      "filePath": "plugins/community/claude-never-forgets/skills/memory/SKILL.md"
    },
    {
      "slug": "migrating-apis",
      "name": "migrating-apis",
      "description": "Implement API migrations between versions, platforms, or frameworks with minimal downtime. Use when upgrading APIs between versions. Trigger with phrases like \"migrate the API\", \"upgrade API version\", or \"migrate to new API\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:migrate-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Migrating Apis\n\n## Overview\n\n\nThis skill provides automated assistance for api migration tool tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:migrate-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-migration-tool",
        "category": "api-development",
        "path": "plugins/api-development/api-migration-tool",
        "version": "1.0.0",
        "description": "Migrate APIs between versions with backward compatibility"
      },
      "filePath": "plugins/api-development/api-migration-tool/skills/migrating-apis/SKILL.md"
    },
    {
      "slug": "mocking-apis",
      "name": "mocking-apis",
      "description": "Generate mock API servers for testing and development with realistic response data. Use when creating mock APIs for development and testing. Trigger with phrases like \"create mock API\", \"generate API mock\", or \"setup mock server\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:mock-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Mocking Apis\n\n## Overview\n\n\nThis skill provides automated assistance for api mock server tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:mock-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-mock-server",
        "category": "api-development",
        "path": "plugins/api-development/api-mock-server",
        "version": "1.0.0",
        "description": "Create mock API servers from OpenAPI specs for testing"
      },
      "filePath": "plugins/api-development/api-mock-server/skills/mocking-apis/SKILL.md"
    },
    {
      "slug": "modeling-nosql-data",
      "name": "modeling-nosql-data",
      "description": "Build use when you need to work with NoSQL data modeling. This skill provides NoSQL database design with comprehensive guidance and automation. Trigger with phrases like \"model NoSQL data\", \"design document structure\", or \"optimize NoSQL schema\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Nosql Data Modeler\n\nThis skill provides automated assistance for nosql data modeler tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/nosql-data-modeler/`\n\n**Documentation and Guides**: `{baseDir}/docs/nosql-data-modeler/`\n\n**Example Scripts and Code**: `{baseDir}/examples/nosql-data-modeler/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/nosql-data-modeler-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/nosql-data-modeler-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/nosql-data-modeler-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "nosql-data-modeler",
        "category": "database",
        "path": "plugins/database/nosql-data-modeler",
        "version": "1.0.0",
        "description": "Database plugin for nosql-data-modeler"
      },
      "filePath": "plugins/database/nosql-data-modeler/skills/modeling-nosql-data/SKILL.md"
    },
    {
      "slug": "monitoring-apis",
      "name": "monitoring-apis",
      "description": "Build real-time API monitoring dashboards with metrics, alerts, and health checks. Use when tracking API health and performance metrics. Trigger with phrases like \"monitor the API\", \"add API metrics\", or \"setup API monitoring\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:monitor-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Monitoring Apis\n\n## Overview\n\n\nThis skill provides automated assistance for api monitoring dashboard tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:monitor-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-monitoring-dashboard",
        "category": "api-development",
        "path": "plugins/api-development/api-monitoring-dashboard",
        "version": "1.0.0",
        "description": "Create monitoring dashboards for API health, metrics, and alerts"
      },
      "filePath": "plugins/api-development/api-monitoring-dashboard/skills/monitoring-apis/SKILL.md"
    },
    {
      "slug": "monitoring-cpu-usage",
      "name": "monitoring-cpu-usage",
      "description": "Monitor this skill enables AI assistant to monitor and analyze cpu usage patterns within applications. it helps identify cpu hotspots, analyze algorithmic complexity, and detect blocking operations. use this skill when the user asks to \"monitor cpu usage\", \"opt... Use when setting up monitoring or observability. Trigger with phrases like 'monitor', 'metrics', or 'alerts'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Cpu Usage Monitor\n\nThis skill provides automated assistance for cpu usage monitor tasks.\n\n## Overview\n\nThis skill empowers Claude to analyze code for CPU-intensive operations, offering detailed optimization recommendations to improve processor utilization. By pinpointing areas of high CPU usage, it facilitates targeted improvements for enhanced application performance.\n\n## How It Works\n\n1. **Initiate CPU Monitoring**: Claude activates the `cpu-usage-monitor` plugin.\n2. **Code Analysis**: The plugin analyzes the codebase for computationally expensive operations, synchronous blocking calls, inefficient loops, and regex patterns.\n3. **Optimization Recommendations**: Claude provides a detailed report outlining areas for optimization, including suggestions for algorithmic improvements, asynchronous processing, and regex optimization.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify CPU bottlenecks in your application.\n- Optimize application performance by reducing CPU load.\n- Analyze code for computationally intensive operations.\n\n## Examples\n\n### Example 1: Identifying CPU Hotspots\n\nUser request: \"Monitor CPU usage in my Python script and suggest optimizations.\"\n\nThe skill will:\n1. Analyze the provided Python script for CPU-intensive functions.\n2. Identify potential bottlenecks such as inefficient loops or complex regex patterns.\n3. Provide recommendations for optimizing the code, such as using more efficient algorithms or asynchronous operations.\n\n### Example 2: Analyzing Algorithmic Complexity\n\nUser request: \"Analyze the CPU load of this Java code and identify areas with high algorithmic complexity.\"\n\nThe skill will:\n1. Analyze the provided Java code, focusing on algorithmic complexity (e.g., O(n^2) or worse).\n2. Pinpoint specific methods or sections of code with high complexity.\n3. Suggest alternative algorithms or data structures to improve performance.\n\n## Best Practices\n\n- **Targeted Analysis**: Focus the analysis on specific sections of code known to be CPU-intensive.\n- **Asynchronous Operations**: Consider using asynchronous operations to prevent blocking the main thread.\n- **Regex Optimization**: Carefully review and optimize regular expressions for performance.\n\n## Integration\n\nThis skill can be used in conjunction with other code analysis and refactoring tools to implement the suggested optimizations. It can also be integrated into CI/CD pipelines to automatically monitor CPU usage and identify performance regressions.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "cpu-usage-monitor",
        "category": "performance",
        "path": "plugins/performance/cpu-usage-monitor",
        "version": "1.0.0",
        "description": "Monitor and analyze CPU usage patterns in applications"
      },
      "filePath": "plugins/performance/cpu-usage-monitor/skills/monitoring-cpu-usage/SKILL.md"
    },
    {
      "slug": "monitoring-cross-chain-bridges",
      "name": "monitoring-cross-chain-bridges",
      "description": "Monitor cross-chain bridge security, liquidity, and transaction status across networks. Use when monitoring cross-chain asset transfers. Trigger with phrases like \"monitor bridges\", \"check cross-chain\", or \"track bridge transfers\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:bridge-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Monitoring Cross Chain Bridges\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:bridge-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "cross-chain-bridge-monitor",
        "category": "crypto",
        "path": "plugins/crypto/cross-chain-bridge-monitor",
        "version": "1.0.0",
        "description": "Monitor cross-chain bridge activity, track transfers, analyze security, and detect bridge exploits"
      },
      "filePath": "plugins/crypto/cross-chain-bridge-monitor/skills/monitoring-cross-chain-bridges/SKILL.md"
    },
    {
      "slug": "monitoring-database-health",
      "name": "monitoring-database-health",
      "description": "Monitor use when you need to work with monitoring and observability. This skill provides health monitoring and alerting with comprehensive guidance and automation. Trigger with phrases like \"monitor system health\", \"set up alerts\", or \"track metrics\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Health Monitor\n\nThis skill provides automated assistance for database health monitor tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-health-monitor/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-health-monitor/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-health-monitor/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-health-monitor-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-health-monitor-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-health-monitor-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-health-monitor",
        "category": "database",
        "path": "plugins/database/database-health-monitor",
        "version": "1.0.0",
        "description": "Database plugin for database-health-monitor"
      },
      "filePath": "plugins/database/database-health-monitor/skills/monitoring-database-health/SKILL.md"
    },
    {
      "slug": "monitoring-database-transactions",
      "name": "monitoring-database-transactions",
      "description": "Monitor use when you need to work with monitoring and observability. This skill provides health monitoring and alerting with comprehensive guidance and automation. Trigger with phrases like \"monitor system health\", \"set up alerts\", or \"track metrics\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Transaction Monitor\n\nThis skill provides automated assistance for database transaction monitor tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-transaction-monitor/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-transaction-monitor/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-transaction-monitor/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-transaction-monitor-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-transaction-monitor-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-transaction-monitor-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-transaction-monitor",
        "category": "database",
        "path": "plugins/database/database-transaction-monitor",
        "version": "1.0.0",
        "description": "Database plugin for database-transaction-monitor"
      },
      "filePath": "plugins/database/database-transaction-monitor/skills/monitoring-database-transactions/SKILL.md"
    },
    {
      "slug": "monitoring-error-rates",
      "name": "monitoring-error-rates",
      "description": "Monitor and analyze application error rates to improve reliability. Use when tracking errors in applications including HTTP errors, exceptions, and database issues. Trigger with phrases like \"monitor error rates\", \"track application errors\", or \"analyze error patterns\".",
      "allowedTools": [
        "\"Read",
        "Bash(monitoring:*)",
        "Bash(metrics:*)",
        "Bash(logs:*)",
        "Grep",
        "Glob\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Error Rate Monitor\n\nThis skill provides automated assistance for error rate monitor tasks.\n\n## Overview\n\nThis skill automates the process of setting up comprehensive error monitoring and alerting for various components of an application. It helps identify, track, and analyze different types of errors, enabling proactive identification and resolution of issues before they impact users.\n\n## How It Works\n\n1. **Analyze Error Sources**: Identifies potential error sources within the application architecture, including HTTP endpoints, database queries, external APIs, background jobs, and client-side code.\n2. **Define Monitoring Criteria**: Establishes specific error types and thresholds for each source, such as HTTP status codes (4xx, 5xx), exception types, query timeouts, and API response failures.\n3. **Configure Alerting**: Sets up alerts to trigger when error rates exceed defined thresholds, notifying relevant teams or individuals for investigation and remediation.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Set up error monitoring for a new application.\n- Analyze existing error rates and identify areas for improvement.\n- Configure alerts to be notified of critical errors in real-time.\n- Establish error budgets and track progress towards reliability goals.\n\n## Examples\n\n### Example 1: Setting up Error Monitoring for a Web Application\n\nUser request: \"Monitor errors in my web application, especially 500 errors and database connection issues.\"\n\nThe skill will:\n1. Analyze the web application's architecture to identify potential error sources (e.g., HTTP endpoints, database connections).\n2. Configure monitoring for 500 errors and database connection failures, setting appropriate thresholds and alerts.\n\n### Example 2: Analyzing Error Rates in a Background Job Processor\n\nUser request: \"Analyze error rates for my background job processor. I'm seeing a lot of failed jobs.\"\n\nThe skill will:\n1. Focus on the background job processor and identify the types of errors occurring (e.g., task failures, timeouts, resource exhaustion).\n2. Analyze the frequency and patterns of these errors to identify potential root causes.\n\n## Best Practices\n\n- **Granularity**: Monitor errors at a granular level to identify specific problem areas.\n- **Thresholding**: Set appropriate alert thresholds to avoid alert fatigue and focus on critical issues.\n- **Context**: Include relevant context in error messages and alerts to facilitate troubleshooting.\n\n## Integration\n\nThis skill can be integrated with other monitoring and alerting tools, such as Prometheus, Grafana, and PagerDuty, to provide a comprehensive view of application health and performance. It can also be used in conjunction with incident management tools to streamline incident response workflows.\n\n## Prerequisites\n\n- Access to application logs and metrics\n- Monitoring infrastructure (Prometheus, Grafana, or similar)\n- Read permissions for log files in {baseDir}/logs/\n- Network access to monitoring endpoints\n\n## Instructions\n\n1. Identify error sources by analyzing application architecture\n2. Define error types and monitoring thresholds\n3. Configure alerting rules with appropriate severity levels\n4. Set up dashboards for error rate visualization\n5. Establish notification channels for critical errors\n6. Document error baselines and SLO targets\n\n## Output\n\n- Error rate metrics and trends\n- Alert configurations for critical thresholds\n- Dashboard definitions for error monitoring\n- Reports on error patterns and root causes\n- Recommendations for error reduction strategies\n\n## Error Handling\n\nIf monitoring setup fails:\n- Verify log file permissions and paths\n- Check monitoring service connectivity\n- Validate metric export configurations\n- Review alert rule syntax\n- Ensure notification channels are configured\n\n## Resources\n\n- Monitoring platform documentation (Prometheus, Grafana)\n- Application log format specifications\n- Error taxonomy and classification guides\n- SLO/SLI definition best practices",
      "parentPlugin": {
        "name": "error-rate-monitor",
        "category": "performance",
        "path": "plugins/performance/error-rate-monitor",
        "version": "1.0.0",
        "description": "Monitor and analyze application error rates"
      },
      "filePath": "plugins/performance/error-rate-monitor/skills/monitoring-error-rates/SKILL.md"
    },
    {
      "slug": "monitoring-whale-activity",
      "name": "monitoring-whale-activity",
      "description": "Track large crypto transactions and whale wallet movements across blockchains. Use when tracking large holder movements. Trigger with phrases like \"track whales\", \"monitor large transfers\", or \"check whale activity\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:whale-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Monitoring Whale Activity\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:whale-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "whale-alert-monitor",
        "category": "crypto",
        "path": "plugins/crypto/whale-alert-monitor",
        "version": "1.0.0",
        "description": "Monitor large crypto transactions and whale wallet movements in real-time"
      },
      "filePath": "plugins/crypto/whale-alert-monitor/skills/monitoring-whale-activity/SKILL.md"
    },
    {
      "slug": "ollama-setup",
      "name": "ollama-setup",
      "description": "Configure auto-configure Ollama when user needs local LLM deployment, free AI alternatives, or wants to eliminate hosted API costs. Trigger phrases: \"install ollama\", \"local AI\", \"free LLM\", \"self-hosted AI\", \"replace OpenAI\", \"no API costs\". Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Ollama Setup\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "ollama-local-ai",
        "category": "ai-ml",
        "path": "plugins/ai-ml/ollama-local-ai",
        "version": "1.0.0",
        "description": "Run AI models locally with Ollama - free alternative to OpenAI, Anthropic, and other paid LLM APIs. Zero-cost, privacy-first AI infrastructure."
      },
      "filePath": "plugins/ai-ml/ollama-local-ai/skills/ollama-setup/SKILL.md"
    },
    {
      "slug": "optimizing-cache-performance",
      "name": "optimizing-cache-performance",
      "description": "Execute this skill enables AI assistant to analyze and improve application caching strategies. it optimizes cache hit rates, ttl configurations, cache key design, and invalidation strategies. use this skill when the user requests to \"optimize cache performance\"... Use when optimizing performance. Trigger with phrases like 'optimize', 'performance', or 'speed up'. allowed-tools: Read, Write, Bash(cmd:*), Grep version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Cache Performance Optimizer\n\nThis skill provides automated assistance for cache performance optimizer tasks.\n\n## Overview\n\nThis skill empowers Claude to diagnose and resolve caching-related performance issues. It guides users through a comprehensive optimization process, ensuring efficient use of caching resources.\n\n## How It Works\n\n1. **Identify Caching Implementation**: Locates the caching implementation within the project (e.g., Redis, Memcached, in-memory caches).\n2. **Analyze Cache Configuration**: Examines the existing cache configuration, including TTL values, eviction policies, and key structures.\n3. **Recommend Optimizations**: Suggests improvements to cache hit rates, TTLs, key design, invalidation strategies, and memory usage.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Improve application performance by optimizing caching mechanisms.\n- Identify and resolve caching-related bottlenecks.\n- Review and improve cache key design for better hit rates.\n\n## Examples\n\n### Example 1: Optimizing Redis Cache\n\nUser request: \"Optimize Redis cache performance.\"\n\nThe skill will:\n1. Analyze the Redis configuration, including TTLs and memory usage.\n2. Recommend optimal TTL values based on data access patterns.\n\n### Example 2: Improving Cache Hit Rate\n\nUser request: \"Improve cache hit rate in my application.\"\n\nThe skill will:\n1. Analyze cache key design and identify potential areas for improvement.\n2. Suggest more effective cache key structures to increase hit rates.\n\n## Best Practices\n\n- **TTL Management**: Set appropriate TTL values to balance data freshness and cache hit rates.\n- **Key Design**: Use consistent and well-structured cache keys for efficient retrieval.\n- **Invalidation Strategies**: Implement proper cache invalidation strategies to avoid serving stale data.\n\n## Integration\n\nThis skill can integrate with code analysis tools to automatically identify caching implementations and configuration. It can also work with monitoring tools to track cache hit rates and performance metrics.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "cache-performance-optimizer",
        "category": "performance",
        "path": "plugins/performance/cache-performance-optimizer",
        "version": "1.0.0",
        "description": "Optimize caching strategies for improved performance"
      },
      "filePath": "plugins/performance/cache-performance-optimizer/skills/optimizing-cache-performance/SKILL.md"
    },
    {
      "slug": "optimizing-cloud-costs",
      "name": "optimizing-cloud-costs",
      "description": "Execute use when you need to work with cloud cost optimization. This skill provides cost analysis and optimization with comprehensive guidance and automation. Trigger with phrases like \"optimize costs\", \"analyze spending\", or \"reduce costs\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(aws:*), Bash(gcloud:*), Bash(az:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Cloud Cost Optimizer\n\nThis skill provides automated assistance for cloud cost optimizer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/cloud-cost-optimizer/`\n\n**Documentation and Guides**: `{baseDir}/docs/cloud-cost-optimizer/`\n\n**Example Scripts and Code**: `{baseDir}/examples/cloud-cost-optimizer/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/cloud-cost-optimizer-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/cloud-cost-optimizer-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/cloud-cost-optimizer-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "cloud-cost-optimizer",
        "category": "devops",
        "path": "plugins/devops/cloud-cost-optimizer",
        "version": "1.0.0",
        "description": "Optimize cloud costs and generate cost reports"
      },
      "filePath": "plugins/devops/cloud-cost-optimizer/skills/optimizing-cloud-costs/SKILL.md"
    },
    {
      "slug": "optimizing-database-connection-pooling",
      "name": "optimizing-database-connection-pooling",
      "description": "Process use when you need to work with connection management. This skill provides connection pooling and management with comprehensive guidance and automation. Trigger with phrases like \"manage connections\", \"configure pooling\", or \"optimize connection usage\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Connection Pooler\n\nThis skill provides automated assistance for database connection pooler tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-connection-pooler/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-connection-pooler/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-connection-pooler/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-connection-pooler-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-connection-pooler-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-connection-pooler-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-connection-pooler",
        "category": "database",
        "path": "plugins/database/database-connection-pooler",
        "version": "1.0.0",
        "description": "Implement and optimize database connection pooling for improved performance and resource management"
      },
      "filePath": "plugins/database/database-connection-pooler/skills/optimizing-database-connection-pooling/SKILL.md"
    },
    {
      "slug": "optimizing-deep-learning-models",
      "name": "optimizing-deep-learning-models",
      "description": "Optimize deep learning models using Adam, SGD, and learning rate scheduling to improve accuracy and reduce training time. Use when asked to \"optimize deep learning model\" or \"improve model performance\". Trigger with phrases like 'optimize', 'performance', or 'speed up'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Deep Learning Optimizer\n\nThis skill provides automated assistance for deep learning optimizer tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for deep learning optimizer tasks.\nThis skill empowers Claude to automatically optimize deep learning models, enhancing their performance and efficiency. It intelligently applies various optimization techniques based on the model's characteristics and the user's objectives.\n\n## How It Works\n\n1. **Analyze Model**: Examines the deep learning model's architecture, training data, and performance metrics.\n2. **Identify Optimizations**: Determines the most effective optimization strategies based on the analysis, such as adjusting the learning rate, applying regularization techniques, or modifying the optimizer.\n3. **Apply Optimizations**: Generates optimized code that implements the chosen strategies.\n4. **Evaluate Performance**: Assesses the impact of the optimizations on model performance, providing metrics like accuracy, training time, and resource consumption.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Optimize the performance of a deep learning model.\n- Reduce the training time of a deep learning model.\n- Improve the accuracy of a deep learning model.\n- Optimize the learning rate for a deep learning model.\n- Reduce resource consumption during deep learning model training.\n\n## Examples\n\n### Example 1: Improving Model Accuracy\n\nUser request: \"Optimize this deep learning model for improved image classification accuracy.\"\n\nThe skill will:\n1. Analyze the model and identify potential areas for improvement, such as adjusting the learning rate or adding regularization.\n2. Apply the selected optimization techniques and generate optimized code.\n3. Evaluate the model's performance and report the improved accuracy.\n\n### Example 2: Reducing Training Time\n\nUser request: \"Reduce the training time of this deep learning model.\"\n\nThe skill will:\n1. Analyze the model and identify bottlenecks in the training process.\n2. Apply techniques like batch size adjustment or optimizer selection to reduce training time.\n3. Evaluate the model's performance and report the reduced training time.\n\n## Best Practices\n\n- **Optimizer Selection**: Experiment with different optimizers (e.g., Adam, SGD) to find the best fit for the model and dataset.\n- **Learning Rate Scheduling**: Implement learning rate scheduling to dynamically adjust the learning rate during training.\n- **Regularization**: Apply regularization techniques (e.g., L1, L2 regularization) to prevent overfitting.\n\n## Integration\n\nThis skill can be integrated with other plugins that provide model building and data preprocessing capabilities. It can also be used in conjunction with monitoring tools to track the performance of optimized models.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "deep-learning-optimizer",
        "category": "ai-ml",
        "path": "plugins/ai-ml/deep-learning-optimizer",
        "version": "1.0.0",
        "description": "Deep learning optimization techniques"
      },
      "filePath": "plugins/ai-ml/deep-learning-optimizer/skills/optimizing-deep-learning-models/SKILL.md"
    },
    {
      "slug": "optimizing-defi-yields",
      "name": "optimizing-defi-yields",
      "description": "Execute find and compare DeFi yield opportunities across protocols with APY calculations. Use when finding optimal DeFi yield opportunities. Trigger with phrases like \"find yield\", \"optimize returns\", or \"compare APY\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:yield-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Optimizing Defi Yields\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:yield-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "defi-yield-optimizer",
        "category": "crypto",
        "path": "plugins/crypto/defi-yield-optimizer",
        "version": "1.0.0",
        "description": "Optimize DeFi yield farming strategies across protocols with APY tracking and risk assessment"
      },
      "filePath": "plugins/crypto/defi-yield-optimizer/skills/optimizing-defi-yields/SKILL.md"
    },
    {
      "slug": "optimizing-gas-fees",
      "name": "optimizing-gas-fees",
      "description": "Execute predict optimal gas prices and transaction timing to minimize blockchain transaction costs. Use when optimizing blockchain transaction costs. Trigger with phrases like \"optimize gas\", \"check gas prices\", or \"minimize fees\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:gas-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Optimizing Gas Fees\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:gas-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "gas-fee-optimizer",
        "category": "crypto",
        "path": "plugins/crypto/gas-fee-optimizer",
        "version": "1.0.0",
        "description": "Optimize transaction gas fees with timing and routing recommendations"
      },
      "filePath": "plugins/crypto/gas-fee-optimizer/skills/optimizing-gas-fees/SKILL.md"
    },
    {
      "slug": "optimizing-prompts",
      "name": "optimizing-prompts",
      "description": "Execute this skill optimizes prompts for large language models (llms) to reduce token usage, lower costs, and improve performance. it analyzes the prompt, identifies areas for simplification and redundancy removal, and rewrites the prompt to be more conci... Use when optimizing performance. Trigger with phrases like 'optimize', 'performance', or 'speed up'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Ai Ml Engineering Pack\n\nThis skill provides automated assistance for ai ml engineering pack tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for ai ml engineering pack tasks.\nThis skill empowers Claude to refine prompts for optimal LLM performance. It streamlines prompts to minimize token count, thereby reducing costs and enhancing response speed, all while maintaining or improving output quality.\n\n## How It Works\n\n1. **Analyzing Prompt**: The skill analyzes the input prompt to identify areas of redundancy, verbosity, and potential for simplification.\n2. **Rewriting Prompt**: It rewrites the prompt using techniques like concise language, targeted instructions, and efficient phrasing.\n3. **Suggesting Alternatives**: The skill provides the optimized prompt along with an explanation of the changes made and their expected impact.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Reduce the cost of using an LLM.\n- Improve the speed of LLM responses.\n- Enhance the quality or clarity of LLM outputs by refining the prompt.\n\n## Examples\n\n### Example 1: Reducing LLM Costs\n\nUser request: \"Optimize this prompt for cost and quality: 'I would like you to create a detailed product description for a new ergonomic office chair, highlighting its features, benefits, and target audience, and also include information about its warranty and return policy.'\"\n\nThe skill will:\n1. Analyze the prompt for redundancies and areas for simplification.\n2. Rewrite the prompt to be more concise: \"Create a product description for an ergonomic office chair. Include features, benefits, target audience, warranty, and return policy.\"\n3. Provide the optimized prompt and explain the token reduction achieved.\n\n### Example 2: Improving Prompt Performance\n\nUser request: \"Optimize this prompt for better summarization: 'Please read the following document and provide a comprehensive summary of all the key points, main arguments, supporting evidence, and overall conclusion, ensuring that the summary is accurate, concise, and easy to understand.'\"\n\nThe skill will:\n1. Identify areas for improvement in the prompt's clarity and focus.\n2. Rewrite the prompt to be more direct: \"Summarize this document, including key points, arguments, evidence, and the conclusion.\"\n3. Present the optimized prompt and explain how it enhances summarization performance.\n\n## Best Practices\n\n- **Clarity**: Ensure the original prompt is clear and well-defined before optimization.\n- **Context**: Provide sufficient context to the skill so it can understand the prompt's purpose.\n- **Iteration**: Iterate on the optimized prompt based on the LLM's output to fine-tune performance.\n\n## Integration\n\nThis skill integrates with the `prompt-architect` agent to leverage advanced prompt engineering techniques. It can also be used in conjunction with the `llm-integration-expert` to optimize prompts for specific LLM APIs.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "ai-ml-engineering-pack",
        "category": "packages",
        "path": "plugins/packages/ai-ml-engineering-pack",
        "version": "1.0.0",
        "description": "Professional AI/ML Engineering toolkit: Prompt engineering, LLM integration, RAG systems, AI safety with 12 expert plugins"
      },
      "filePath": "plugins/packages/ai-ml-engineering-pack/skills/optimizing-prompts/SKILL.md"
    },
    {
      "slug": "optimizing-sql-queries",
      "name": "optimizing-sql-queries",
      "description": "Execute use when you need to work with query optimization. This skill provides query performance analysis with comprehensive guidance and automation. Trigger with phrases like \"optimize queries\", \"analyze performance\", or \"improve query speed\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Sql Query Optimizer\n\nThis skill provides automated assistance for sql query optimizer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/sql-query-optimizer/`\n\n**Documentation and Guides**: `{baseDir}/docs/sql-query-optimizer/`\n\n**Example Scripts and Code**: `{baseDir}/examples/sql-query-optimizer/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/sql-query-optimizer-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/sql-query-optimizer-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/sql-query-optimizer-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "sql-query-optimizer",
        "category": "database",
        "path": "plugins/database/sql-query-optimizer",
        "version": "1.0.0",
        "description": "Analyze and optimize SQL queries for better performance, suggesting indexes, query rewrites, and execution plan improvements"
      },
      "filePath": "plugins/database/sql-query-optimizer/skills/optimizing-sql-queries/SKILL.md"
    },
    {
      "slug": "optimizing-staking-rewards",
      "name": "optimizing-staking-rewards",
      "description": "Execute compare staking rewards across validators and networks with ROI calculations. Use when optimizing proof-of-stake rewards. Trigger with phrases like \"optimize staking\", \"compare validators\", or \"calculate rewards\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:staking-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Optimizing Staking Rewards\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:staking-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "staking-rewards-optimizer",
        "category": "crypto",
        "path": "plugins/crypto/staking-rewards-optimizer",
        "version": "1.0.0",
        "description": "Optimize staking rewards across multiple protocols and chains"
      },
      "filePath": "plugins/crypto/staking-rewards-optimizer/skills/optimizing-staking-rewards/SKILL.md"
    },
    {
      "slug": "orchestrating-deployment-pipelines",
      "name": "orchestrating-deployment-pipelines",
      "description": "Deploy use when you need to work with deployment and CI/CD. This skill provides deployment automation and orchestration with comprehensive guidance and automation. Trigger with phrases like \"deploy application\", \"create pipeline\", or \"automate deployment\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(git:*), Bash(docker:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Deployment Pipeline Orchestrator\n\nThis skill provides automated assistance for deployment pipeline orchestrator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/deployment-pipeline-orchestrator/`\n\n**Documentation and Guides**: `{baseDir}/docs/deployment-pipeline-orchestrator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/deployment-pipeline-orchestrator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/deployment-pipeline-orchestrator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/deployment-pipeline-orchestrator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/deployment-pipeline-orchestrator-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "deployment-pipeline-orchestrator",
        "category": "devops",
        "path": "plugins/devops/deployment-pipeline-orchestrator",
        "version": "1.0.0",
        "description": "Orchestrate complex multi-stage deployment pipelines"
      },
      "filePath": "plugins/devops/deployment-pipeline-orchestrator/skills/orchestrating-deployment-pipelines/SKILL.md"
    },
    {
      "slug": "orchestrating-multi-agent-systems",
      "name": "orchestrating-multi-agent-systems",
      "description": "Execute orchestrate multi-agent systems with handoffs, routing, and workflows across AI providers. Use when building complex AI systems requiring agent collaboration, task delegation, or workflow coordination. Trigger with phrases like \"create multi-agent system\", \"orchestrate agents\", or \"coordinate agent workflows\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(npm:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Orchestrating Multi Agent Systems\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Node.js 18+ installed for TypeScript agent development\n- AI SDK v5 package installed (`npm install ai`)\n- API keys for AI providers (OpenAI, Anthropic, Google, etc.)\n- Understanding of agent-based architecture patterns\n- TypeScript knowledge for agent implementation\n- Project directory structure for multi-agent systems\n\n## Instructions\n\n1. Create project directory with necessary subdirectories\n2. Initialize npm project with TypeScript configuration\n3. Install AI SDK v5 and provider-specific packages\n4. Set up configuration files for agent orchestration\n1. Write agent initialization code with AI SDK\n2. Configure system prompts for agent behavior\n3. Define tool functions for agent capabilities\n4. Implement handoff rules for inter-agent delegation\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- TypeScript files with AI SDK v5 integration\n- System prompts tailored to each agent role\n- Tool definitions and implementations\n- Handoff rules and coordination logic\n- Workflow definitions for task sequences\n- Routing rules for intelligent task distribution\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- AI SDK v5 official documentation for agent creation\n- Provider-specific integration guides (OpenAI, Anthropic, Google)\n- Tool definition and implementation examples\n- Handoff and routing pattern references\n- Coordinator-worker pattern for task distribution",
      "parentPlugin": {
        "name": "ai-sdk-agents",
        "category": "ai-ml",
        "path": "plugins/ai-ml/ai-sdk-agents",
        "version": "1.0.0",
        "description": "Multi-agent orchestration with AI SDK v5 - handoffs, routing, and coordination for any AI provider (OpenAI, Anthropic, Google)"
      },
      "filePath": "plugins/ai-ml/ai-sdk-agents/skills/orchestrating-multi-agent-systems/SKILL.md"
    },
    {
      "slug": "orchestrating-test-execution",
      "name": "orchestrating-test-execution",
      "description": "Test coordinate parallel test execution across multiple environments and frameworks. Use when performing specialized testing. Trigger with phrases like \"orchestrate tests\", \"run parallel tests\", or \"coordinate test execution\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:orchestrate-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Test Orchestrator\n\nThis skill provides automated assistance for test orchestrator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:orchestrate-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for test orchestrator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "test-orchestrator",
        "category": "testing",
        "path": "plugins/testing/test-orchestrator",
        "version": "1.0.0",
        "description": "Orchestrate complex test workflows with dependencies, parallel execution, and smart test selection"
      },
      "filePath": "plugins/testing/test-orchestrator/skills/orchestrating-test-execution/SKILL.md"
    },
    {
      "slug": "overnight-development",
      "name": "overnight-development",
      "description": "Automates software development overnight using git hooks to enforce test-driven Use when appropriate context detected. Trigger with relevant phrases based on skill purpose.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(general:*)",
        "Bash(util:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Overnight Development\n\n## Overview\n\nThis skill automates software development overnight by leveraging Git hooks to enforce test-driven development (TDD). It ensures that all code changes are fully tested and meet specified quality standards before being committed. This approach allows Claude to work autonomously, building new features, refactoring existing code, or fixing bugs while adhering to a rigorous TDD process.\n\n## Prerequisites\n\n- Access to project files in {baseDir}/\n- Required tools and dependencies installed\n- Understanding of skill functionality\n- Permissions for file operations\n\n## Instructions\n\n1. Identify skill activation trigger and context\n2. Gather required inputs and parameters\n3. Execute skill workflow systematically\n4. Validate outputs meet requirements\n5. Handle errors and edge cases appropriately\n6. Provide clear results and next steps\n\n## Output\n\n- Primary deliverables based on skill purpose\n- Status indicators and success metrics\n- Generated files or configurations\n- Reports and summaries as applicable\n- Recommendations for follow-up actions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Official documentation for related tools\n- Best practices guides\n- Example use cases and templates\n- Community forums and support channels",
      "parentPlugin": {
        "name": "overnight-dev",
        "category": "productivity",
        "path": "plugins/productivity/overnight-dev",
        "version": "1.0.0",
        "description": "Run Claude autonomously for 6-8 hours overnight using Git hooks that enforce TDD - wake up to fully tested features"
      },
      "filePath": "plugins/productivity/overnight-dev/skills/overnight-development/SKILL.md"
    },
    {
      "slug": "performing-penetration-testing",
      "name": "performing-penetration-testing",
      "description": "Perform security penetration testing to identify vulnerabilities. Use when conducting security assessments. Trigger with 'run pentest', 'security testing', or 'find vulnerabilities'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Penetration Tester\n\nThis skill provides automated assistance for penetration tester tasks.\n\n## Overview\n\nThis skill automates the process of penetration testing for web applications, identifying vulnerabilities and suggesting exploitation techniques. It leverages the penetration-tester plugin to assess web application security posture.\n\n## How It Works\n\n1. **Target Identification**: Analyzes the user's request to identify the target web application or API endpoint.\n2. **Vulnerability Scanning**: Executes automated scans to discover potential vulnerabilities, covering OWASP Top 10 risks.\n3. **Reporting**: Generates a detailed penetration test report, including identified vulnerabilities, risk ratings, and remediation recommendations.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Perform a penetration test on a web application.\n- Identify vulnerabilities in a web application or API.\n- Assess the security posture of a web application.\n- Generate a report detailing security flaws and remediation steps.\n\n## Examples\n\n### Example 1: Performing a Full Penetration Test\n\nUser request: \"Run a penetration test on example.com\"\n\nThe skill will:\n1. Initiate a comprehensive penetration test on the specified domain.\n2. Generate a detailed report outlining identified vulnerabilities, including SQL injection, XSS, and CSRF.\n\n### Example 2: Assessing API Security\n\nUser request: \"Perform vulnerability assessment on the /api/users endpoint\"\n\nThe skill will:\n1. Target the specified API endpoint for vulnerability scanning.\n2. Identify potential security flaws in the API, such as authentication bypass or authorization issues, and provide remediation advice.\n\n## Best Practices\n\n- **Authorization**: Always ensure you have explicit authorization before performing penetration testing on any system.\n- **Scope Definition**: Clearly define the scope of the penetration test to avoid unintended consequences.\n- **Safe Exploitation**: Use exploitation techniques carefully to demonstrate vulnerabilities without causing damage.\n\n## Integration\n\nThis skill can be integrated with other security tools and plugins to enhance vulnerability management and remediation efforts. For example, findings can be exported to vulnerability tracking systems.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "penetration-tester",
        "category": "security",
        "path": "plugins/security/penetration-tester",
        "version": "1.0.0",
        "description": "Automated penetration testing for web applications with OWASP Top 10 coverage"
      },
      "filePath": "plugins/security/penetration-tester/skills/performing-penetration-testing/SKILL.md"
    },
    {
      "slug": "performing-regression-analysis",
      "name": "performing-regression-analysis",
      "description": "Execute this skill empowers AI assistant to perform regression analysis and modeling using the regression-analysis-tool plugin. it analyzes datasets, generates appropriate regression models (linear, polynomial, etc.), validates the models, and provides performa... Use when analyzing code or data. Trigger with phrases like 'analyze', 'review', or 'examine'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Regression Analysis Tool\n\nThis skill provides automated assistance for regression analysis tool tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for regression analysis tool tasks.\nThis skill enables Claude to analyze data, build regression models, and provide insights into the relationships between variables. It leverages the regression-analysis-tool plugin to automate the process and ensure best practices are followed.\n\n## How It Works\n\n1. **Data Analysis**: Claude analyzes the provided data to understand its structure and identify potential relationships between variables.\n2. **Model Generation**: Based on the data, Claude generates appropriate regression models (e.g., linear, polynomial).\n3. **Model Validation**: Claude validates the generated models to ensure their accuracy and reliability.\n4. **Performance Reporting**: Claude provides performance metrics and insights into the model's effectiveness.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Perform regression analysis on a given dataset.\n- Predict future values based on existing data using regression models.\n- Understand the relationship between independent and dependent variables.\n- Evaluate the performance of a regression model.\n\n## Examples\n\n### Example 1: Predicting House Prices\n\nUser request: \"Can you build a regression model to predict house prices based on square footage and number of bedrooms?\"\n\nThe skill will:\n1. Analyze the provided data on house prices, square footage, and number of bedrooms.\n2. Generate a regression model (likely multiple to compare) to predict house prices.\n3. Provide performance metrics such as R-squared and RMSE.\n\n### Example 2: Analyzing Sales Trends\n\nUser request: \"I need to analyze the sales data for the past year and identify any trends using regression analysis.\"\n\nThe skill will:\n1. Analyze the provided sales data.\n2. Generate a regression model to identify trends and patterns in the sales data.\n3. Visualize the trend and report the equation and R-squared value.\n\n## Best Practices\n\n- **Data Preparation**: Ensure the data is clean and preprocessed before performing regression analysis.\n- **Model Selection**: Choose the appropriate regression model based on the data and the problem.\n- **Validation**: Always validate the model to ensure its accuracy and reliability.\n\n## Integration\n\nThis skill works independently using the regression-analysis-tool plugin. It can be used in conjunction with other data analysis and visualization tools to provide a comprehensive understanding of the data.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "regression-analysis-tool",
        "category": "ai-ml",
        "path": "plugins/ai-ml/regression-analysis-tool",
        "version": "1.0.0",
        "description": "Regression analysis and modeling"
      },
      "filePath": "plugins/ai-ml/regression-analysis-tool/skills/performing-regression-analysis/SKILL.md"
    },
    {
      "slug": "performing-security-audits",
      "name": "performing-security-audits",
      "description": "Analyze code, infrastructure, and configurations by conducting comprehensive security audits. It leverages tools within the security-pro-pack plugin, including vulnerability scanning, compliance checking, and cryptography review. Use when assessing security or running audits. Trigger with phrases like 'security scan', 'audit', or 'vulnerability'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Security Pro Pack\n\nThis skill provides automated assistance for security pro pack tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for security pro pack tasks.\nThis skill empowers Claude to perform in-depth security audits across various domains, from code vulnerability scanning to compliance verification and infrastructure security assessment. It utilizes the specialized tools within the security-pro-pack to provide a comprehensive security posture analysis.\n\n## How It Works\n\n1. **Analysis Selection**: Claude determines the appropriate security-pro-pack tool (e.g., `Security Auditor Expert`, `Compliance Checker`, `Crypto Audit`) based on the user's request and the context of the code or system being analyzed.\n2. **Execution**: Claude executes the selected tool, providing it with the relevant code, configuration files, or API endpoints.\n3. **Reporting**: Claude aggregates and presents the findings in a clear, actionable report, highlighting vulnerabilities, compliance issues, and potential security risks, along with suggested remediation steps.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Assess the security of code for vulnerabilities like those in the OWASP Top 10.\n- Evaluate compliance with standards such as HIPAA, PCI DSS, GDPR, or SOC 2.\n- Review cryptographic implementations for weaknesses.\n- Perform container security scans or API security audits.\n\n## Examples\n\n### Example 1: Vulnerability Assessment\n\nUser request: \"Please perform a security audit on this authentication code to find any potential vulnerabilities.\"\n\nThe skill will:\n1. Invoke the `Security Auditor Expert` agent.\n2. Analyze the provided authentication code for common vulnerabilities.\n3. Generate a report detailing any identified vulnerabilities, their severity, and recommended fixes.\n\n### Example 2: Compliance Check\n\nUser request: \"Check this application against GDPR compliance requirements.\"\n\nThe skill will:\n1. Invoke the `Compliance Checker` agent.\n2. Evaluate the application's architecture and code against GDPR guidelines.\n3. Generate a report highlighting any non-compliant areas and suggesting necessary changes.\n\n## Best Practices\n\n- **Specificity**: Provide clear and specific instructions about the scope of the audit (e.g., \"audit this specific function\" instead of \"audit the whole codebase\").\n- **Context**: Include relevant context about the application, infrastructure, or data being audited to enable more accurate and relevant results.\n- **Iteration**: Use the skill iteratively, addressing the most critical findings first and then progressively improving the overall security posture.\n\n## Integration\n\nThis skill seamlessly integrates with all other components of the security-pro-pack plugin. It also works well with Claude's existing code analysis capabilities, allowing for a holistic and integrated security review process.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "security-pro-pack",
        "category": "packages",
        "path": "plugins/packages/security-pro-pack",
        "version": "1.0.0",
        "description": "Professional security tools for Claude Code: vulnerability scanning, compliance, cryptography audit, container & API security"
      },
      "filePath": "plugins/packages/security-pro-pack/skills/performing-security-audits/SKILL.md"
    },
    {
      "slug": "performing-security-code-review",
      "name": "performing-security-code-review",
      "description": "Execute this skill enables AI assistant to conduct a security-focused code review using the security-agent plugin. it analyzes code for potential vulnerabilities like sql injection, xss, authentication flaws, and insecure dependencies. AI assistant uses this skill wh... Use when assessing security or running audits. Trigger with phrases like 'security scan', 'audit', or 'vulnerability'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore",
      "license": "MIT",
      "content": "# Security Agent\n\nThis skill provides automated assistance for security agent tasks.\n\n## Overview\n\nThis skill empowers Claude to act as a security expert, identifying and explaining potential vulnerabilities within code. It leverages the security-agent plugin to provide detailed security analysis, helping developers improve the security posture of their applications.\n\n## How It Works\n\n1. **Receiving Request**: Claude identifies a user's request for a security review or audit of code.\n2. **Activating Security Agent**: Claude invokes the security-agent plugin to analyze the provided code.\n3. **Generating Security Report**: The security-agent produces a structured report detailing identified vulnerabilities, their severity, affected code locations, and recommended remediation steps.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Review code for security vulnerabilities.\n- Perform a security audit of a codebase.\n- Identify potential security risks in a software application.\n\n## Examples\n\n### Example 1: Identifying SQL Injection Vulnerability\n\nUser request: \"Please review this database query code for SQL injection vulnerabilities.\"\n\nThe skill will:\n1. Activate the security-agent plugin to analyze the database query code.\n2. Generate a report identifying potential SQL injection vulnerabilities, including the vulnerable code snippet, its severity, and suggested remediation, such as using parameterized queries.\n\n### Example 2: Checking for Insecure Dependencies\n\nUser request: \"Can you check this project's dependencies for known security vulnerabilities?\"\n\nThe skill will:\n1. Utilize the security-agent plugin to scan the project's dependencies against known vulnerability databases.\n2. Produce a report listing any vulnerable dependencies, their Common Vulnerabilities and Exposures (CVE) identifiers, and recommendations for updating to secure versions.\n\n## Best Practices\n\n- **Specificity**: Provide the exact code or project you want reviewed.\n- **Context**: Clearly state the security concerns you have regarding the code.\n- **Iteration**: Use the findings to address vulnerabilities and request further reviews.\n\n## Integration\n\nThis skill integrates with Claude's code understanding capabilities and leverages the security-agent plugin to provide specialized security analysis. It can be used in conjunction with other code analysis tools to provide a comprehensive assessment of code quality and security.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "security-agent",
        "category": "examples",
        "path": "plugins/examples/security-agent",
        "version": "1.0.0",
        "description": "Specialized security review subagent"
      },
      "filePath": "plugins/examples/security-agent/skills/performing-security-code-review/SKILL.md"
    },
    {
      "slug": "performing-security-testing",
      "name": "performing-security-testing",
      "description": "Test automate security vulnerability testing covering OWASP Top 10, SQL injection, XSS, CSRF, and authentication issues. Use when performing security assessments, penetration tests, or vulnerability scans. Trigger with phrases like \"scan for vulnerabilities\", \"test security\", or \"run penetration test\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:security-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Security Test Scanner\n\nThis skill provides automated assistance for security test scanner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Target application or API endpoint URLs accessible for testing\n- Authentication credentials if testing protected resources\n- Appropriate authorization to perform security testing on the target system\n- Test environment configured (avoid production without explicit approval)\n- Security testing tools installed (OWASP ZAP, sqlmap, or equivalent)\n\n## Instructions\n\n### Step 1: Define Test Scope\nIdentify the security testing parameters:\n- Target URLs and endpoints to scan\n- Authentication requirements and test credentials\n- Specific vulnerability types to focus on (OWASP Top 10, injection, XSS, etc.)\n- Testing depth level (passive scan vs. active exploitation)\n\n### Step 2: Execute Security Scan\nRun automated vulnerability detection:\n1. Use Read tool to analyze application structure and identify entry points\n2. Execute security testing tools via Bash(test:security-*) with proper scope\n3. Monitor scan progress and capture all findings\n4. Document identified vulnerabilities with severity ratings\n\n### Step 3: Analyze Vulnerabilities\nProcess scan results to identify:\n- SQL injection vulnerabilities in database queries\n- Cross-Site Scripting (XSS) in user input fields\n- Cross-Site Request Forgery (CSRF) token weaknesses\n- Authentication and authorization bypass opportunities\n- Security misconfigurations and exposed sensitive data\n\n### Step 4: Generate Security Report\nCreate comprehensive documentation in {baseDir}/security-reports/:\n- Executive summary with risk overview\n- Detailed vulnerability findings with CVSS scores\n- Proof-of-concept exploit examples where applicable\n- Prioritized remediation recommendations\n- Compliance assessment against security standards\n\n## Output\n\nThe skill generates structured security assessment reports:\n\n### Vulnerability Summary\n- Total vulnerabilities discovered by severity (Critical, High, Medium, Low)\n- OWASP Top 10 category mapping for each finding\n- Attack surface analysis showing exposed endpoints\n\n### Detailed Findings\nEach vulnerability includes:\n- Unique identifier and CVSS score\n- Affected URLs, parameters, and HTTP methods\n- Technical description of the security weakness\n- Proof-of-concept demonstration or reproduction steps\n- Potential impact on confidentiality, integrity, and availability\n\n### Remediation Guidance\n- Specific code fixes or configuration changes required\n- Secure coding best practices to prevent recurrence\n- Priority recommendations based on risk and effort\n- Verification testing procedures after remediation\n\n### Compliance Assessment\n- Alignment with OWASP Application Security Verification Standard (ASVS)\n- PCI DSS requirements if applicable to payment processing\n- General Data Protection Regulation (GDPR) security considerations\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Access Denied**\n- Error: HTTP 403 or authentication failures during scan\n- Solution: Verify credentials are valid and have sufficient permissions; use provided test accounts\n\n**Rate Limiting**\n- Error: Too many requests blocked by WAF or rate limiter\n- Solution: Configure scan throttling to reduce request rate; use authenticated sessions to increase limits\n\n**False Positives**\n- Error: Reported vulnerabilities that cannot be exploited\n- Solution: Manually verify each finding; adjust scanner sensitivity; whitelist known safe patterns\n\n**Tool Installation Missing**\n- Error: Security testing tools not found on system\n- Solution: Install required tools using Bash(test:security-install) with package manager\n\n## Resources\n\n### Security Testing Tools\n- OWASP ZAP for automated vulnerability scanning\n- Burp Suite for manual penetration testing\n- sqlmap for SQL injection detection and exploitation\n- Nikto for web server vulnerability scanning\n\n### Vulnerability Databases\n- Common Vulnerabilities and Exposures (CVE) database\n- National Vulnerability Database (NVD) for CVSS scoring\n- OWASP Top 10 documentation and remediation guides\n\n### Secure Coding Guidelines\n- OWASP Secure Coding Practices checklist\n- CWE (Common Weakness Enumeration) catalog\n- SANS Top 25 Most Dangerous Software Errors\n\n### Best Practices\n- Always test in non-production environments first\n- Obtain written authorization before security testing\n- Document all testing activities for audit trails\n- Validate remediation effectiveness with regression testing\n\n## Overview\n\n\nThis skill provides automated assistance for security test scanner tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "security-test-scanner",
        "category": "testing",
        "path": "plugins/testing/security-test-scanner",
        "version": "1.0.0",
        "description": "Automated security vulnerability testing covering OWASP Top 10, SQL injection, XSS, CSRF, and authentication issues"
      },
      "filePath": "plugins/testing/security-test-scanner/skills/performing-security-testing/SKILL.md"
    },
    {
      "slug": "planning-disaster-recovery",
      "name": "planning-disaster-recovery",
      "description": "Execute use when you need to work with backup and recovery. This skill provides backup automation and disaster recovery with comprehensive guidance and automation. Trigger with phrases like \"create backups\", \"automate backups\", or \"implement disaster recovery\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(tar:*), Bash(rsync:*), Bash(aws:s3:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Disaster Recovery Planner\n\nThis skill provides automated assistance for disaster recovery planner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/disaster-recovery-planner/`\n\n**Documentation and Guides**: `{baseDir}/docs/disaster-recovery-planner/`\n\n**Example Scripts and Code**: `{baseDir}/examples/disaster-recovery-planner/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/disaster-recovery-planner-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/disaster-recovery-planner-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/disaster-recovery-planner-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "disaster-recovery-planner",
        "category": "devops",
        "path": "plugins/devops/disaster-recovery-planner",
        "version": "1.0.0",
        "description": "Plan and implement disaster recovery procedures"
      },
      "filePath": "plugins/devops/disaster-recovery-planner/skills/planning-disaster-recovery/SKILL.md"
    },
    {
      "slug": "plugin-auditor",
      "name": "plugin-auditor",
      "description": "Audit automatically audits AI assistant code plugins for security vulnerabilities, best practices, AI assistant.md compliance, and quality standards when user mentions audit plugin, security review, or best practices check. specific to AI assistant-code-plugins repositor... Use when assessing security or running audits. Trigger with phrases like 'security scan', 'audit', or 'vulnerability'. allowed-tools: Read, Grep, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <plugins@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Plugin Auditor\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "jeremy-plugin-tool",
        "category": "examples",
        "path": "plugins/examples/jeremy-plugin-tool",
        "version": "2.0.0",
        "description": "Ultimate plugin management toolkit with 4 auto-invoked Skills for creating, validating, auditing, and versioning plugins in the claude-code-plugins marketplace"
      },
      "filePath": "plugins/examples/jeremy-plugin-tool/skills/plugin-auditor/SKILL.md"
    },
    {
      "slug": "plugin-creator",
      "name": "plugin-creator",
      "description": "Create automatically creates new AI assistant code plugins with proper structure, validation, and marketplace integration when user mentions creating a plugin, new plugin, or plugin from template. specific to AI assistant-code-plugins repository workflow. Use when generating or creating new content. Trigger with phrases like 'generate', 'create', or 'scaffold'. allowed-tools: Write, Read, Grep, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <plugins@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Plugin Creator\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n[Step-by-step for Claude]\n```\n\n## Output\n\n\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "jeremy-plugin-tool",
        "category": "examples",
        "path": "plugins/examples/jeremy-plugin-tool",
        "version": "2.0.0",
        "description": "Ultimate plugin management toolkit with 4 auto-invoked Skills for creating, validating, auditing, and versioning plugins in the claude-code-plugins marketplace"
      },
      "filePath": "plugins/examples/jeremy-plugin-tool/skills/plugin-creator/SKILL.md"
    },
    {
      "slug": "plugin-validator",
      "name": "plugin-validator",
      "description": "Validate automatically validates AI assistant code plugin structure, schemas, and compliance when user mentions validate plugin, check plugin, or plugin errors. runs comprehensive validation specific to AI assistant-code-plugins repository standards. Use when validating configurations or code. Trigger with phrases like 'validate', 'check', or 'verify'. allowed-tools: Read, Grep, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <plugins@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Plugin Validator\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "jeremy-plugin-tool",
        "category": "examples",
        "path": "plugins/examples/jeremy-plugin-tool",
        "version": "2.0.0",
        "description": "Ultimate plugin management toolkit with 4 auto-invoked Skills for creating, validating, auditing, and versioning plugins in the claude-code-plugins marketplace"
      },
      "filePath": "plugins/examples/jeremy-plugin-tool/skills/plugin-validator/SKILL.md"
    },
    {
      "slug": "preprocessing-data-with-automated-pipelines",
      "name": "preprocessing-data-with-automated-pipelines",
      "description": "Process automate data cleaning, transformation, and validation for ML tasks. Use when requesting \"preprocess data\", \"clean data\", \"ETL pipeline\", or \"data transformation\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Data Preprocessing Pipeline\n\nThis skill provides automated assistance for data preprocessing pipeline tasks.\n\n## Overview\n\nThis skill enables Claude to construct and execute automated data preprocessing pipelines, ensuring data quality and readiness for machine learning. It streamlines the data preparation process by automating common tasks such as data cleaning, transformation, and validation.\n\n## How It Works\n\n1. **Analyze Requirements**: Claude analyzes the user's request to understand the specific data preprocessing needs, including data sources, target format, and desired transformations.\n2. **Generate Pipeline Code**: Based on the requirements, Claude generates Python code for an automated data preprocessing pipeline using relevant libraries and best practices. This includes data validation and error handling.\n3. **Execute Pipeline**: The generated code is executed, performing the data preprocessing steps.\n4. **Provide Metrics and Insights**: Claude provides performance metrics and insights about the pipeline's execution, including data quality reports and potential issues encountered.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Prepare raw data for machine learning models.\n- Automate data cleaning and transformation processes.\n- Implement a robust ETL (Extract, Transform, Load) pipeline.\n\n## Examples\n\n### Example 1: Cleaning Customer Data\n\nUser request: \"Preprocess the customer data from the CSV file to remove duplicates and handle missing values.\"\n\nThe skill will:\n1. Generate a Python script to read the CSV file, remove duplicate entries, and impute missing values using appropriate techniques (e.g., mean imputation).\n2. Execute the script and provide a summary of the changes made, including the number of duplicates removed and the number of missing values imputed.\n\n### Example 2: Transforming Sensor Data\n\nUser request: \"Create an ETL pipeline to transform the sensor data from the database into a format suitable for time series analysis.\"\n\nThe skill will:\n1. Generate a Python script to extract sensor data from the database, transform it into a time series format (e.g., resampling to a fixed frequency), and load it into a suitable storage location.\n2. Execute the script and provide performance metrics, such as the time taken for each step of the pipeline and the size of the transformed data.\n\n## Best Practices\n\n- **Data Validation**: Always include data validation steps to ensure data quality and catch potential errors early in the pipeline.\n- **Error Handling**: Implement robust error handling to gracefully handle unexpected issues during pipeline execution.\n- **Performance Optimization**: Optimize the pipeline for performance by using efficient algorithms and data structures.\n\n## Integration\n\nThis skill can be integrated with other Claude Code skills for data analysis, model training, and deployment. It provides a standardized way to prepare data for these tasks, ensuring consistency and reliability.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "data-preprocessing-pipeline",
        "category": "ai-ml",
        "path": "plugins/ai-ml/data-preprocessing-pipeline",
        "version": "1.0.0",
        "description": "Automated data preprocessing and cleaning pipelines"
      },
      "filePath": "plugins/ai-ml/data-preprocessing-pipeline/skills/preprocessing-data-with-automated-pipelines/SKILL.md"
    },
    {
      "slug": "processing-api-batches",
      "name": "processing-api-batches",
      "description": "Optimize bulk API requests with batching, throttling, and parallel execution. Use when processing bulk API operations efficiently. Trigger with phrases like \"process bulk requests\", \"batch API calls\", or \"handle batch operations\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:batch-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Processing Api Batches\n\n## Overview\n\n\nThis skill provides automated assistance for api batch processor tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:batch-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-batch-processor",
        "category": "api-development",
        "path": "plugins/api-development/api-batch-processor",
        "version": "1.0.0",
        "description": "Implement batch API operations with bulk processing and job queues"
      },
      "filePath": "plugins/api-development/api-batch-processor/skills/processing-api-batches/SKILL.md"
    },
    {
      "slug": "processing-computer-vision-tasks",
      "name": "processing-computer-vision-tasks",
      "description": "Process images using object detection, classification, and segmentation. Use when requesting \"analyze image\", \"object detection\", \"image classification\", or \"computer vision\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Computer Vision Processor\n\nThis skill provides automated assistance for computer vision processor tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for computer vision processor tasks.\nThis skill empowers Claude to leverage the computer-vision-processor plugin to analyze images, detect objects, and extract meaningful information. It automates computer vision workflows, optimizes performance, and provides detailed insights based on image content.\n\n## How It Works\n\n1. **Analyzing the Request**: Claude identifies the need for computer vision processing based on the user's request and trigger terms.\n2. **Generating Code**: Claude generates the appropriate Python code to interact with the computer-vision-processor plugin, specifying the desired analysis type (e.g., object detection, image classification).\n3. **Executing the Task**: The generated code is executed using the `/process-vision` command, which processes the image and returns the results.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Analyze an image for specific objects or features.\n- Classify an image into predefined categories.\n- Segment an image to identify different regions or objects.\n\n## Examples\n\n### Example 1: Object Detection\n\nUser request: \"Analyze this image and identify all the cars and pedestrians.\"\n\nThe skill will:\n1. Generate code to perform object detection on the provided image using the computer-vision-processor plugin.\n2. Return a list of bounding boxes and labels for each detected car and pedestrian.\n\n### Example 2: Image Classification\n\nUser request: \"Classify this image. Is it a cat or a dog?\"\n\nThe skill will:\n1. Generate code to perform image classification on the provided image using the computer-vision-processor plugin.\n2. Return the classification result (e.g., \"cat\" or \"dog\") along with a confidence score.\n\n## Best Practices\n\n- **Data Validation**: Always validate the input image to ensure it's in a supported format and resolution.\n- **Error Handling**: Implement robust error handling to gracefully manage potential issues during image processing.\n- **Performance Optimization**: Choose the appropriate computer vision techniques and parameters to optimize performance for the specific task.\n\n## Integration\n\nThis skill utilizes the `/process-vision` command provided by the computer-vision-processor plugin. It can be integrated with other skills to further process the results of the computer vision analysis, such as generating reports or triggering actions based on detected objects.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "computer-vision-processor",
        "category": "ai-ml",
        "path": "plugins/ai-ml/computer-vision-processor",
        "version": "1.0.0",
        "description": "Computer vision image processing and analysis"
      },
      "filePath": "plugins/ai-ml/computer-vision-processor/skills/processing-computer-vision-tasks/SKILL.md"
    },
    {
      "slug": "profiling-application-performance",
      "name": "profiling-application-performance",
      "description": "Execute this skill enables AI assistant to profile application performance, analyzing cpu usage, memory consumption, and execution time. it is triggered when the user requests performance analysis, bottleneck identification, or optimization recommendations. the... Use when optimizing performance. Trigger with phrases like 'optimize', 'performance', or 'speed up'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Application Profiler\n\nThis skill provides automated assistance for application profiler tasks.\n\n## Overview\n\nThis skill empowers Claude to analyze application performance, pinpoint bottlenecks, and recommend optimizations. By leveraging the application-profiler plugin, it provides insights into CPU usage, memory allocation, and execution time, enabling targeted improvements.\n\n## How It Works\n\n1. **Identify Application Stack**: Determines the application's technology (e.g., Node.js, Python, Java).\n2. **Locate Entry Points**: Identifies main application entry points and critical execution paths.\n3. **Analyze Performance Metrics**: Examines CPU usage, memory allocation, and execution time to detect bottlenecks.\n4. **Generate Profile**: Compiles the analysis into a comprehensive performance profile, highlighting areas for optimization.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Analyze application performance for bottlenecks.\n- Identify CPU-intensive operations and memory leaks.\n- Optimize application execution time.\n\n## Examples\n\n### Example 1: Identifying Memory Leaks\n\nUser request: \"Analyze my Node.js application for memory leaks.\"\n\nThe skill will:\n1. Activate the application-profiler plugin.\n2. Analyze the application's memory allocation patterns.\n3. Generate a profile highlighting potential memory leaks.\n\n### Example 2: Optimizing CPU Usage\n\nUser request: \"Profile my Python script and find the most CPU-intensive functions.\"\n\nThe skill will:\n1. Activate the application-profiler plugin.\n2. Analyze the script's CPU usage.\n3. Generate a profile identifying the functions consuming the most CPU time.\n\n## Best Practices\n\n- **Code Instrumentation**: Ensure the application code is instrumented for accurate profiling.\n- **Realistic Workloads**: Use realistic workloads during profiling to simulate real-world scenarios.\n- **Iterative Optimization**: Apply optimizations iteratively and re-profile to measure improvements.\n\n## Integration\n\nThis skill can be used in conjunction with code editing plugins to implement the recommended optimizations directly within the application's source code. It can also integrate with monitoring tools to track performance improvements over time.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "application-profiler",
        "category": "performance",
        "path": "plugins/performance/application-profiler",
        "version": "1.0.0",
        "description": "Profile application performance with CPU, memory, and execution time analysis"
      },
      "filePath": "plugins/performance/application-profiler/skills/profiling-application-performance/SKILL.md"
    },
    {
      "slug": "providing-performance-optimization-advice",
      "name": "providing-performance-optimization-advice",
      "description": "Provide comprehensive prioritized performance optimization recommendations for frontend, backend, and infrastructure. Use when analyzing bottlenecks or seeking improvement strategies. Trigger with phrases like \"optimize performance\", \"improve speed\", or \"performance recommendations\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(profiling:*)",
        "Bash(analysis:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Performance Optimization Advisor\n\nThis skill provides automated assistance for performance optimization advisor tasks.\n\n## Overview\n\nThis skill empowers Claude to act as a performance optimization advisor, delivering a detailed report of potential improvements across various layers of a software application. It prioritizes recommendations based on impact and effort, allowing for a focused and efficient optimization strategy.\n\n## How It Works\n\n1. **Analyze Project**: Claude uses the plugin to analyze the project's codebase, infrastructure configuration, and architecture.\n2. **Identify Optimization Areas**: The plugin identifies potential optimization areas in the frontend, backend, and infrastructure.\n3. **Prioritize Recommendations**: The plugin prioritizes recommendations based on estimated performance gains and implementation effort.\n4. **Generate Report**: Claude presents a comprehensive report with actionable advice, performance gain estimates, and a phased implementation roadmap.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify performance bottlenecks in a software application.\n- Get recommendations for improving website loading speed.\n- Optimize database query performance.\n- Improve API response times.\n- Reduce infrastructure costs.\n\n## Examples\n\n### Example 1: Optimizing a Slow Website\n\nUser request: \"My website is loading very slowly. Can you help me optimize its performance?\"\n\nThe skill will:\n1. Analyze the website's frontend code, backend APIs, and infrastructure configuration.\n2. Identify issues such as unoptimized images, inefficient database queries, and lack of CDN usage.\n3. Generate a report with prioritized recommendations, including image optimization, database query optimization, and CDN implementation.\n\n### Example 2: Improving API Response Time\n\nUser request: \"The API response time is too slow. What can I do to improve it?\"\n\nThe skill will:\n1. Analyze the API code, database queries, and caching strategies.\n2. Identify issues such as inefficient database queries, lack of caching, and slow processing logic.\n3. Generate a report with prioritized recommendations, including database query optimization, caching implementation, and asynchronous processing.\n\n## Best Practices\n\n- **Specificity**: Provide specific details about the project and its performance issues to get more accurate and relevant recommendations.\n- **Context**: Explain the context of the performance problem, such as the expected user load or the specific use case.\n- **Iteration**: Review the recommendations and provide feedback to refine the optimization strategy.\n\n## Integration\n\nThis skill integrates well with other plugins that provide code analysis, infrastructure management, and deployment automation capabilities. For example, it can be used in conjunction with a code linting plugin to identify code-level performance issues or with an infrastructure-as-code plugin to automate infrastructure optimization tasks.\n\n## Prerequisites\n\n- Access to application codebase in {baseDir}/\n- Infrastructure configuration files\n- Performance profiling tools\n- Current performance metrics and baselines\n\n## Instructions\n\n1. Analyze frontend code for rendering and asset optimization\n2. Review backend code for query and processing efficiency\n3. Examine infrastructure for scaling and resource usage\n4. Identify high-impact optimization opportunities\n5. Prioritize recommendations by effort vs impact\n6. Generate phased implementation roadmap\n\n## Output\n\n- Comprehensive optimization report by layer (frontend/backend/infra)\n- Prioritized recommendations with impact estimates\n- Code examples for suggested improvements\n- Performance gain projections\n- Implementation effort estimates and timeline\n\n## Error Handling\n\nIf optimization analysis fails:\n- Verify codebase access permissions\n- Check profiling tool installation\n- Validate configuration file formats\n- Ensure sufficient analysis resources\n- Review project structure completeness\n\n## Resources\n\n- Web performance optimization guides\n- Database query optimization best practices\n- Infrastructure scaling patterns\n- Caching strategies and CDN usage",
      "parentPlugin": {
        "name": "performance-optimization-advisor",
        "category": "performance",
        "path": "plugins/performance/performance-optimization-advisor",
        "version": "1.0.0",
        "description": "Get comprehensive performance optimization recommendations"
      },
      "filePath": "plugins/performance/performance-optimization-advisor/skills/providing-performance-optimization-advice/SKILL.md"
    },
    {
      "slug": "rate-limiting-apis",
      "name": "rate-limiting-apis",
      "description": "Implement sophisticated rate limiting with sliding windows, token buckets, and quotas. Use when protecting APIs from excessive requests. Trigger with phrases like \"add rate limiting\", \"limit API requests\", or \"implement rate limits\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:ratelimit-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Rate Limiting Apis\n\n## Overview\n\n\nThis skill provides automated assistance for api rate limiter tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:ratelimit-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-rate-limiter",
        "category": "api-development",
        "path": "plugins/api-development/api-rate-limiter",
        "version": "1.0.0",
        "description": "Implement rate limiting with token bucket, sliding window, and Redis"
      },
      "filePath": "plugins/api-development/api-rate-limiter/skills/rate-limiting-apis/SKILL.md"
    },
    {
      "slug": "responding-to-security-incidents",
      "name": "responding-to-security-incidents",
      "description": "Analyze and guide security incident response, investigation, and remediation processes. Use when you need to handle security breaches, classify incidents, develop response playbooks, gather forensic evidence, or coordinate remediation efforts. Trigger with phrases like \"security incident response\", \"ransomware attack response\", \"data breach investigation\", \"incident playbook\", or \"security forensics\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(log-analysis:*), Bash(forensics:*), Bash(network-trace:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Responding To Security Incidents\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Access to system and application logs in {baseDir}/logs/\n- Network traffic captures or SIEM data available\n- Incident response team contact information\n- Backup systems operational and accessible\n- Write permissions for incident documentation in {baseDir}/incidents/\n- Communication channels established for stakeholder updates\n\n## Instructions\n\n1. Triage the incident and scope affected systems/data.\n2. Preserve evidence (logs, snapshots, network captures) before making changes.\n3. Contain the blast radius and eradicate root cause.\n4. Recover safely and document follow-ups (AAR + backlog).\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: Incident response playbook saved to {baseDir}/incidents/incident-YYYYMMDD-HHMM.md\n\n**Playbook Structure**:\n```\n# Security Incident Response - [Incident Type]\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- NIST Computer Security Incident Handling Guide: https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-61r2.pdf\n- SANS Incident Handler's Handbook: https://www.sans.org/white-papers/33901/\n- CISA Incident Response Guide: https://www.cisa.gov/incident-response\n- Memory analysis: Volatility Framework\n- Disk forensics: Autopsy, FTK Imager",
      "parentPlugin": {
        "name": "security-incident-responder",
        "category": "security",
        "path": "plugins/security/security-incident-responder",
        "version": "1.0.0",
        "description": "Assist with security incident response"
      },
      "filePath": "plugins/security/security-incident-responder/skills/responding-to-security-incidents/SKILL.md"
    },
    {
      "slug": "routing-dex-trades",
      "name": "routing-dex-trades",
      "description": "Optimize trade routing across multiple DEXs to find optimal prices and minimize slippage. Use when routing trades for best execution. Trigger with phrases like \"find best price\", \"route trade\", or \"check DEX prices\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:dex-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Routing Dex Trades\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:dex-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "dex-aggregator-router",
        "category": "crypto",
        "path": "plugins/crypto/dex-aggregator-router",
        "version": "1.0.0",
        "description": "Find optimal DEX routes for token swaps across multiple exchanges"
      },
      "filePath": "plugins/crypto/dex-aggregator-router/skills/routing-dex-trades/SKILL.md"
    },
    {
      "slug": "running-chaos-tests",
      "name": "running-chaos-tests",
      "description": "Execute chaos engineering experiments to test system resilience. Use when performing specialized testing. Trigger with phrases like \"run chaos tests\", \"test resilience\", or \"inject failures\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:chaos-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Chaos Engineering Toolkit\n\nThis skill provides automated assistance for chaos engineering toolkit tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:chaos-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for chaos engineering toolkit tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "chaos-engineering-toolkit",
        "category": "testing",
        "path": "plugins/testing/chaos-engineering-toolkit",
        "version": "1.0.0",
        "description": "Chaos testing for resilience with failure injection, latency simulation, and system resilience validation"
      },
      "filePath": "plugins/testing/chaos-engineering-toolkit/skills/running-chaos-tests/SKILL.md"
    },
    {
      "slug": "running-clustering-algorithms",
      "name": "running-clustering-algorithms",
      "description": "Analyze datasets by running clustering algorithms (K-means, DBSCAN, hierarchical) to identify data groups. Use when requesting \"run clustering\", \"cluster analysis\", or \"group data points\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Clustering Algorithm Runner\n\nThis skill provides automated assistance for clustering algorithm runner tasks.\n\n## Overview\n\nThis skill empowers Claude to perform clustering analysis on provided datasets. It allows for automated execution of various clustering algorithms, providing insights into data groupings and structures.\n\n## How It Works\n\n1. **Analyzing the Context**: Claude analyzes the user's request to determine the dataset, desired clustering algorithm (if specified), and any specific requirements.\n2. **Generating Code**: Claude generates Python code using appropriate ML libraries (e.g., scikit-learn) to perform the clustering task, including data loading, preprocessing, algorithm execution, and result visualization.\n3. **Executing Clustering**: The generated code is executed, and the clustering algorithm is applied to the dataset.\n4. **Providing Results**: Claude presents the results, including cluster assignments, performance metrics (e.g., silhouette score, Davies-Bouldin index), and visualizations (e.g., scatter plots with cluster labels).\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify distinct groups within a dataset.\n- Perform a cluster analysis to understand data structure.\n- Run K-means, DBSCAN, or hierarchical clustering on a given dataset.\n\n## Examples\n\n### Example 1: Customer Segmentation\n\nUser request: \"Run clustering on this customer data to identify customer segments. The data is in customer_data.csv.\"\n\nThe skill will:\n1. Load the customer_data.csv dataset.\n2. Perform K-means clustering to identify distinct customer segments based on their attributes.\n3. Provide a visualization of the customer segments and their characteristics.\n\n### Example 2: Anomaly Detection\n\nUser request: \"Perform DBSCAN clustering on this network traffic data to identify anomalies. The data is available at network_traffic.txt.\"\n\nThe skill will:\n1. Load the network_traffic.txt dataset.\n2. Perform DBSCAN clustering to identify outliers representing anomalous network traffic.\n3. Report the identified anomalies and their characteristics.\n\n## Best Practices\n\n- **Data Preprocessing**: Always preprocess the data (e.g., scaling, normalization) before applying clustering algorithms to improve performance and accuracy.\n- **Algorithm Selection**: Choose the appropriate clustering algorithm based on the data characteristics and the desired outcome. K-means is suitable for spherical clusters, while DBSCAN is better for non-spherical clusters and anomaly detection.\n- **Parameter Tuning**: Tune the parameters of the clustering algorithm (e.g., number of clusters in K-means, epsilon and min_samples in DBSCAN) to optimize the results.\n\n## Integration\n\nThis skill can be integrated with data loading skills to retrieve datasets from various sources. It can also be combined with visualization skills to generate insightful visualizations of the clustering results.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "clustering-algorithm-runner",
        "category": "ai-ml",
        "path": "plugins/ai-ml/clustering-algorithm-runner",
        "version": "1.0.0",
        "description": "Run clustering algorithms on datasets"
      },
      "filePath": "plugins/ai-ml/clustering-algorithm-runner/skills/running-clustering-algorithms/SKILL.md"
    },
    {
      "slug": "running-e2e-tests",
      "name": "running-e2e-tests",
      "description": "Execute end-to-end tests covering full user workflows across frontend and backend. Use when performing specialized testing. Trigger with phrases like \"run end-to-end tests\", \"test user flows\", or \"execute E2E suite\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:e2e-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# E2E Test Framework\n\nThis skill provides automated assistance for e2e test framework tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:e2e-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for e2e test framework tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "e2e-test-framework",
        "category": "testing",
        "path": "plugins/testing/e2e-test-framework",
        "version": "1.0.0",
        "description": "End-to-end test automation with Playwright, Cypress, and Selenium for browser-based testing"
      },
      "filePath": "plugins/testing/e2e-test-framework/skills/running-e2e-tests/SKILL.md"
    },
    {
      "slug": "running-integration-tests",
      "name": "running-integration-tests",
      "description": "Execute integration tests validating component interactions and system integration. Use when performing specialized testing. Trigger with phrases like \"run integration tests\", \"test integration\", or \"validate component interactions\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:integration-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Integration Test Runner\n\nThis skill provides automated assistance for integration test runner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:integration-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for integration test runner tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "integration-test-runner",
        "category": "testing",
        "path": "plugins/testing/integration-test-runner",
        "version": "1.0.0",
        "description": "Run and manage integration test suites with environment setup, database seeding, and cleanup"
      },
      "filePath": "plugins/testing/integration-test-runner/skills/running-integration-tests/SKILL.md"
    },
    {
      "slug": "running-load-tests",
      "name": "running-load-tests",
      "description": "Create and execute load tests for performance validation using k6, JMeter, and Artillery. Use when validating application performance under load conditions or identifying bottlenecks. Trigger with phrases like \"run load test\", \"create stress test\", or \"validate performance under load\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(k6:*)",
        "Bash(jmeter:*)",
        "Bash(artillery:*)",
        "Bash(performance:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Load Test Runner\n\nThis skill provides automated assistance for load test runner tasks.\n\n## Overview\n\nThis skill empowers Claude to automate the creation and execution of load tests, ensuring applications can handle expected traffic and identify potential performance bottlenecks. It streamlines the process of defining test scenarios, generating scripts, and executing tests for comprehensive performance validation.\n\n## How It Works\n\n1. **Analyze Application**: Claude analyzes the user's request to understand the application's endpoints and critical paths.\n2. **Identify Test Scenarios**: Claude identifies relevant test scenarios, such as baseline load, stress test, spike test, soak test, or scalability test, based on the user's requirements.\n3. **Generate Load Test Scripts**: Claude generates load test scripts (k6, JMeter, Artillery, etc.) based on the selected scenarios and application details.\n4. **Define Performance Thresholds**: Claude defines performance thresholds and provides execution instructions for the generated scripts.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create load tests for a web application or API.\n- Validate the performance of an application under different load conditions.\n- Identify performance bottlenecks and breaking points.\n\n## Examples\n\n### Example 1: Creating a Stress Test\n\nUser request: \"Create a stress test for the /api/users endpoint to simulate 1000 concurrent users.\"\n\nThe skill will:\n1. Analyze the request and identify the need for a stress test on the /api/users endpoint.\n2. Generate a k6 script that simulates 1000 concurrent users hitting the /api/users endpoint.\n\n### Example 2: Validating Performance After a Code Change\n\nUser request: \"Validate the performance of the application after the recent code changes with a baseline load test.\"\n\nThe skill will:\n1. Identify the need for a baseline load test to validate performance.\n2. Generate a JMeter script that simulates normal traffic patterns for the application.\n\n## Best Practices\n\n- **Realistic Scenarios**: Define load test scenarios that accurately reflect real-world usage patterns.\n- **Threshold Definition**: Establish clear performance thresholds to identify potential issues.\n- **Iterative Testing**: Run load tests iteratively to identify and address performance bottlenecks early in the development cycle.\n\n## Integration\n\nThis skill can be integrated with CI/CD pipelines to automate performance testing as part of the deployment process. It can also be used in conjunction with monitoring tools to correlate performance metrics with application behavior.\n\n## Prerequisites\n\n- Load testing tools installed (k6, JMeter, or Artillery)\n- Access to target application endpoints\n- Test scenario definitions and expected load patterns\n- Results storage location at {baseDir}/load-tests/\n\n## Instructions\n\n1. Analyze application architecture and identify critical endpoints\n2. Define test scenarios (baseline, stress, spike, soak, scalability)\n3. Generate appropriate load test scripts using selected tool\n4. Configure performance thresholds and acceptance criteria\n5. Execute load tests and capture metrics\n6. Analyze results and identify performance bottlenecks\n\n## Output\n\n- Load test scripts (k6, JMeter, or Artillery format)\n- Test execution logs and metrics\n- Performance reports with response times and throughput\n- Threshold violation alerts\n- Recommendations for performance improvements\n\n## Error Handling\n\nIf load test execution fails:\n- Verify tool installation and configuration\n- Check network connectivity to target endpoints\n- Validate authentication and authorization\n- Review test script syntax and parameters\n- Ensure sufficient system resources for test execution\n\n## Resources\n\n- k6 documentation and examples\n- JMeter user manual and best practices\n- Artillery load testing guides\n- Performance testing methodology references",
      "parentPlugin": {
        "name": "load-test-runner",
        "category": "performance",
        "path": "plugins/performance/load-test-runner",
        "version": "1.0.0",
        "description": "Create and execute load tests for performance validation"
      },
      "filePath": "plugins/performance/load-test-runner/skills/running-load-tests/SKILL.md"
    },
    {
      "slug": "running-mutation-tests",
      "name": "running-mutation-tests",
      "description": "Execute mutation testing to evaluate test suite effectiveness. Use when performing specialized testing. Trigger with phrases like \"run mutation tests\", \"test the tests\", or \"validate test effectiveness\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:mutation-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Mutation Test Runner\n\nThis skill provides automated assistance for mutation test runner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:mutation-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for mutation test runner tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "mutation-test-runner",
        "category": "testing",
        "path": "plugins/testing/mutation-test-runner",
        "version": "1.0.0",
        "description": "Mutation testing to validate test quality by introducing code changes and verifying tests catch them"
      },
      "filePath": "plugins/testing/mutation-test-runner/skills/running-mutation-tests/SKILL.md"
    },
    {
      "slug": "running-performance-tests",
      "name": "running-performance-tests",
      "description": "Execute load testing, stress testing, and performance benchmarking. Use when performing specialized testing. Trigger with phrases like \"run load tests\", \"test performance\", or \"benchmark the system\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:perf-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Performance Test Suite\n\nThis skill provides automated assistance for performance test suite tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:perf-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for performance test suite tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "performance-test-suite",
        "category": "testing",
        "path": "plugins/testing/performance-test-suite",
        "version": "1.0.0",
        "description": "Load testing and performance benchmarking with metrics analysis and bottleneck identification"
      },
      "filePath": "plugins/testing/performance-test-suite/skills/running-performance-tests/SKILL.md"
    },
    {
      "slug": "running-smoke-tests",
      "name": "running-smoke-tests",
      "description": "Execute fast smoke tests validating critical functionality after deployment. Use when performing specialized testing. Trigger with phrases like \"run smoke tests\", \"quick validation\", or \"test critical paths\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:smoke-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Smoke Test Runner\n\nThis skill provides automated assistance for smoke test runner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:smoke-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for smoke test runner tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "smoke-test-runner",
        "category": "testing",
        "path": "plugins/testing/smoke-test-runner",
        "version": "1.0.0",
        "description": "Quick smoke test suites to verify critical functionality after deployments"
      },
      "filePath": "plugins/testing/smoke-test-runner/skills/running-smoke-tests/SKILL.md"
    },
    {
      "slug": "scanning-accessibility",
      "name": "scanning-accessibility",
      "description": "Validate WCAG compliance and accessibility standards (ARIA, keyboard navigation). Use when auditing WCAG compliance or screen reader compatibility. Trigger with phrases like \"scan accessibility\", \"check WCAG compliance\", or \"validate screen readers\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:a11y-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Accessibility Test Scanner\n\nThis skill provides automated assistance for accessibility test scanner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:a11y-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for accessibility test scanner tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "accessibility-test-scanner",
        "category": "testing",
        "path": "plugins/testing/accessibility-test-scanner",
        "version": "1.0.0",
        "description": "A11y compliance testing with WCAG 2.1/2.2 validation, screen reader compatibility, and automated accessibility audits"
      },
      "filePath": "plugins/testing/accessibility-test-scanner/skills/scanning-accessibility/SKILL.md"
    },
    {
      "slug": "scanning-api-security",
      "name": "scanning-api-security",
      "description": "Detect API security vulnerabilities including injection, broken auth, and data exposure. Use when scanning APIs for security vulnerabilities. Trigger with phrases like \"scan API security\", \"check for vulnerabilities\", or \"audit API security\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:security-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Scanning Api Security\n\n## Overview\n\n\nThis skill provides automated assistance for api security scanner tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:security-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-security-scanner",
        "category": "api-development",
        "path": "plugins/api-development/api-security-scanner",
        "version": "1.0.0",
        "description": "Scan APIs for security vulnerabilities and OWASP API Top 10"
      },
      "filePath": "plugins/api-development/api-security-scanner/skills/scanning-api-security/SKILL.md"
    },
    {
      "slug": "scanning-container-security",
      "name": "scanning-container-security",
      "description": "Execute use when you need to work with security and compliance. This skill provides security scanning and vulnerability detection with comprehensive guidance and automation. Trigger with phrases like \"scan for vulnerabilities\", \"implement security controls\", or \"audit security\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(docker:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Container Security Scanner\n\nThis skill provides automated assistance for container security scanner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/container-security-scanner/`\n\n**Documentation and Guides**: `{baseDir}/docs/container-security-scanner/`\n\n**Example Scripts and Code**: `{baseDir}/examples/container-security-scanner/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/container-security-scanner-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/container-security-scanner-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/container-security-scanner-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "container-security-scanner",
        "category": "devops",
        "path": "plugins/devops/container-security-scanner",
        "version": "1.0.0",
        "description": "Scan containers for vulnerabilities using Trivy, Snyk, and other security tools"
      },
      "filePath": "plugins/devops/container-security-scanner/skills/scanning-container-security/SKILL.md"
    },
    {
      "slug": "scanning-database-security",
      "name": "scanning-database-security",
      "description": "Process use when you need to work with security and compliance. This skill provides security scanning and vulnerability detection with comprehensive guidance and automation. Trigger with phrases like \"scan for vulnerabilities\", \"implement security controls\", or \"audit security\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Security Scanner\n\nThis skill provides automated assistance for database security scanner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-security-scanner/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-security-scanner/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-security-scanner/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-security-scanner-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-security-scanner-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-security-scanner-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-security-scanner",
        "category": "database",
        "path": "plugins/database/database-security-scanner",
        "version": "1.0.0",
        "description": "Database plugin for database-security-scanner"
      },
      "filePath": "plugins/database/database-security-scanner/skills/scanning-database-security/SKILL.md"
    },
    {
      "slug": "scanning-for-data-privacy-issues",
      "name": "scanning-for-data-privacy-issues",
      "description": "Scan for data privacy issues and sensitive information exposure. Use when reviewing data handling practices. Trigger with 'scan privacy issues', 'check sensitive data', or 'validate data protection'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Data Privacy Scanner\n\nThis skill provides automated assistance for data privacy scanner tasks.\n\n## Overview\n\nThis skill automates the process of identifying data privacy risks within a codebase. By leveraging the data-privacy-scanner plugin, Claude can quickly pinpoint potential vulnerabilities, helping developers proactively address compliance requirements and protect sensitive user data.\n\n## How It Works\n\n1. **Initiate Scan**: Upon detecting a privacy-related trigger phrase, Claude activates the data-privacy-scanner plugin.\n2. **Analyze Codebase**: The plugin analyzes the specified files or the entire project for potential data privacy violations.\n3. **Report Findings**: The plugin generates a detailed report outlining identified risks, including the location of the vulnerability and a description of the potential impact.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify potential data privacy vulnerabilities in a codebase.\n- Ensure compliance with data privacy regulations such as GDPR, CCPA, or HIPAA.\n- Perform a privacy audit of a project involving sensitive user data.\n\n## Examples\n\n### Example 1: Identifying PII Leaks\n\nUser request: \"Scan this project for PII leaks.\"\n\nThe skill will:\n1. Activate the data-privacy-scanner plugin to analyze the project.\n2. Generate a report highlighting potential Personally Identifiable Information (PII) leaks, such as exposed email addresses or phone numbers.\n\n### Example 2: Checking GDPR Compliance\n\nUser request: \"Check this configuration file for GDPR compliance issues.\"\n\nThe skill will:\n1. Activate the data-privacy-scanner plugin to analyze the specified configuration file.\n2. Generate a report identifying potential GDPR violations, such as insufficient data anonymization or improper consent management.\n\n## Best Practices\n\n- **Scope**: Specify the relevant files or directories to narrow the scope of the scan and improve performance.\n- **Context**: Provide context about the type of data being processed to help the plugin identify relevant privacy risks.\n- **Review**: Carefully review the generated report to understand the identified vulnerabilities and implement appropriate remediation measures.\n\n## Integration\n\nThis skill can be integrated with other security and compliance tools to provide a comprehensive approach to data privacy. For example, it can be combined with vulnerability scanning tools to identify related security risks or with reporting tools to track progress on remediation efforts.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "data-privacy-scanner",
        "category": "security",
        "path": "plugins/security/data-privacy-scanner",
        "version": "1.0.0",
        "description": "Scan for data privacy issues"
      },
      "filePath": "plugins/security/data-privacy-scanner/skills/scanning-for-data-privacy-issues/SKILL.md"
    },
    {
      "slug": "scanning-for-gdpr-compliance",
      "name": "scanning-for-gdpr-compliance",
      "description": "Scan for GDPR compliance issues in data handling and privacy practices. Use when ensuring EU data protection compliance. Trigger with 'scan GDPR compliance', 'check data privacy', or 'validate GDPR'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Gdpr Compliance Scanner\n\nThis skill provides automated assistance for gdpr compliance scanner tasks.\n\n## Overview\n\nThis skill allows Claude to automatically assess an application's GDPR compliance posture. It provides a comprehensive scan, identifying potential violations and offering actionable recommendations to improve compliance. The skill simplifies the complex process of GDPR auditing, making it easier to identify and address critical gaps.\n\n## How It Works\n\n1. **Initiate Scan**: The user requests a GDPR compliance scan using natural language.\n2. **Plugin Activation**: Claude activates the `gdpr-compliance-scanner` plugin.\n3. **Compliance Assessment**: The plugin scans the application or system based on GDPR requirements.\n4. **Report Generation**: A detailed report is generated, highlighting compliance scores, critical gaps, and recommended actions.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Assess an application's GDPR compliance.\n- Identify potential GDPR violations.\n- Generate a report outlining compliance gaps and recommendations.\n- Audit data processing activities for adherence to GDPR principles.\n\n## Examples\n\n### Example 1: Assess GDPR Compliance of a Web Application\n\nUser request: \"Scan my web application for GDPR compliance.\"\n\nThe skill will:\n1. Activate the `gdpr-compliance-scanner` plugin.\n2. Scan the web application for GDPR compliance issues related to data collection, storage, and processing.\n3. Generate a report highlighting compliance scores, critical gaps such as missing cookie consent mechanisms, and actionable recommendations like implementing a cookie consent banner.\n\n### Example 2: Audit Data Processing Activities\n\nUser request: \"Check our data processing activities for GDPR compliance.\"\n\nThe skill will:\n1. Activate the `gdpr-compliance-scanner` plugin.\n2. Analyze data processing activities, including data collection methods, storage practices, and security measures.\n3. Generate a report identifying potential violations, such as inadequate data encryption or missing data processing agreements, along with recommendations for remediation.\n\n## Best Practices\n\n- **Specificity**: Provide as much context as possible about the application or system being scanned to improve the accuracy of the assessment.\n- **Regularity**: Schedule regular GDPR compliance scans to ensure ongoing adherence to regulatory requirements.\n- **Actionable Insights**: Prioritize addressing the critical gaps identified in the report to mitigate potential risks.\n\n## Integration\n\nThis skill can be integrated with other security and compliance tools to provide a holistic view of an application's security posture. It can also be used in conjunction with code generation tools to automatically implement recommended changes and improve GDPR compliance.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "gdpr-compliance-scanner",
        "category": "security",
        "path": "plugins/security/gdpr-compliance-scanner",
        "version": "1.0.0",
        "description": "Scan for GDPR compliance issues"
      },
      "filePath": "plugins/security/gdpr-compliance-scanner/skills/scanning-for-gdpr-compliance/SKILL.md"
    },
    {
      "slug": "scanning-for-secrets",
      "name": "scanning-for-secrets",
      "description": "Detect exposed secrets, API keys, and credentials in code. Use when auditing for secret leaks. Trigger with 'scan for secrets', 'find exposed keys', or 'check credentials'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Secret Scanner\n\nThis skill provides automated assistance for secret scanner tasks.\n\n## Overview\n\nThis skill enables Claude to scan your codebase for exposed secrets, API keys, passwords, and other sensitive credentials. It helps you identify and remediate potential security vulnerabilities before they are committed or deployed.\n\n## How It Works\n\n1. **Initiate Scan**: Claude activates the `secret-scanner` plugin.\n2. **Codebase Analysis**: The plugin scans the codebase using pattern matching and entropy analysis.\n3. **Report Generation**: A detailed report is generated, highlighting identified secrets, their locations, and suggested remediation steps.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Scan your codebase for exposed API keys (e.g., AWS, Google, Azure).\n- Check for hardcoded passwords in configuration files.\n- Identify potential private keys (SSH, PGP) accidentally committed to the repository.\n- Proactively find secrets before committing changes.\n\n## Examples\n\n### Example 1: Identifying Exposed AWS Keys\n\nUser request: \"Scan for AWS keys in the codebase\"\n\nThe skill will:\n1. Activate the `secret-scanner` plugin.\n2. Scan the codebase for patterns matching AWS Access Keys (AKIA[0-9A-Z]{16}).\n3. Generate a report listing any found keys, their file locations, and remediation steps (e.g., revoking the key).\n\n### Example 2: Checking for Hardcoded Passwords\n\nUser request: \"Check for exposed credentials in config files\"\n\nThe skill will:\n1. Activate the `secret-scanner` plugin.\n2. Scan configuration files (e.g., `database.yml`, `.env`) for password patterns.\n3. Generate a report detailing any found passwords and suggesting the use of environment variables.\n\n## Best Practices\n\n- **Regular Scanning**: Schedule regular scans to catch newly introduced secrets.\n- **Pre-Commit Hooks**: Integrate the `secret-scanner` into your pre-commit hooks to prevent committing secrets.\n- **Review Entropy Analysis**: Carefully review results from entropy analysis, as they may indicate potential secrets not caught by pattern matching.\n\n## Integration\n\nThis skill can be integrated with other security tools, such as vulnerability scanners, to provide a comprehensive security assessment of your codebase. It can also be combined with notification plugins to alert you when new secrets are detected.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "secret-scanner",
        "category": "security",
        "path": "plugins/security/secret-scanner",
        "version": "1.0.0",
        "description": "Scan codebase for exposed secrets, API keys, passwords, and sensitive credentials"
      },
      "filePath": "plugins/security/secret-scanner/skills/scanning-for-secrets/SKILL.md"
    },
    {
      "slug": "scanning-for-vulnerabilities",
      "name": "scanning-for-vulnerabilities",
      "description": "Execute this skill enables comprehensive vulnerability scanning using the vulnerability-scanner plugin. it identifies security vulnerabilities in code, dependencies, and configurations, including cve detection. use this skill when the user asks to scan fo... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Vulnerability Scanner\n\nThis skill provides automated assistance for vulnerability scanner tasks.\n\n## Overview\n\nThis skill empowers Claude to automatically scan your codebase for security vulnerabilities. It leverages the vulnerability-scanner plugin to identify potential risks, including code-level flaws, vulnerable dependencies, and insecure configurations.\n\n## How It Works\n\n1. **Initiate Scan**: The skill activates the vulnerability-scanner plugin based on user input.\n2. **Perform Analysis**: The plugin scans the codebase, dependencies, and configurations for vulnerabilities, including CVE detection.\n3. **Generate Report**: The plugin creates a detailed vulnerability report with findings, severity levels, and remediation guidance.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify security vulnerabilities in your code.\n- Check your project's dependencies for known CVEs.\n- Review your project's configurations for security weaknesses.\n\n## Examples\n\n### Example 1: Identifying SQL Injection Risks\n\nUser request: \"Scan my code for SQL injection vulnerabilities.\"\n\nThe skill will:\n1. Activate the vulnerability-scanner plugin.\n2. Analyze the codebase for potential SQL injection flaws.\n3. Generate a report highlighting any identified SQL injection risks and providing remediation steps.\n\n### Example 2: Checking for Vulnerable npm Packages\n\nUser request: \"Check my project's npm dependencies for known vulnerabilities.\"\n\nThe skill will:\n1. Activate the vulnerability-scanner plugin.\n2. Scan the project's `package.json` file and identify any npm packages with known CVEs.\n3. Generate a report listing the vulnerable packages, their CVE identifiers, and recommended updates.\n\n## Best Practices\n\n- **Regular Scanning**: Run vulnerability scans regularly, especially before deployments.\n- **Prioritize Remediation**: Focus on addressing critical and high-severity vulnerabilities first.\n- **Validate Fixes**: After applying fixes, run another scan to ensure the vulnerabilities are resolved.\n\n## Integration\n\nThis skill integrates with the core Claude Code environment by providing automated vulnerability scanning capabilities. It can be used in conjunction with other plugins to create a comprehensive security workflow, such as integrating with a ticketing system to automatically create tickets for identified vulnerabilities.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "vulnerability-scanner",
        "category": "security",
        "path": "plugins/security/vulnerability-scanner",
        "version": "1.0.0",
        "description": "Comprehensive vulnerability scanning for code, dependencies, and configurations with CVE detection"
      },
      "filePath": "plugins/security/vulnerability-scanner/skills/scanning-for-vulnerabilities/SKILL.md"
    },
    {
      "slug": "scanning-for-xss-vulnerabilities",
      "name": "scanning-for-xss-vulnerabilities",
      "description": "Execute this skill enables AI assistant to automatically scan for xss (cross-site scripting) vulnerabilities in code. it is triggered when the user requests to \"scan for xss vulnerabilities\", \"check for xss\", or uses the command \"/xss\". the skill identifies ref... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, WebFetch, WebSearch, Grep version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Xss Vulnerability Scanner\n\nThis skill provides automated assistance for xss vulnerability scanner tasks.\n\n## Overview\n\nThis skill empowers Claude to proactively identify and report XSS vulnerabilities within your codebase. By leveraging advanced detection techniques, including context-aware analysis and WAF bypass testing, this skill ensures your web applications are resilient against common XSS attack vectors. It provides detailed insights into vulnerability types and offers guidance on remediation strategies.\n\n## How It Works\n\n1. **Activation**: Claude recognizes the user's intent to scan for XSS vulnerabilities through specific trigger phrases like \"scan for XSS\" or the shortcut \"/xss\".\n2. **Code Analysis**: The plugin analyzes the codebase, identifying potential XSS vulnerabilities across different contexts (HTML, JavaScript, CSS, URL).\n3. **Vulnerability Detection**: The plugin detects reflected, stored, and DOM-based XSS vulnerabilities by injecting various payloads and analyzing the responses.\n4. **Reporting**: The plugin generates a report highlighting identified vulnerabilities, their location in the code, and recommended remediation steps.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Perform a security audit of your web application.\n- Review code for potential XSS vulnerabilities.\n- Ensure compliance with security standards.\n- Test the effectiveness of your Content Security Policy (CSP).\n- Identify and mitigate XSS vulnerabilities before deploying to production.\n\n## Examples\n\n### Example 1: Detecting Reflected XSS\n\nUser request: \"scan for XSS vulnerabilities in the search functionality\"\n\nThe skill will:\n1. Analyze the code related to the search functionality.\n2. Identify a reflected XSS vulnerability in how search queries are displayed.\n3. Report the vulnerability, including the affected code snippet and a suggested fix using proper sanitization.\n\n### Example 2: Identifying Stored XSS\n\nUser request: \"/xss check the comment submission form\"\n\nThe skill will:\n1. Analyze the comment submission form and its associated backend code.\n2. Detect a stored XSS vulnerability where user comments are saved to the database without sanitization.\n3. Report the vulnerability, highlighting the unsanitized comment storage and suggesting the use of a sanitization library like `sanitizeHtml`.\n\n## Best Practices\n\n- **Sanitization**: Always sanitize user input before displaying it on the page. Use appropriate escaping functions for the specific context (HTML, JavaScript, URL).\n- **Content Security Policy (CSP)**: Implement a strong CSP to restrict the sources from which the browser can load resources, mitigating the impact of XSS vulnerabilities.\n- **Regular Updates**: Keep your web application framework and libraries up to date to patch known XSS vulnerabilities.\n\n## Integration\n\nThis skill complements other security-focused plugins by providing targeted XSS vulnerability detection. It can be integrated with code review tools to automate security checks and provide developers with immediate feedback on potential XSS issues.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "xss-vulnerability-scanner",
        "category": "security",
        "path": "plugins/security/xss-vulnerability-scanner",
        "version": "1.0.0",
        "description": "Scan for XSS vulnerabilities"
      },
      "filePath": "plugins/security/xss-vulnerability-scanner/skills/scanning-for-xss-vulnerabilities/SKILL.md"
    },
    {
      "slug": "scanning-input-validation-practices",
      "name": "scanning-input-validation-practices",
      "description": "Scan for input validation vulnerabilities and injection risks. Use when reviewing user input handling. Trigger with 'scan input validation', 'check injection vulnerabilities', or 'validate sanitization'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Input Validation Scanner\n\nThis skill provides automated assistance for input validation scanner tasks.\n\n## Overview\n\nThis skill automates the process of identifying potential input validation flaws within a codebase. By analyzing how user-provided data is handled, it helps developers proactively address security vulnerabilities before they can be exploited. This skill streamlines security audits and improves the overall security posture of applications.\n\n## How It Works\n\n1. **Initiate Scan**: The user requests an input validation scan, triggering the skill.\n2. **Code Analysis**: The skill uses the input-validation-scanner plugin to analyze the specified codebase or file.\n3. **Vulnerability Identification**: The plugin identifies instances where input validation may be missing or insufficient.\n4. **Report Generation**: The skill presents a report highlighting potential vulnerabilities and their locations in the code.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Audit a codebase for input validation vulnerabilities.\n- Review newly written code for potential XSS or SQL injection flaws.\n- Harden an application against common web security exploits.\n- Ensure compliance with security best practices related to input handling.\n\n## Examples\n\n### Example 1: Identifying XSS Vulnerabilities\n\nUser request: \"Scan the user profile module for potential XSS vulnerabilities.\"\n\nThe skill will:\n1. Activate the input-validation-scanner plugin on the specified module.\n2. Generate a report highlighting areas where user input is directly rendered without proper sanitization, indicating potential XSS vulnerabilities.\n\n### Example 2: Checking for SQL Injection Risks\n\nUser request: \"Check the database access layer for potential SQL injection risks.\"\n\nThe skill will:\n1. Use the input-validation-scanner plugin to examine the database access code.\n2. Identify instances where user input is used directly in SQL queries without proper parameterization or escaping, indicating potential SQL injection vulnerabilities.\n\n## Best Practices\n\n- **Regular Scanning**: Integrate input validation scanning into your regular development workflow.\n- **Contextual Analysis**: Always review the identified vulnerabilities in context to determine their actual impact and severity.\n- **Comprehensive Validation**: Ensure that all user-supplied data is validated, including data from forms, APIs, and external sources.\n\n## Integration\n\nThis skill can be used in conjunction with other security-related skills to provide a more comprehensive security assessment. For example, it can be combined with a static analysis skill to identify other types of vulnerabilities or with a dependency scanning skill to identify vulnerable third-party libraries.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "input-validation-scanner",
        "category": "security",
        "path": "plugins/security/input-validation-scanner",
        "version": "1.0.0",
        "description": "Scan input validation practices"
      },
      "filePath": "plugins/security/input-validation-scanner/skills/scanning-input-validation-practices/SKILL.md"
    },
    {
      "slug": "scanning-market-movers",
      "name": "scanning-market-movers",
      "description": "Detect significant price movements and unusual volume across crypto markets. Use when tracking significant price movements. Trigger with phrases like \"scan market movers\", \"check biggest gainers\", or \"find pumps\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:movers-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Scanning Market Movers\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:movers-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "market-movers-scanner",
        "category": "crypto",
        "path": "plugins/crypto/market-movers-scanner",
        "version": "1.0.0",
        "description": "Scan for top market movers - gainers, losers, volume spikes, and unusual activity"
      },
      "filePath": "plugins/crypto/market-movers-scanner/skills/scanning-market-movers/SKILL.md"
    },
    {
      "slug": "setting-up-distributed-tracing",
      "name": "setting-up-distributed-tracing",
      "description": "Execute this skill automates the setup of distributed tracing for microservices. it helps developers implement end-to-end request visibility by configuring context propagation, span creation, trace collection, and analysis. use this skill when the user re... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Distributed Tracing Setup\n\nThis skill provides automated assistance for distributed tracing setup tasks.\n\n## Overview\n\nThis skill streamlines the process of setting up distributed tracing in a microservices environment. It guides you through the key steps of instrumenting your services, configuring trace context propagation, and selecting a backend for trace collection and analysis, enabling comprehensive monitoring and debugging.\n\n## How It Works\n\n1. **Backend Selection**: Determines the preferred tracing backend (e.g., Jaeger, Zipkin, Datadog).\n2. **Instrumentation Strategy**: Designs an instrumentation strategy for each service, focusing on key operations and dependencies.\n3. **Configuration Generation**: Generates the necessary configuration files and code snippets to enable distributed tracing.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Implement distributed tracing in a microservices application.\n- Gain end-to-end visibility into request flows across multiple services.\n- Troubleshoot performance bottlenecks and latency issues.\n\n## Examples\n\n### Example 1: Adding Tracing to a New Microservice\n\nUser request: \"setup tracing for the new payment service\"\n\nThe skill will:\n1. Prompt for the preferred tracing backend (e.g., Jaeger).\n2. Generate code snippets for OpenTelemetry instrumentation in the payment service.\n\n### Example 2: Troubleshooting Performance Issues\n\nUser request: \"implement distributed tracing to debug slow checkout process\"\n\nThe skill will:\n1. Guide the user through instrumenting relevant services in the checkout flow.\n2. Provide configuration examples for context propagation.\n\n## Best Practices\n\n- **Backend Choice**: Select a tracing backend that aligns with your existing infrastructure and monitoring tools.\n- **Sampling Strategy**: Implement a sampling strategy to manage trace volume and cost, especially in high-traffic environments.\n- **Context Propagation**: Ensure proper context propagation across all services to maintain trace continuity.\n\n## Integration\n\nThis skill can be used in conjunction with other plugins to automate the deployment and configuration of tracing infrastructure. For example, it can integrate with infrastructure-as-code tools to provision Jaeger or Zipkin clusters.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "distributed-tracing-setup",
        "category": "performance",
        "path": "plugins/performance/distributed-tracing-setup",
        "version": "1.0.0",
        "description": "Set up distributed tracing for microservices"
      },
      "filePath": "plugins/performance/distributed-tracing-setup/skills/setting-up-distributed-tracing/SKILL.md"
    },
    {
      "slug": "setting-up-experiment-tracking",
      "name": "setting-up-experiment-tracking",
      "description": "Implement machine learning experiment tracking using MLflow or Weights & Biases. Configures environment and provides code for logging parameters, metrics, and artifacts. Use when asked to \"setup experiment tracking\" or \"initialize MLflow\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Experiment Tracking Setup\n\nThis skill provides automated assistance for experiment tracking setup tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for experiment tracking setup tasks.\nThis skill streamlines the process of setting up experiment tracking for machine learning projects. It automates environment configuration, tool initialization, and provides code examples to get you started quickly.\n\n## How It Works\n\n1. **Analyze Context**: The skill analyzes the current project context to determine the appropriate experiment tracking tool (MLflow or W&B) based on user preference or existing project configuration.\n2. **Configure Environment**: It configures the environment by installing necessary Python packages and setting environment variables.\n3. **Initialize Tracking**: The skill initializes the chosen tracking tool, potentially starting a local MLflow server or connecting to a W&B project.\n4. **Provide Code Snippets**: It provides code snippets demonstrating how to log experiment parameters, metrics, and artifacts within your ML code.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Start tracking machine learning experiments in a new project.\n- Integrate experiment tracking into an existing ML project.\n- Quickly set up MLflow or Weights & Biases for experiment management.\n- Automate the process of logging parameters, metrics, and artifacts.\n\n## Examples\n\n### Example 1: Starting a New Project with MLflow\n\nUser request: \"track experiments using mlflow\"\n\nThe skill will:\n1. Install the `mlflow` Python package.\n2. Generate example code for logging parameters, metrics, and artifacts to an MLflow server.\n\n### Example 2: Integrating W&B into an Existing Project\n\nUser request: \"setup experiment tracking with wandb\"\n\nThe skill will:\n1. Install the `wandb` Python package.\n2. Generate example code for initializing W&B and logging experiment data.\n\n## Best Practices\n\n- **Tool Selection**: Consider the scale and complexity of your project when choosing between MLflow and W&B. MLflow is well-suited for local tracking, while W&B offers cloud-based collaboration and advanced features.\n- **Consistent Logging**: Establish a consistent logging strategy for parameters, metrics, and artifacts to ensure comparability across experiments.\n- **Artifact Management**: Utilize artifact logging to track models, datasets, and other relevant files associated with each experiment.\n\n## Integration\n\nThis skill can be used in conjunction with other skills that generate or modify machine learning code, such as skills for model training or data preprocessing. It ensures that all experiments are properly tracked and documented.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "experiment-tracking-setup",
        "category": "ai-ml",
        "path": "plugins/ai-ml/experiment-tracking-setup",
        "version": "1.0.0",
        "description": "Set up ML experiment tracking"
      },
      "filePath": "plugins/ai-ml/experiment-tracking-setup/skills/setting-up-experiment-tracking/SKILL.md"
    },
    {
      "slug": "setting-up-log-aggregation",
      "name": "setting-up-log-aggregation",
      "description": "Execute use when setting up log aggregation solutions using ELK, Loki, or Splunk. Trigger with phrases like \"setup log aggregation\", \"deploy ELK stack\", \"configure Loki\", or \"install Splunk\". Generates production-ready configurations for data ingestion, processing, storage, and visualization with proper security and scalability. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(docker:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Log Aggregation Setup\n\nThis skill provides automated assistance for log aggregation setup tasks.\n\n## Overview\n\nSets up centralized log aggregation (ELK/Loki/Splunk) including ingestion pipelines, parsing, retention policies, dashboards, and security controls.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Target infrastructure is identified (Kubernetes, Docker, VMs)\n- Storage requirements are calculated based on log volume\n- Network connectivity between log sources and aggregation platform\n- Authentication mechanism is defined (LDAP, OAuth, basic auth)\n- Resource allocation planned (CPU, memory, disk)\n\n## Instructions\n\n1. **Select Platform**: Choose ELK, Loki, Grafana Loki, or Splunk\n2. **Configure Ingestion**: Set up log shippers (Filebeat, Promtail, Fluentd)\n3. **Define Storage**: Configure retention policies and index lifecycle\n4. **Set Up Processing**: Create parsing rules and field extractions\n5. **Deploy Visualization**: Configure Kibana/Grafana dashboards\n6. **Implement Security**: Enable authentication, encryption, and RBAC\n7. **Test Pipeline**: Verify logs flow from sources to visualization\n\n## Output\n\n**ELK Stack (Docker Compose):**\n```yaml\n# {baseDir}/elk/docker-compose.yml\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\nversion: '3.8'\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0\n    environment:\n      - discovery.type=single-node\n      - xpack.security.enabled=true\n    volumes:\n      - es-data:/usr/share/elasticsearch/data\n    ports:\n      - \"9200:9200\"\n\n  logstash:\n    image: docker.elastic.co/logstash/logstash:8.11.0\n    volumes:\n      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf\n    depends_on:\n      - elasticsearch\n\n  kibana:\n    image: docker.elastic.co/kibana/kibana:8.11.0\n    ports:\n      - \"5601:5601\"\n    depends_on:\n      - elasticsearch\n```\n\n**Loki Configuration:**\n```yaml\n# {baseDir}/loki/loki-config.yaml\nauth_enabled: false\n\nserver:\n  http_listen_port: 3100\n\ningester:\n  lifecycler:\n    ring:\n      kvstore:\n        store: inmemory\n      replication_factor: 1\n  chunk_idle_period: 5m\n  chunk_retain_period: 30s\n\nschema_config:\n  configs:\n    - from: 2024-01-01\n      store: boltdb-shipper\n      object_store: filesystem\n      schema: v11\n      index:\n        prefix: index_\n        period: 24h\n```\n\n## Error Handling\n\n**Out of Memory**\n- Error: \"Elasticsearch heap space exhausted\"\n- Solution: Increase heap size in elasticsearch.yml or add more nodes\n\n**Connection Refused**\n- Error: \"Cannot connect to Elasticsearch\"\n- Solution: Verify network connectivity and firewall rules\n\n**Index Creation Failed**\n- Error: \"Failed to create index\"\n- Solution: Check disk space and index template configuration\n\n**Log Parsing Errors**\n- Error: \"Failed to parse log line\"\n- Solution: Review grok patterns or JSON parsing configuration\n\n## Examples\n\n- \"Deploy Loki + Promtail on Kubernetes with 14-day retention and basic auth.\"\n- \"Set up an ELK stack for app + nginx logs and create a dashboard for 5xx errors.\"\n\n## Resources\n\n- ELK Stack guide: https://www.elastic.co/guide/\n- Loki documentation: https://grafana.com/docs/loki/\n- Example configurations in {baseDir}/log-aggregation-examples/",
      "parentPlugin": {
        "name": "log-aggregation-setup",
        "category": "devops",
        "path": "plugins/devops/log-aggregation-setup",
        "version": "1.0.0",
        "description": "Set up log aggregation (ELK, Loki, Splunk)"
      },
      "filePath": "plugins/devops/log-aggregation-setup/skills/setting-up-log-aggregation/SKILL.md"
    },
    {
      "slug": "setting-up-synthetic-monitoring",
      "name": "setting-up-synthetic-monitoring",
      "description": "Setup synthetic monitoring for proactive performance tracking including uptime checks, transaction monitoring, and API health. Use when implementing availability monitoring or tracking critical user journeys. Trigger with phrases like \"setup synthetic monitoring\", \"monitor uptime\", or \"configure health checks\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(curl:*)",
        "Bash(monitoring:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Synthetic Monitoring Setup\n\nThis skill provides automated assistance for synthetic monitoring setup tasks.\n\n## Overview\n\nThis skill streamlines the process of setting up synthetic monitoring, enabling proactive performance tracking for applications. It guides the user through defining key monitoring scenarios and configuring alerts to ensure optimal application performance and availability.\n\n## How It Works\n\n1. **Identify Monitoring Needs**: Determine the critical endpoints, user journeys, and APIs to monitor based on the user's application requirements.\n2. **Design Monitoring Scenarios**: Create specific monitoring scenarios for uptime, transactions, and API performance, including frequency and location.\n3. **Configure Monitoring**: Set up the synthetic monitoring tool with the designed scenarios, including alerts and dashboards for performance visualization.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Implement uptime monitoring for a web application.\n- Track the performance of critical user journeys through transaction monitoring.\n- Monitor the response time and availability of API endpoints.\n\n## Examples\n\n### Example 1: Setting up Uptime Monitoring\n\nUser request: \"Set up uptime monitoring for my website example.com.\"\n\nThe skill will:\n1. Identify example.com as the target endpoint.\n2. Configure uptime monitoring to check the availability of example.com every 5 minutes from multiple locations.\n\n### Example 2: Monitoring API Performance\n\nUser request: \"Configure API monitoring for the /users endpoint of my application.\"\n\nThe skill will:\n1. Identify the /users endpoint as the target for API monitoring.\n2. Set up monitoring to track the response time and status code of the /users endpoint every minute.\n\n## Best Practices\n\n- **Prioritize Critical Endpoints**: Focus on monitoring the most critical endpoints and user journeys that directly impact user experience.\n- **Set Realistic Thresholds**: Configure alerts with realistic thresholds to avoid false positives and ensure timely notifications.\n- **Regularly Review and Adjust**: Periodically review the monitoring configuration and adjust scenarios and thresholds based on application changes and performance trends.\n\n## Integration\n\nThis skill can be integrated with other plugins for incident management and alerting, such as those that handle notifications via Slack or PagerDuty, allowing for automated incident response workflows based on synthetic monitoring results.\n\n## Prerequisites\n\n- Access to synthetic monitoring platform (Pingdom, Datadog, New Relic)\n- List of critical endpoints and user journeys in {baseDir}/monitoring/endpoints.yaml\n- Alerting infrastructure configuration\n- Geographic monitoring location requirements\n\n## Instructions\n\n1. Identify critical endpoints and user journeys to monitor\n2. Design monitoring scenarios (uptime, transactions, API checks)\n3. Configure monitoring frequency and locations\n4. Set up performance and availability thresholds\n5. Configure alerting for failures and degradation\n6. Create dashboards for monitoring visualization\n\n## Output\n\n- Synthetic monitoring configuration files\n- Uptime check definitions for endpoints\n- Transaction monitoring scripts\n- Alert rule configurations\n- Dashboard definitions for monitoring status\n\n## Error Handling\n\nIf synthetic monitoring setup fails:\n- Verify monitoring platform credentials\n- Check endpoint accessibility from monitoring locations\n- Validate transaction script syntax\n- Ensure alert channel configuration\n- Review threshold definitions\n\n## Resources\n\n- Synthetic monitoring best practices\n- Uptime monitoring service documentation\n- Transaction monitoring script examples\n- Alert threshold tuning guides",
      "parentPlugin": {
        "name": "synthetic-monitoring-setup",
        "category": "performance",
        "path": "plugins/performance/synthetic-monitoring-setup",
        "version": "1.0.0",
        "description": "Set up synthetic monitoring for proactive performance tracking"
      },
      "filePath": "plugins/performance/synthetic-monitoring-setup/skills/setting-up-synthetic-monitoring/SKILL.md"
    },
    {
      "slug": "simulating-flash-loans",
      "name": "simulating-flash-loans",
      "description": "Execute simulate flash loan arbitrage strategies and profitability across DeFi protocols. Use when performing crypto analysis. Trigger with phrases like \"analyze crypto\", \"check blockchain\", or \"monitor market\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:flashloan-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Simulating Flash Loans\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:flashloan-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "flash-loan-simulator",
        "category": "crypto",
        "path": "plugins/crypto/flash-loan-simulator",
        "version": "1.0.0",
        "description": "Simulate and analyze flash loan strategies including arbitrage, liquidations, and collateral swaps"
      },
      "filePath": "plugins/crypto/flash-loan-simulator/skills/simulating-flash-loans/SKILL.md"
    },
    {
      "slug": "skill-adapter",
      "name": "skill-adapter",
      "description": "Execute analyzes existing plugins to extract their capabilities, then adapts and applies those skills to the current task. Acts as a universal skill chameleon that learns from other plugins. Activates when you request \"skill adapter\" functionality. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Skill Adapter\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "pi-pathfinder",
        "category": "examples",
        "path": "plugins/examples/pi-pathfinder",
        "version": "1.0.0",
        "description": "PI Pathfinder - Finds the path through 229 plugins. Automatically picks the best plugin for your task, extracts its skills, and applies them. You don't pick plugins, PI does."
      },
      "filePath": "plugins/examples/pi-pathfinder/skills/skill-adapter/SKILL.md"
    },
    {
      "slug": "spec-writing",
      "name": "spec-writing",
      "description": "Execute this skill should be used when the user asks about \"writing specs\", \"specs.md format\", \"how to write specifications\", \"sprint requirements\", \"testing configuration\", \"scope definition\", or needs guidance on creating effective sprint specifications for agentic development. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read version: 1.0.0 author: Damien Laine <damien.laine@gmail.com> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Damien Laine <damien.laine@gmail.com>",
      "license": "MIT",
      "content": "# Spec Writing\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "sprint",
        "category": "community",
        "path": "plugins/community/sprint",
        "version": "1.0.0",
        "description": "Autonomous multi-agent development framework with spec-driven sprints and convergent iteration"
      },
      "filePath": "plugins/community/sprint/skills/spec-writing/SKILL.md"
    },
    {
      "slug": "splitting-datasets",
      "name": "splitting-datasets",
      "description": "Process split datasets into training, validation, and testing sets for ML model development. Use when requesting \"split dataset\", \"train-test split\", or \"data partitioning\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Dataset Splitter\n\nThis skill provides automated assistance for dataset splitter tasks.\n\n## Overview\n\nThis skill automates the process of dividing a dataset into subsets for training, validating, and testing machine learning models. It ensures proper data preparation and facilitates robust model evaluation.\n\n## How It Works\n\n1. **Analyze Request**: The skill analyzes the user's request to determine the dataset to be split and the desired proportions for each subset.\n2. **Generate Code**: Based on the request, the skill generates Python code utilizing standard ML libraries to perform the data splitting.\n3. **Execute Splitting**: The code is executed to split the dataset into training, validation, and testing sets according to the specified ratios.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Prepare a dataset for machine learning model training.\n- Create training, validation, and testing sets.\n- Partition data to evaluate model performance.\n\n## Examples\n\n### Example 1: Splitting a CSV file\n\nUser request: \"Split the data in 'my_data.csv' into 70% training, 15% validation, and 15% testing sets.\"\n\nThe skill will:\n1. Generate Python code to read the 'my_data.csv' file.\n2. Execute the code to split the data according to the specified proportions, creating 'train.csv', 'validation.csv', and 'test.csv' files.\n\n### Example 2: Creating a Train-Test Split\n\nUser request: \"Create a train-test split of 'large_dataset.csv' with an 80/20 ratio.\"\n\nThe skill will:\n1. Generate Python code to load 'large_dataset.csv'.\n2. Execute the code to split the dataset into 80% training and 20% testing sets, saving them as 'train.csv' and 'test.csv'.\n\n## Best Practices\n\n- **Data Integrity**: Verify that the splitting process maintains the integrity of the data, ensuring no data loss or corruption.\n- **Stratification**: Consider stratification when splitting imbalanced datasets to maintain class distributions in each subset.\n- **Randomization**: Ensure the splitting process is randomized to avoid bias in the resulting datasets.\n\n## Integration\n\nThis skill can be integrated with other data processing and model training tools within the Claude Code ecosystem to create a complete machine learning workflow.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "dataset-splitter",
        "category": "ai-ml",
        "path": "plugins/ai-ml/dataset-splitter",
        "version": "1.0.0",
        "description": "Split datasets for training, validation, and testing"
      },
      "filePath": "plugins/ai-ml/dataset-splitter/skills/splitting-datasets/SKILL.md"
    },
    {
      "slug": "sprint-workflow",
      "name": "sprint-workflow",
      "description": "Execute this skill should be used when the user asks about \"how sprints work\", \"sprint phases\", \"iteration workflow\", \"convergent development\", \"sprint lifecycle\", \"when to use sprints\", or wants to understand the sprint execution model and its convergent diffusion approach. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read version: 1.0.0 author: Damien Laine <damien.laine@gmail.com> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Damien Laine <damien.laine@gmail.com>",
      "license": "MIT",
      "content": "# Sprint Workflow\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "sprint",
        "category": "community",
        "path": "plugins/community/sprint",
        "version": "1.0.0",
        "description": "Autonomous multi-agent development framework with spec-driven sprints and convergent iteration"
      },
      "filePath": "plugins/community/sprint/skills/sprint-workflow/SKILL.md"
    },
    {
      "slug": "supabase-advanced-troubleshooting",
      "name": "supabase-advanced-troubleshooting",
      "description": "Execute apply Supabase advanced debugging techniques for hard-to-diagnose issues. Use when standard troubleshooting fails, investigating complex race conditions, or preparing evidence bundles for Supabase support escalation. Trigger with phrases like \"supabase hard bug\", \"supabase mystery error\", \"supabase impossible to debug\", \"difficult supabase issue\", \"supabase deep debug\". allowed-tools: Read, Grep, Bash(kubectl:*), Bash(curl:*), Bash(tcpdump:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Advanced Troubleshooting\n\n## Prerequisites\n- Access to production logs and metrics\n- kubectl access to clusters\n- Network capture tools available\n- Understanding of distributed tracing\n\n## Instructions\n\n### Step 1: Collect Evidence Bundle\nRun the comprehensive debug script to gather all relevant data.\n\n### Step 2: Systematic Isolation\nTest each layer independently to identify the failure point.\n\n### Step 3: Create Minimal Reproduction\nStrip down to the simplest failing case.\n\n### Step 4: Escalate with Evidence\nUse the support template with all collected evidence.\n\n## Output\n- Comprehensive debug bundle collected\n- Failure layer identified\n- Minimal reproduction created\n- Support escalation submitted\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase Support Portal](https://support.supabase.com)\n- [Supabase Status Page](https://status.supabase.com)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-advanced-troubleshooting/SKILL.md"
    },
    {
      "slug": "supabase-architecture-variants",
      "name": "supabase-architecture-variants",
      "description": "Execute choose and implement Supabase validated architecture blueprints for different scales. Use when designing new Supabase integrations, choosing between monolith/service/microservice architectures, or planning migration paths for Supabase applications. Trigger with phrases like \"supabase architecture\", \"supabase blueprint\", \"how to structure supabase\", \"supabase project layout\", \"supabase microservice\". allowed-tools: Read, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Architecture Variants\n\n## Prerequisites\n- Understanding of team size and DAU requirements\n- Knowledge of deployment infrastructure\n- Clear SLA requirements\n- Growth projections available\n\n## Instructions\n\n### Step 1: Assess Requirements\nUse the decision matrix to identify appropriate variant.\n\n### Step 2: Choose Architecture\nSelect Monolith, Service Layer, or Microservice based on needs.\n\n### Step 3: Implement Structure\nSet up project layout following the chosen blueprint.\n\n### Step 4: Plan Migration Path\nDocument upgrade path for future scaling.\n\n## Output\n- Architecture variant selected\n- Project structure implemented\n- Migration path documented\n- Appropriate patterns applied\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Monolith First](https://martinfowler.com/bliki/MonolithFirst.html)\n- [Microservices Guide](https://martinfowler.com/microservices/)\n- [Supabase Architecture Guide](https://supabase.com/docs/architecture)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-architecture-variants/SKILL.md"
    },
    {
      "slug": "supabase-auth-storage-realtime-core",
      "name": "supabase-auth-storage-realtime-core",
      "description": "Execute Supabase secondary workflow: Auth + Storage + Realtime. Use when implementing secondary use case, or complementing primary workflow. Trigger with phrases like \"supabase auth storage realtime\", \"implement full stack features with supabase\". allowed-tools: Read, Write, Edit, Bash(npm:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Auth + Storage + Realtime\n\n## Overview\nImplement the core Supabase trifecta: authentication, file storage,\nand real-time subscriptions in a single cohesive setup.\n\n\n## Prerequisites\n- Completed `supabase-install-auth` setup\n- Familiarity with `supabase-schema-from-requirements`\n- Valid API credentials configured\n\n## Instructions\n\n### Step 1: Setup\n```typescript\n// Step 1 implementation\n```\n\n### Step 2: Process\n```typescript\n// Step 2 implementation\n```\n\n### Step 3: Complete\n```typescript\n// Step 3 implementation\n```\n\n## Output\n- Completed Auth + Storage + Realtime execution\n- Results from Supabase API\n- Success confirmation or error details\n\n## Error Handling\n| Aspect | Schema from Requirements | Auth + Storage + Realtime |\n|--------|------------|------------|\n| Use Case | Starting a new project with defined data requirements | Secondary |\n| Complexity | Medium | Lower |\n| Performance | Standard | Optimized |\n\n## Examples\n\n### Complete Workflow\n```typescript\n// Complete workflow example\n```\n\n### Error Recovery\n```typescript\n// Error handling code\n```\n\n## Resources\n- [Supabase Documentation](https://supabase.com/docs)\n- [Supabase API Reference](https://supabase.com/docs/api)\n\n## Next Steps\nFor common errors, see `supabase-common-errors`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-auth-storage-realtime-core/SKILL.md"
    },
    {
      "slug": "supabase-ci-integration",
      "name": "supabase-ci-integration",
      "description": "Configure Supabase CI/CD integration with GitHub Actions and testing. Use when setting up automated testing, configuring CI pipelines, or integrating Supabase tests into your build process. Trigger with phrases like \"supabase CI\", \"supabase GitHub Actions\", \"supabase automated tests\", \"CI supabase\". allowed-tools: Read, Write, Edit, Bash(gh:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Ci Integration\n\n## Prerequisites\n- GitHub repository with Actions enabled\n- Supabase test API key\n- npm/pnpm project configured\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n- Automated test pipeline\n- PR checks configured\n- Coverage reports uploaded\n- Release workflow ready\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [GitHub Actions Documentation](https://docs.github.com/en/actions)\n- [Supabase CI Guide](https://supabase.com/docs/ci)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-ci-integration/SKILL.md"
    },
    {
      "slug": "supabase-common-errors",
      "name": "supabase-common-errors",
      "description": "Execute diagnose and fix Supabase common errors and exceptions. Use when encountering Supabase errors, debugging failed requests, or troubleshooting integration issues. Trigger with phrases like \"supabase error\", \"fix supabase\", \"supabase not working\", \"debug supabase\". allowed-tools: Read, Grep, Bash(curl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Common Errors\n\n## Prerequisites\n- Supabase SDK installed\n- API credentials configured\n- Access to error logs\n\n## Instructions\n\n### Step 1: Identify the Error\nCheck error message and code in your logs or console.\n\n### Step 2: Find Matching Error Below\nMatch your error to one of the documented cases.\n\n### Step 3: Apply Solution\nFollow the solution steps for your specific error.\n\n## Output\n- Identified error cause\n- Applied fix\n- Verified resolution\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase Status Page](https://status.supabase.com)\n- [Supabase Support](https://supabase.com/docs/support)\n- [Supabase Error Codes](https://supabase.com/docs/errors)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-common-errors/SKILL.md"
    },
    {
      "slug": "supabase-cost-tuning",
      "name": "supabase-cost-tuning",
      "description": "Optimize Supabase costs through tier selection, sampling, and usage monitoring. Use when analyzing Supabase billing, reducing API costs, or implementing usage monitoring and budget alerts. Trigger with phrases like \"supabase cost\", \"supabase billing\", \"reduce supabase costs\", \"supabase pricing\", \"supabase expensive\", \"supabase budget\". allowed-tools: Read, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Cost Tuning\n\n## Prerequisites\n- Access to Supabase billing dashboard\n- Understanding of current usage patterns\n- Database for usage tracking (optional)\n- Alerting system configured (optional)\n\n## Instructions\n\n### Step 1: Analyze Current Usage\nReview Supabase dashboard for usage patterns and costs.\n\n### Step 2: Select Optimal Tier\nUse the cost estimation function to find the right tier.\n\n### Step 3: Implement Monitoring\nAdd usage tracking to catch budget overruns early.\n\n### Step 4: Apply Optimizations\nEnable batching, caching, and sampling where appropriate.\n\n## Output\n- Optimized tier selection\n- Usage monitoring implemented\n- Budget alerts configured\n- Cost reduction strategies applied\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase Pricing](https://supabase.com/pricing)\n- [Supabase Billing Dashboard](https://dashboard.supabase.com/billing)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-cost-tuning/SKILL.md"
    },
    {
      "slug": "supabase-data-handling",
      "name": "supabase-data-handling",
      "description": "Implement Supabase PII handling, data retention, and GDPR/CCPA compliance patterns. Use when handling sensitive data, implementing data redaction, configuring retention policies, or ensuring compliance with privacy regulations for Supabase integrations. Trigger with phrases like \"supabase data\", \"supabase PII\", \"supabase GDPR\", \"supabase data retention\", \"supabase privacy\", \"supabase CCPA\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Data Handling\n\n## Overview\nHandle sensitive data correctly when integrating with Supabase.\n\n## Prerequisites\n- Understanding of GDPR/CCPA requirements\n- Supabase SDK with data export capabilities\n- Database for audit logging\n- Scheduled job infrastructure for cleanup\n\n## Data Classification\n\n| Category | Examples | Handling |\n|----------|----------|----------|\n| PII | Email, name, phone | Encrypt, minimize |\n| Sensitive | API keys, tokens | Never log, rotate |\n| Business | Usage metrics | Aggregate when possible |\n| Public | Product names | Standard handling |\n\n## PII Detection\n\n```typescript\nconst PII_PATTERNS = [\n  { type: 'email', regex: /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}/g },\n  { type: 'phone', regex: /\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b/g },\n  { type: 'ssn', regex: /\\b\\d{3}-\\d{2}-\\d{4}\\b/g },\n  { type: 'credit_card', regex: /\\b\\d{4}[- ]?\\d{4}[- ]?\\d{4}[- ]?\\d{4}\\b/g },\n];\n\nfunction detectPII(text: string): { type: string; match: string }[] {\n  const findings: { type: string; match: string }[] = [];\n\n  for (const pattern of PII_PATTERNS) {\n    const matches = text.matchAll(pattern.regex);\n    for (const match of matches) {\n\n## Detailed Reference\n\nSee `{baseDir}/references/implementation.md` for complete data handling guide.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-data-handling/SKILL.md"
    },
    {
      "slug": "supabase-debug-bundle",
      "name": "supabase-debug-bundle",
      "description": "Execute collect Supabase debug evidence for support tickets and troubleshooting. Use when encountering persistent issues, preparing support tickets, or collecting diagnostic information for Supabase problems. Trigger with phrases like \"supabase debug\", \"supabase support bundle\", \"collect supabase logs\", \"supabase diagnostic\". allowed-tools: Read, Bash(grep:*), Bash(curl:*), Bash(tar:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Debug Bundle\n\n## Prerequisites\n- Supabase SDK installed\n- Access to application logs\n- Permission to collect environment info\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n- `supabase-debug-YYYYMMDD-HHMMSS.tar.gz` archive containing:\n  - `summary.txt` - Environment and SDK info\n  - `logs.txt` - Recent redacted logs\n  - `config-redacted.txt` - Configuration (secrets removed)\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase Support](https://supabase.com/docs/support)\n- [Supabase Status](https://status.supabase.com)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-debug-bundle/SKILL.md"
    },
    {
      "slug": "supabase-deploy-integration",
      "name": "supabase-deploy-integration",
      "description": "Deploy Supabase integrations to Vercel, Fly.io, and Cloud Run platforms. Use when deploying Supabase-powered applications to production, configuring platform-specific secrets, or setting up deployment pipelines. Trigger with phrases like \"deploy supabase\", \"supabase Vercel\", \"supabase production deploy\", \"supabase Cloud Run\", \"supabase Fly.io\". allowed-tools: Read, Write, Edit, Bash(vercel:*), Bash(fly:*), Bash(gcloud:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Deploy Integration\n\n## Prerequisites\n- Supabase API keys for production environment\n- Platform CLI installed (vercel, fly, or gcloud)\n- Application code ready for deployment\n- Environment variables documented\n\n## Instructions\n\n### Step 1: Choose Deployment Platform\nSelect the platform that best fits your infrastructure needs and follow the platform-specific guide below.\n\n### Step 2: Configure Secrets\nStore Supabase API keys securely using the platform's secrets management.\n\n### Step 3: Deploy Application\nUse the platform CLI to deploy your application with Supabase integration.\n\n### Step 4: Verify Health\nTest the health check endpoint to confirm Supabase connectivity.\n\n## Output\n- Application deployed to production\n- Supabase secrets securely configured\n- Health check endpoint functional\n- Environment-specific configuration in place\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel Documentation](https://vercel.com/docs)\n- [Fly.io Documentation](https://fly.io/docs)\n- [Cloud Run Documentation](https://cloud.google.com/run/docs)\n- [Supabase Deploy Guide](https://supabase.com/docs/deploy)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-deploy-integration/SKILL.md"
    },
    {
      "slug": "supabase-enterprise-rbac",
      "name": "supabase-enterprise-rbac",
      "description": "Configure Supabase enterprise SSO, role-based access control, and organization management. Use when implementing SSO integration, configuring role-based permissions, or setting up organization-level controls for Supabase. Trigger with phrases like \"supabase SSO\", \"supabase RBAC\", \"supabase enterprise\", \"supabase roles\", \"supabase permissions\", \"supabase SAML\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Enterprise Rbac\n\n## Prerequisites\n- Supabase Enterprise tier subscription\n- Identity Provider (IdP) with SAML/OIDC support\n- Understanding of role-based access patterns\n- Audit logging infrastructure\n\n## Instructions\n\n### Step 1: Define Roles\nMap organizational roles to Supabase permissions.\n\n### Step 2: Configure SSO\nSet up SAML or OIDC integration with your IdP.\n\n### Step 3: Implement Middleware\nAdd permission checks to API endpoints.\n\n### Step 4: Enable Audit Logging\nTrack all access for compliance.\n\n## Output\n- Role definitions implemented\n- SSO integration configured\n- Permission middleware active\n- Audit trail enabled\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase Enterprise Guide](https://supabase.com/docs/enterprise)\n- [SAML 2.0 Specification](https://wiki.oasis-open.org/security/FrontPage)\n- [OpenID Connect Spec](https://openid.net/specs/openid-connect-core-1_0.html)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-enterprise-rbac/SKILL.md"
    },
    {
      "slug": "supabase-hello-world",
      "name": "supabase-hello-world",
      "description": "Create a minimal working Supabase example. Use when starting a new Supabase integration, testing your setup, or learning basic Supabase API patterns. Trigger with phrases like \"supabase hello world\", \"supabase example\", \"supabase quick start\", \"simple supabase code\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Hello World\n\n## Overview\nMinimal working example demonstrating core Supabase functionality.\n\n## Prerequisites\n- Completed `supabase-install-auth` setup\n- Valid API credentials configured\n- Development environment ready\n\n## Instructions\n\n### Step 1: Create Entry File\nCreate a new file for your hello world example.\n\n### Step 2: Import and Initialize Client\n```typescript\nimport { SupabaseClient } from '@supabase/supabase-js';\n\nconst client = new SupabaseClient({\n  apiKey: process.env.SUPABASE_API_KEY,\n});\n```\n\n### Step 3: Make Your First API Call\n```typescript\nasync function main() {\n  const result = await supabase.from('todos').insert({ task: 'Hello!' }).select(); console.log(result.data);\n}\n\nmain().catch(console.error);\n```\n\n## Output\n- Working code file with Supabase client initialization\n- Successful API response confirming connection\n- Console output showing:\n```\nSuccess! Your Supabase connection is working.\n```\n\n## Error Handling\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Import Error | SDK not installed | Verify with `npm list` or `pip show` |\n| Auth Error | Invalid credentials | Check environment variable is set |\n| Timeout | Network issues | Increase timeout or check connectivity |\n| Rate Limit | Too many requests | Wait and retry with exponential backoff |\n\n## Examples\n\n### TypeScript Example\n```typescript\nimport { SupabaseClient } from '@supabase/supabase-js';\n\nconst client = new SupabaseClient({\n  apiKey: process.env.SUPABASE_API_KEY,\n});\n\nasync function main() {\n  const result = await supabase.from('todos').insert({ task: 'Hello!' }).select(); console.log(result.data);\n}\n\nmain().catch(console.error);\n```\n\n### Python Example\n```python\nfrom supabase import SupabaseClient\n\nclient = SupabaseClient()\n\nresponse = supabase.table('todos').insert({'task': 'Hello!'}).execute(); print(response.data)\n```\n\n## Resources\n- [Supabase Getting Started](https://supabase.com/docs/getting-started)\n- [Supabase API Reference](https://supabase.com/docs/api)\n- [Supabase Examples](https://supabase.com/docs/examples)\n\n## Next Steps\nProceed to `supabase-local-dev-loop` for development workflow setup.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-hello-world/SKILL.md"
    },
    {
      "slug": "supabase-incident-runbook",
      "name": "supabase-incident-runbook",
      "description": "Execute Supabase incident response procedures with triage, mitigation, and postmortem. Use when responding to Supabase-related outages, investigating errors, or running post-incident reviews for Supabase integration failures. Trigger with phrases like \"supabase incident\", \"supabase outage\", \"supabase down\", \"supabase on-call\", \"supabase emergency\", \"supabase broken\". allowed-tools: Read, Grep, Bash(kubectl:*), Bash(curl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Incident Runbook\n\n## Prerequisites\n- Access to Supabase dashboard and status page\n- kubectl access to production cluster\n- Prometheus/Grafana access\n- Communication channels (Slack, PagerDuty)\n\n## Instructions\n\n### Step 1: Quick Triage\nRun the triage commands to identify the issue source.\n\n### Step 2: Follow Decision Tree\nDetermine if the issue is Supabase-side or internal.\n\n### Step 3: Execute Immediate Actions\nApply the appropriate remediation for the error type.\n\n### Step 4: Communicate Status\nUpdate internal and external stakeholders.\n\n## Output\n- Issue identified and categorized\n- Remediation applied\n- Stakeholders notified\n- Evidence collected for postmortem\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase Status Page](https://status.supabase.com)\n- [Supabase Support](https://support.supabase.com)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-incident-runbook/SKILL.md"
    },
    {
      "slug": "supabase-install-auth",
      "name": "supabase-install-auth",
      "description": "Install and configure Supabase SDK/CLI authentication. Use when setting up a new Supabase integration, configuring API keys, or initializing Supabase in your project. Trigger with phrases like \"install supabase\", \"setup supabase\", \"supabase auth\", \"configure supabase API key\". allowed-tools: Read, Write, Edit, Bash(npm:*), Bash(pip:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Install & Auth\n\n## Overview\nSet up Supabase SDK/CLI and configure authentication credentials.\n\n## Prerequisites\n- Node.js 18+ or Python 3.10+\n- Package manager (npm, pnpm, or pip)\n- Supabase account with API access\n- API key from Supabase dashboard\n\n## Instructions\n\n### Step 1: Install SDK\n```bash\n# Node.js\nnpm install @supabase/supabase-js\n\n# Python\npip install supabase\n```\n\n### Step 2: Configure Authentication\n```bash\n# Set environment variable\nexport SUPABASE_API_KEY=\"your-api-key\"\n\n# Or create .env file\necho 'SUPABASE_API_KEY=your-api-key' >> .env\n```\n\n### Step 3: Verify Connection\n```typescript\nconst result = await supabase.from('_test').select('*').limit(1); console.log(result.error ? 'Failed' : 'OK');\n```\n\n## Output\n- Installed SDK package in node_modules or site-packages\n- Environment variable or .env file with API key\n- Successful connection verification output\n\n## Error Handling\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Invalid API Key | Incorrect or expired key | Verify key in Supabase dashboard |\n| Rate Limited | Exceeded quota | Check quota at https://supabase.com/docs |\n| Network Error | Firewall blocking | Ensure outbound HTTPS allowed |\n| Module Not Found | Installation failed | Run `npm install` or `pip install` again |\n\n## Examples\n\n### TypeScript Setup\n```typescript\nimport { SupabaseClient } from '@supabase/supabase-js';\n\nconst client = new SupabaseClient({\n  apiKey: process.env.SUPABASE_API_KEY,\n});\n```\n\n### Python Setup\n```python\nfrom supabase import SupabaseClient\n\nclient = SupabaseClient(\n    api_key=os.environ.get('SUPABASE_API_KEY')\n)\n```\n\n## Resources\n- [Supabase Documentation](https://supabase.com/docs)\n- [Supabase Dashboard](https://api.supabase.com)\n- [Supabase Status](https://status.supabase.com)\n\n## Next Steps\nAfter successful auth, proceed to `supabase-hello-world` for your first API call.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-install-auth/SKILL.md"
    },
    {
      "slug": "supabase-known-pitfalls",
      "name": "supabase-known-pitfalls",
      "description": "Execute identify and avoid Supabase anti-patterns and common integration mistakes. Use when reviewing Supabase code for issues, onboarding new developers, or auditing existing Supabase integrations for best practices violations. Trigger with phrases like \"supabase mistakes\", \"supabase anti-patterns\", \"supabase pitfalls\", \"supabase what not to do\", \"supabase code review\". allowed-tools: Read, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Known Pitfalls\n\n## Prerequisites\n- Access to Supabase codebase for review\n- Understanding of async/await patterns\n- Knowledge of security best practices\n- Familiarity with rate limiting concepts\n\n## Instructions\n\n### Step 1: Review for Anti-Patterns\nScan codebase for each pitfall pattern.\n\n### Step 2: Prioritize Fixes\nAddress security issues first, then performance.\n\n### Step 3: Implement Better Approach\nReplace anti-patterns with recommended patterns.\n\n### Step 4: Add Prevention\nSet up linting and CI checks to prevent recurrence.\n\n## Output\n- Anti-patterns identified\n- Fixes prioritized and implemented\n- Prevention measures in place\n- Code quality improved\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase Security Guide](https://supabase.com/docs/security)\n- [Supabase Best Practices](https://supabase.com/docs/best-practices)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-known-pitfalls/SKILL.md"
    },
    {
      "slug": "supabase-load-scale",
      "name": "supabase-load-scale",
      "description": "Implement Supabase load testing, auto-scaling, and capacity planning strategies. Use when running performance tests, configuring horizontal scaling, or planning capacity for Supabase integrations. Trigger with phrases like \"supabase load test\", \"supabase scale\", \"supabase performance test\", \"supabase capacity\", \"supabase k6\", \"supabase benchmark\". allowed-tools: Read, Write, Edit, Bash(k6:*), Bash(kubectl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Load Scale\n\n## Prerequisites\n- k6 load testing tool installed\n- Kubernetes cluster with HPA configured\n- Prometheus for metrics collection\n- Test environment API keys\n\n## Instructions\n\n### Step 1: Create Load Test Script\nWrite k6 test script with appropriate thresholds.\n\n### Step 2: Configure Auto-Scaling\nSet up HPA with CPU and custom metrics.\n\n### Step 3: Run Load Test\nExecute test and collect metrics.\n\n### Step 4: Analyze and Document\nRecord results in benchmark template.\n\n## Output\n- Load test script created\n- HPA configured\n- Benchmark results documented\n- Capacity recommendations defined\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [k6 Documentation](https://k6.io/docs/)\n- [Kubernetes HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)\n- [Supabase Rate Limits](https://supabase.com/docs/rate-limits)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-load-scale/SKILL.md"
    },
    {
      "slug": "supabase-local-dev-loop",
      "name": "supabase-local-dev-loop",
      "description": "Configure Supabase local development with hot reload and testing. Use when setting up a development environment, configuring test workflows, or establishing a fast iteration cycle with Supabase. Trigger with phrases like \"supabase dev setup\", \"supabase local development\", \"supabase dev environment\", \"develop with supabase\". allowed-tools: Read, Write, Edit, Bash(npm:*), Bash(pnpm:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Local Dev Loop\n\n## Prerequisites\n- Completed `supabase-install-auth` setup\n- Node.js 18+ with npm/pnpm\n- Code editor with TypeScript support\n- Git for version control\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n- Working development environment with hot reload\n- Configured test suite with mocking\n- Environment variable management\n- Fast iteration cycle for Supabase development\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase SDK Reference](https://supabase.com/docs/sdk)\n- [Vitest Documentation](https://vitest.dev/)\n- [tsx Documentation](https://github.com/esbuild-kit/tsx)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-local-dev-loop/SKILL.md"
    },
    {
      "slug": "supabase-migration-deep-dive",
      "name": "supabase-migration-deep-dive",
      "description": "Execute Supabase major re-architecture and migration strategies with strangler fig pattern. Use when migrating to or from Supabase, performing major version upgrades, or re-platforming existing integrations to Supabase. Trigger with phrases like \"migrate supabase\", \"supabase migration\", \"switch to supabase\", \"supabase replatform\", \"supabase upgrade major\". allowed-tools: Read, Write, Edit, Bash(npm:*), Bash(node:*), Bash(kubectl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Migration Deep Dive\n\n## Prerequisites\n- Current system documentation\n- Supabase SDK installed\n- Feature flag infrastructure\n- Rollback strategy tested\n\n## Instructions\n\n### Step 1: Assess Current State\nDocument existing implementation and data inventory.\n\n### Step 2: Build Adapter Layer\nCreate abstraction layer for gradual migration.\n\n### Step 3: Migrate Data\nRun batch data migration with error handling.\n\n### Step 4: Shift Traffic\nGradually route traffic to new Supabase integration.\n\n## Output\n- Migration assessment complete\n- Adapter layer implemented\n- Data migrated successfully\n- Traffic fully shifted to Supabase\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Strangler Fig Pattern](https://martinfowler.com/bliki/StranglerFigApplication.html)\n- [Supabase Migration Guide](https://supabase.com/docs/migration)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-migration-deep-dive/SKILL.md"
    },
    {
      "slug": "supabase-multi-env-setup",
      "name": "supabase-multi-env-setup",
      "description": "Configure Supabase across development, staging, and production environments. Use when setting up multi-environment deployments, configuring per-environment secrets, or implementing environment-specific Supabase configurations. Trigger with phrases like \"supabase environments\", \"supabase staging\", \"supabase dev prod\", \"supabase environment setup\", \"supabase config by env\". allowed-tools: Read, Write, Edit, Bash(aws:*), Bash(gcloud:*), Bash(vault:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Multi Env Setup\n\n## Prerequisites\n- Separate Supabase accounts or API keys per environment\n- Secret management solution (Vault, AWS Secrets Manager, etc.)\n- CI/CD pipeline with environment variables\n- Environment detection in application\n\n## Instructions\n\n### Step 1: Create Config Structure\nSet up the base and per-environment configuration files.\n\n### Step 2: Implement Environment Detection\nAdd logic to detect and load environment-specific config.\n\n### Step 3: Configure Secrets\nStore API keys securely using your secret management solution.\n\n### Step 4: Add Environment Guards\nImplement safeguards for production-only operations.\n\n## Output\n- Multi-environment config structure\n- Environment detection logic\n- Secure secret management\n- Production safeguards enabled\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase Environments Guide](https://supabase.com/docs/environments)\n- [12-Factor App Config](https://12factor.net/config)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-multi-env-setup/SKILL.md"
    },
    {
      "slug": "supabase-observability",
      "name": "supabase-observability",
      "description": "Execute set up comprehensive observability for Supabase integrations with metrics, traces, and alerts. Use when implementing monitoring for Supabase operations, setting up dashboards, or configuring alerting for Supabase integration health. Trigger with phrases like \"supabase monitoring\", \"supabase metrics\", \"supabase observability\", \"monitor supabase\", \"supabase alerts\", \"supabase tracing\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Observability\n\n## Prerequisites\n- Prometheus or compatible metrics backend\n- OpenTelemetry SDK installed\n- Grafana or similar dashboarding tool\n- AlertManager configured\n\n## Instructions\n\n### Step 1: Set Up Metrics Collection\nImplement Prometheus counters, histograms, and gauges for key operations.\n\n### Step 2: Add Distributed Tracing\nIntegrate OpenTelemetry for end-to-end request tracing.\n\n### Step 3: Configure Structured Logging\nSet up JSON logging with consistent field names.\n\n### Step 4: Create Alert Rules\nDefine Prometheus alerting rules for error rates and latency.\n\n## Output\n- Metrics collection enabled\n- Distributed tracing configured\n- Structured logging implemented\n- Alert rules deployed\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Prometheus Best Practices](https://prometheus.io/docs/practices/naming/)\n- [OpenTelemetry Documentation](https://opentelemetry.io/docs/)\n- [Supabase Observability Guide](https://supabase.com/docs/observability)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-observability/SKILL.md"
    },
    {
      "slug": "supabase-performance-tuning",
      "name": "supabase-performance-tuning",
      "description": "Optimize Supabase API performance with caching, batching, and connection pooling. Use when experiencing slow API responses, implementing caching strategies, or optimizing request throughput for Supabase integrations. Trigger with phrases like \"supabase performance\", \"optimize supabase\", \"supabase latency\", \"supabase caching\", \"supabase slow\", \"supabase batch\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Performance Tuning\n\n## Prerequisites\n- Supabase SDK installed\n- Understanding of async patterns\n- Redis or in-memory cache available (optional)\n- Performance monitoring in place\n\n## Instructions\n\n### Step 1: Establish Baseline\nMeasure current latency for critical Supabase operations.\n\n### Step 2: Implement Caching\nAdd response caching for frequently accessed data.\n\n### Step 3: Enable Batching\nUse DataLoader or similar for automatic request batching.\n\n### Step 4: Optimize Connections\nConfigure connection pooling with keep-alive.\n\n## Output\n- Reduced API latency\n- Caching layer implemented\n- Request batching enabled\n- Connection pooling configured\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase Performance Guide](https://supabase.com/docs/performance)\n- [DataLoader Documentation](https://github.com/graphql/dataloader)\n- [LRU Cache Documentation](https://github.com/isaacs/node-lru-cache)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-performance-tuning/SKILL.md"
    },
    {
      "slug": "supabase-policy-guardrails",
      "name": "supabase-policy-guardrails",
      "description": "Implement Supabase lint rules, policy enforcement, and automated guardrails. Use when setting up code quality rules for Supabase integrations, implementing pre-commit hooks, or configuring CI policy checks for Supabase best practices. Trigger with phrases like \"supabase policy\", \"supabase lint\", \"supabase guardrails\", \"supabase best practices check\", \"supabase eslint\". allowed-tools: Read, Write, Edit, Bash(npx:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Policy Guardrails\n\n## Prerequisites\n- ESLint configured in project\n- Pre-commit hooks infrastructure\n- CI/CD pipeline with policy checks\n- TypeScript for type enforcement\n\n## Instructions\n\n### Step 1: Create ESLint Rules\nImplement custom lint rules for Supabase patterns.\n\n### Step 2: Configure Pre-Commit Hooks\nSet up hooks to catch issues before commit.\n\n### Step 3: Add CI Policy Checks\nImplement policy-as-code in CI pipeline.\n\n### Step 4: Enable Runtime Guardrails\nAdd production safeguards for dangerous operations.\n\n## Output\n- ESLint plugin with Supabase rules\n- Pre-commit hooks blocking secrets\n- CI policy checks passing\n- Runtime guardrails active\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [ESLint Plugin Development](https://eslint.org/docs/latest/extend/plugins)\n- [Pre-commit Framework](https://pre-commit.com/)\n- [Open Policy Agent](https://www.openpolicyagent.org/)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-policy-guardrails/SKILL.md"
    },
    {
      "slug": "supabase-prod-checklist",
      "name": "supabase-prod-checklist",
      "description": "Execute Supabase production deployment checklist and rollback procedures. Use when deploying Supabase integrations to production, preparing for launch, or implementing go-live procedures. Trigger with phrases like \"supabase production\", \"deploy supabase\", \"supabase go-live\", \"supabase launch checklist\". allowed-tools: Read, Bash(kubectl:*), Bash(curl:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Prod Checklist\n\n## Prerequisites\n- Staging environment tested and verified\n- Production API keys available\n- Deployment pipeline configured\n- Monitoring and alerting ready\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n- Deployed Supabase integration\n- Health checks passing\n- Monitoring active\n- Rollback procedure documented\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase Status](https://status.supabase.com)\n- [Supabase Support](https://supabase.com/docs/support)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-prod-checklist/SKILL.md"
    },
    {
      "slug": "supabase-rate-limits",
      "name": "supabase-rate-limits",
      "description": "Implement Supabase rate limiting, backoff, and idempotency patterns. Use when handling rate limit errors, implementing retry logic, or optimizing API request throughput for Supabase. Trigger with phrases like \"supabase rate limit\", \"supabase throttling\", \"supabase 429\", \"supabase retry\", \"supabase backoff\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Rate Limits\n\n## Prerequisites\n- Supabase SDK installed\n- Understanding of async/await patterns\n- Access to rate limit headers\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n- Reliable API calls with automatic retry\n- Idempotent requests preventing duplicates\n- Rate limit headers properly handled\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase Rate Limits](https://supabase.com/docs/rate-limits)\n- [p-queue Documentation](https://github.com/sindresorhus/p-queue)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-rate-limits/SKILL.md"
    },
    {
      "slug": "supabase-reference-architecture",
      "name": "supabase-reference-architecture",
      "description": "Implement Supabase reference architecture with best-practice project layout. Use when designing new Supabase integrations, reviewing project structure, or establishing architecture standards for Supabase applications. Trigger with phrases like \"supabase architecture\", \"supabase best practices\", \"supabase project structure\", \"how to organize supabase\", \"supabase layout\". allowed-tools: Read, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Reference Architecture\n\n## Prerequisites\n- Understanding of layered architecture\n- Supabase SDK knowledge\n- TypeScript project setup\n- Testing framework configured\n\n## Instructions\n\n### Step 1: Create Directory Structure\nSet up the project layout following the reference structure above.\n\n### Step 2: Implement Client Wrapper\nCreate the singleton client with caching and monitoring.\n\n### Step 3: Add Error Handling\nImplement custom error classes for Supabase operations.\n\n### Step 4: Configure Health Checks\nAdd health check endpoint for Supabase connectivity.\n\n## Output\n- Structured project layout\n- Client wrapper with caching\n- Error boundary implemented\n- Health checks configured\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase SDK Documentation](https://supabase.com/docs/sdk)\n- [Supabase Best Practices](https://supabase.com/docs/best-practices)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-reference-architecture/SKILL.md"
    },
    {
      "slug": "supabase-reliability-patterns",
      "name": "supabase-reliability-patterns",
      "description": "Implement Supabase reliability patterns including circuit breakers, idempotency, and graceful degradation. Use when building fault-tolerant Supabase integrations, implementing retry strategies, or adding resilience to production Supabase services. Trigger with phrases like \"supabase reliability\", \"supabase circuit breaker\", \"supabase idempotent\", \"supabase resilience\", \"supabase fallback\", \"supabase bulkhead\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Reliability Patterns\n\n## Prerequisites\n- Understanding of circuit breaker pattern\n- opossum or similar library installed\n- Queue infrastructure for DLQ\n- Caching layer for fallbacks\n\n## Instructions\n\n### Step 1: Implement Circuit Breaker\nWrap Supabase calls with circuit breaker.\n\n### Step 2: Add Idempotency Keys\nGenerate deterministic keys for operations.\n\n### Step 3: Configure Bulkheads\nSeparate queues for different priorities.\n\n### Step 4: Set Up Dead Letter Queue\nHandle permanent failures gracefully.\n\n## Output\n- Circuit breaker protecting Supabase calls\n- Idempotency preventing duplicates\n- Bulkhead isolation implemented\n- DLQ for failed operations\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Circuit Breaker Pattern](https://martinfowler.com/bliki/CircuitBreaker.html)\n- [Opossum Documentation](https://nodeshift.dev/opossum/)\n- [Supabase Reliability Guide](https://supabase.com/docs/reliability)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-reliability-patterns/SKILL.md"
    },
    {
      "slug": "supabase-schema-from-requirements",
      "name": "supabase-schema-from-requirements",
      "description": "Execute Supabase primary workflow: Schema from Requirements. Use when Starting a new project with defined data requirements, Refactoring an existing schema based on new features, or Creating migrations from specification documents. Trigger with phrases like \"supabase schema from requirements\", \"generate database schema with supabase\". allowed-tools: Read, Write, Edit, Bash(npm:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Schema from Requirements\n\n## Overview\nGenerate Supabase database schema from natural language requirements.\nThis is the primary workflow for starting new Supabase projects.\n\n\n## Prerequisites\n- Completed `supabase-install-auth` setup\n- Understanding of Supabase core concepts\n- Valid API credentials configured\n\n## Instructions\n\n### Step 1: Initialize\n```typescript\n// Step 1 implementation\n```\n\n### Step 2: Execute\n```typescript\n// Step 2 implementation\n```\n\n### Step 3: Finalize\n```typescript\n// Step 3 implementation\n```\n\n## Output\n- Completed Schema from Requirements execution\n- Expected results from Supabase API\n- Success confirmation or error details\n\n## Error Handling\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Error 1 | Cause | Solution |\n| Error 2 | Cause | Solution |\n\n## Examples\n\n### Complete Workflow\n```typescript\n// Complete workflow example\n```\n\n### Common Variations\n- Variation 1: Description\n- Variation 2: Description\n\n## Resources\n- [Supabase Documentation](https://supabase.com/docs)\n- [Supabase API Reference](https://supabase.com/docs/api)\n\n## Next Steps\nFor secondary workflow, see `supabase-auth-storage-realtime-core`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-schema-from-requirements/SKILL.md"
    },
    {
      "slug": "supabase-sdk-patterns",
      "name": "supabase-sdk-patterns",
      "description": "Execute apply production-ready Supabase SDK patterns for TypeScript and Python. Use when implementing Supabase integrations, refactoring SDK usage, or establishing team coding standards for Supabase. Trigger with phrases like \"supabase SDK patterns\", \"supabase best practices\", \"supabase code patterns\", \"idiomatic supabase\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Sdk Patterns\n\n## Prerequisites\n- Completed `supabase-install-auth` setup\n- Familiarity with async/await patterns\n- Understanding of error handling best practices\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n- Type-safe client singleton\n- Robust error handling with structured logging\n- Automatic retry with exponential backoff\n- Runtime validation for API responses\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase SDK Reference](https://supabase.com/docs/sdk)\n- [Supabase API Types](https://supabase.com/docs/types)\n- [Zod Documentation](https://zod.dev/)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-sdk-patterns/SKILL.md"
    },
    {
      "slug": "supabase-security-basics",
      "name": "supabase-security-basics",
      "description": "Execute apply Supabase security best practices for secrets and access control. Use when securing API keys, implementing least privilege access, or auditing Supabase security configuration. Trigger with phrases like \"supabase security\", \"supabase secrets\", \"secure supabase\", \"supabase API key security\". allowed-tools: Read, Write, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Security Basics\n\n## Prerequisites\n- Supabase SDK installed\n- Understanding of environment variables\n- Access to Supabase dashboard\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n- Secure API key storage\n- Environment-specific access controls\n- Audit logging enabled\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase Security Guide](https://supabase.com/docs/security)\n- [Supabase API Scopes](https://supabase.com/docs/scopes)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-security-basics/SKILL.md"
    },
    {
      "slug": "supabase-upgrade-migration",
      "name": "supabase-upgrade-migration",
      "description": "Execute analyze, plan, and execute Supabase SDK upgrades with breaking change detection. Use when upgrading Supabase SDK versions, detecting deprecations, or migrating to new API versions. Trigger with phrases like \"upgrade supabase\", \"supabase migration\", \"supabase breaking changes\", \"update supabase SDK\", \"analyze supabase version\". allowed-tools: Read, Write, Edit, Bash(npm:*), Bash(git:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Upgrade Migration\n\n## Prerequisites\n- Current Supabase SDK installed\n- Git for version control\n- Test suite available\n- Staging environment\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n- Updated SDK version\n- Fixed breaking changes\n- Passing test suite\n- Documented rollback procedure\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase Changelog](https://github.com/supabase/sdk/releases)\n- [Supabase Migration Guide](https://supabase.com/docs/migration)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-upgrade-migration/SKILL.md"
    },
    {
      "slug": "supabase-webhooks-events",
      "name": "supabase-webhooks-events",
      "description": "Implement Supabase webhook signature validation and event handling. Use when setting up webhook endpoints, implementing signature verification, or handling Supabase event notifications securely. Trigger with phrases like \"supabase webhook\", \"supabase events\", \"supabase webhook signature\", \"handle supabase events\", \"supabase notifications\". allowed-tools: Read, Write, Edit, Bash(curl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Webhooks Events\n\n## Prerequisites\n- Supabase webhook secret configured\n- HTTPS endpoint accessible from internet\n- Understanding of cryptographic signatures\n- Redis or database for idempotency (optional)\n\n## Instructions\n\n### Step 1: Register Webhook Endpoint\nConfigure your webhook URL in the Supabase dashboard.\n\n### Step 2: Implement Signature Verification\nUse the signature verification code to validate incoming webhooks.\n\n### Step 3: Handle Events\nImplement handlers for each event type your application needs.\n\n### Step 4: Add Idempotency\nPrevent duplicate processing with event ID tracking.\n\n## Output\n- Secure webhook endpoint\n- Signature validation enabled\n- Event handlers implemented\n- Replay attack protection active\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Supabase Webhooks Guide](https://supabase.com/docs/webhooks)\n- [Webhook Security Best Practices](https://supabase.com/docs/webhooks/security)",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-webhooks-events/SKILL.md"
    },
    {
      "slug": "testing-browser-compatibility",
      "name": "testing-browser-compatibility",
      "description": "Test across multiple browsers and devices for cross-browser compatibility. Use when ensuring cross-browser or device compatibility. Trigger with phrases like \"test browser compatibility\", \"check cross-browser\", or \"validate on browsers\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:browser-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Browser Compatibility Tester\n\nThis skill provides automated assistance for browser compatibility tester tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:browser-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for browser compatibility tester tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "browser-compatibility-tester",
        "category": "testing",
        "path": "plugins/testing/browser-compatibility-tester",
        "version": "1.0.0",
        "description": "Cross-browser testing with BrowserStack, Selenium Grid, and Playwright - test across Chrome, Firefox, Safari, Edge"
      },
      "filePath": "plugins/testing/browser-compatibility-tester/skills/testing-browser-compatibility/SKILL.md"
    },
    {
      "slug": "testing-load-balancers",
      "name": "testing-load-balancers",
      "description": "Validate load balancer behavior, failover, and traffic distribution. Use when performing specialized testing. Trigger with phrases like \"test load balancer\", \"validate failover\", or \"check traffic distribution\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:loadbalancer-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Load Balancer Tester\n\nThis skill provides automated assistance for load balancer tester tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:loadbalancer-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for load balancer tester tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "load-balancer-tester",
        "category": "testing",
        "path": "plugins/testing/load-balancer-tester",
        "version": "1.0.0",
        "description": "Test load balancing strategies with traffic distribution validation and failover testing"
      },
      "filePath": "plugins/testing/load-balancer-tester/skills/testing-load-balancers/SKILL.md"
    },
    {
      "slug": "testing-mobile-apps",
      "name": "testing-mobile-apps",
      "description": "Execute mobile app testing on iOS and Android devices/simulators. Use when performing specialized testing. Trigger with phrases like \"test mobile app\", \"run iOS tests\", or \"validate Android functionality\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:mobile-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Mobile App Tester\n\nThis skill provides automated assistance for mobile app tester tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:mobile-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for mobile app tester tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "mobile-app-tester",
        "category": "testing",
        "path": "plugins/testing/mobile-app-tester",
        "version": "1.0.0",
        "description": "Mobile app test automation with Appium, Detox, XCUITest - test iOS and Android apps"
      },
      "filePath": "plugins/testing/mobile-app-tester/skills/testing-mobile-apps/SKILL.md"
    },
    {
      "slug": "testing-visual-regression",
      "name": "testing-visual-regression",
      "description": "Detect visual changes in UI components using screenshot comparison. Use when detecting unintended UI changes or pixel differences. Trigger with phrases like \"test visual changes\", \"compare screenshots\", or \"detect UI regressions\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:visual-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Visual Regression Tester\n\nThis skill provides automated assistance for visual regression tester tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:visual-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for visual regression tester tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "visual-regression-tester",
        "category": "testing",
        "path": "plugins/testing/visual-regression-tester",
        "version": "1.0.0",
        "description": "Visual diff testing with Percy, Chromatic, BackstopJS - catch unintended UI changes"
      },
      "filePath": "plugins/testing/visual-regression-tester/skills/testing-visual-regression/SKILL.md"
    },
    {
      "slug": "throttling-apis",
      "name": "throttling-apis",
      "description": "Implement API throttling policies to protect backend services from overload. Use when controlling API request rates. Trigger with phrases like \"throttle API\", \"control request rate\", or \"add throttling\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:throttle-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Throttling Apis\n\n## Overview\n\n\nThis skill provides automated assistance for api throttling manager tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:throttle-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-throttling-manager",
        "category": "api-development",
        "path": "plugins/api-development/api-throttling-manager",
        "version": "1.0.0",
        "description": "Manage API throttling with dynamic rate limits and quota management"
      },
      "filePath": "plugins/api-development/api-throttling-manager/skills/throttling-apis/SKILL.md"
    },
    {
      "slug": "tracking-application-response-times",
      "name": "tracking-application-response-times",
      "description": "Track and optimize application response times across API endpoints, database queries, and service calls. Use when monitoring performance or identifying bottlenecks. Trigger with phrases like \"track response times\", \"monitor API performance\", or \"analyze latency\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(monitoring:*)",
        "Bash(metrics:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Response Time Tracker\n\nThis skill provides automated assistance for response time tracker tasks.\n\n## Overview\n\nThis skill empowers Claude to proactively monitor and improve application performance by tracking response times across various layers. It provides detailed metrics and insights to identify and resolve performance bottlenecks.\n\n## How It Works\n\n1. **Initiate Tracking**: The user requests response time tracking.\n2. **Configure Monitoring**: The plugin automatically begins monitoring API endpoints, database queries, external service calls, frontend rendering, and background jobs.\n3. **Report Metrics**: The plugin generates reports including P50, P95, P99 percentiles, average, and maximum response times.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify performance bottlenecks in your application.\n- Monitor service level objectives (SLOs) related to response times.\n- Receive alerts about performance degradation.\n\n## Examples\n\n### Example 1: Diagnosing Slow API Endpoint\n\nUser request: \"Track response times for the user authentication API endpoint.\"\n\nThe skill will:\n1. Activate the response-time-tracker plugin.\n2. Monitor the specified API endpoint and report response time metrics, highlighting potential bottlenecks.\n\n### Example 2: Monitoring Database Query Performance\n\nUser request: \"Monitor database query performance for the product catalog.\"\n\nThe skill will:\n1. Activate the response-time-tracker plugin.\n2. Track the execution time of database queries related to the product catalog and provide performance insights.\n\n## Best Practices\n\n- **Granularity**: Track response times at a granular level (e.g., individual API endpoints, specific database queries) for more precise insights.\n- **Alerting**: Configure alerts for significant deviations from baseline performance to proactively address potential issues.\n- **Contextualization**: Correlate response time data with other metrics (e.g., CPU usage, memory consumption) to identify root causes.\n\n## Integration\n\nThis skill can be integrated with other monitoring and alerting tools to provide a comprehensive view of application performance. It can also be used in conjunction with optimization tools to automatically address identified bottlenecks.\n\n## Prerequisites\n\n- Access to application monitoring infrastructure\n- Response time data collection in {baseDir}/metrics/response-times/\n- APM tools or custom instrumentation\n- Performance SLO definitions\n\n## Instructions\n\n1. Configure monitoring for API endpoints and database queries\n2. Collect response time metrics (P50, P95, P99 percentiles)\n3. Analyze trends and identify performance degradation\n4. Compare against performance baselines and SLOs\n5. Identify bottlenecks and root causes\n6. Generate optimization recommendations\n\n## Output\n\n- Response time reports with percentile metrics\n- Performance trend visualizations\n- Bottleneck identification analysis\n- SLO compliance status\n- Optimization recommendations with priorities\n\n## Error Handling\n\nIf response time tracking fails:\n- Verify monitoring agent installation\n- Check instrumentation configuration\n- Validate metric export endpoints\n- Ensure data storage availability\n- Review sampling configuration\n\n## Resources\n\n- APM tool documentation\n- Response time monitoring best practices\n- Percentile-based SLO definitions\n- Performance optimization guides",
      "parentPlugin": {
        "name": "response-time-tracker",
        "category": "performance",
        "path": "plugins/performance/response-time-tracker",
        "version": "1.0.0",
        "description": "Track and optimize application response times"
      },
      "filePath": "plugins/performance/response-time-tracker/skills/tracking-application-response-times/SKILL.md"
    },
    {
      "slug": "tracking-crypto-derivatives",
      "name": "tracking-crypto-derivatives",
      "description": "Track futures, options, and perpetual swap positions with P&L calculations. Use when tracking futures and options positions. Trigger with phrases like \"track derivatives\", \"check futures positions\", or \"analyze perps\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:derivatives-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Tracking Crypto Derivatives\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:derivatives-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "crypto-derivatives-tracker",
        "category": "crypto",
        "path": "plugins/crypto/crypto-derivatives-tracker",
        "version": "1.0.0",
        "description": "Track crypto futures, options, perpetual swaps with funding rates, open interest, and derivatives market analysis"
      },
      "filePath": "plugins/crypto/crypto-derivatives-tracker/skills/tracking-crypto-derivatives/SKILL.md"
    },
    {
      "slug": "tracking-crypto-portfolio",
      "name": "tracking-crypto-portfolio",
      "description": "Track multi-chain crypto portfolio with real-time valuations and performance metrics. Use when managing multi-chain crypto holdings. Trigger with phrases like \"track my portfolio\", \"check holdings\", or \"analyze positions\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:portfolio-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Tracking Crypto Portfolio\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:portfolio-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "crypto-portfolio-tracker",
        "category": "crypto",
        "path": "plugins/crypto/crypto-portfolio-tracker",
        "version": "1.0.0",
        "description": "Professional crypto portfolio tracking with real-time prices, PnL analysis, and risk metrics"
      },
      "filePath": "plugins/crypto/crypto-portfolio-tracker/skills/tracking-crypto-portfolio/SKILL.md"
    },
    {
      "slug": "tracking-crypto-prices",
      "name": "tracking-crypto-prices",
      "description": "Track real-time cryptocurrency prices across exchanges with historical data and alerts. Use when monitoring real-time cryptocurrency prices. Trigger with phrases like \"check price\", \"track prices\", or \"get price alert\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:price-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Tracking Crypto Prices\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:price-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "market-price-tracker",
        "category": "crypto",
        "path": "plugins/crypto/market-price-tracker",
        "version": "1.0.0",
        "description": "Real-time market price tracking with multi-exchange feeds and advanced alerts"
      },
      "filePath": "plugins/crypto/market-price-tracker/skills/tracking-crypto-prices/SKILL.md"
    },
    {
      "slug": "tracking-model-versions",
      "name": "tracking-model-versions",
      "description": "Build this skill enables AI assistant to track and manage ai/ml model versions using the model-versioning-tracker plugin. it should be used when the user asks to manage model versions, track model lineage, log model performance, or implement version control f... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Model Versioning Tracker\n\nThis skill provides automated assistance for model versioning tracker tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for model versioning tracker tasks.\nThis skill empowers Claude to interact with the model-versioning-tracker plugin, providing a streamlined approach to managing and tracking AI/ML model versions. It ensures that model development and deployment are conducted with proper version control, logging, and performance monitoring.\n\n## How It Works\n\n1. **Analyze Request**: Claude analyzes the user's request to determine the specific model versioning task.\n2. **Generate Code**: Claude generates the necessary code to interact with the model-versioning-tracker plugin.\n3. **Execute Task**: The plugin executes the code, performing the requested model versioning operation, such as tracking a new version or retrieving performance metrics.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Track new versions of AI/ML models.\n- Retrieve performance metrics for specific model versions.\n- Implement automated workflows for model versioning.\n\n## Examples\n\n### Example 1: Tracking a New Model Version\n\nUser request: \"Track a new version of my image classification model.\"\n\nThe skill will:\n1. Generate code to log the new model version and its associated metadata using the model-versioning-tracker plugin.\n2. Execute the code, creating a new entry in the model registry.\n\n### Example 2: Retrieving Performance Metrics\n\nUser request: \"Get the performance metrics for version 3 of my sentiment analysis model.\"\n\nThe skill will:\n1. Generate code to query the model-versioning-tracker plugin for the performance metrics associated with the specified model version.\n2. Execute the code and return the metrics to the user.\n\n## Best Practices\n\n- **Data Validation**: Ensure input data is validated before logging model versions.\n- **Error Handling**: Implement robust error handling to manage unexpected issues during version tracking.\n- **Performance Monitoring**: Continuously monitor model performance to identify opportunities for optimization.\n\n## Integration\n\nThis skill integrates with other Claude Code plugins by providing a centralized location for managing AI/ML model versions. It can be used in conjunction with plugins that handle data processing, model training, and deployment to ensure a seamless AI/ML workflow.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "model-versioning-tracker",
        "category": "ai-ml",
        "path": "plugins/ai-ml/model-versioning-tracker",
        "version": "1.0.0",
        "description": "Track and manage model versions"
      },
      "filePath": "plugins/ai-ml/model-versioning-tracker/skills/tracking-model-versions/SKILL.md"
    },
    {
      "slug": "tracking-regression-tests",
      "name": "tracking-regression-tests",
      "description": "Track and manage regression test suites across releases. Use when performing specialized testing. Trigger with phrases like \"track regressions\", \"manage regression suite\", or \"validate against baseline\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:regression-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Regression Test Tracker\n\nThis skill provides automated assistance for regression test tracker tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:regression-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for regression test tracker tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "regression-test-tracker",
        "category": "testing",
        "path": "plugins/testing/regression-test-tracker",
        "version": "1.0.0",
        "description": "Track and run regression tests to ensure new changes don't break existing functionality"
      },
      "filePath": "plugins/testing/regression-test-tracker/skills/tracking-regression-tests/SKILL.md"
    },
    {
      "slug": "tracking-resource-usage",
      "name": "tracking-resource-usage",
      "description": "Track and optimize resource usage across application stack including CPU, memory, disk, and network I/O. Use when identifying bottlenecks or optimizing costs. Trigger with phrases like \"track resource usage\", \"monitor CPU and memory\", or \"optimize resource allocation\".",
      "allowedTools": [
        "\"Read",
        "Bash(top:*)",
        "Bash(ps:*)",
        "Bash(vmstat:*)",
        "Bash(iostat:*)",
        "Grep",
        "Glob\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Resource Usage Tracker\n\nThis skill provides automated assistance for resource usage tracker tasks.\n\n## Overview\n\nThis skill provides a comprehensive solution for monitoring and optimizing resource usage within an application. It leverages the resource-usage-tracker plugin to gather real-time metrics, identify performance bottlenecks, and suggest optimization strategies.\n\n## How It Works\n\n1. **Identify Resources**: The skill identifies the resources to be tracked based on the user's request and the application's configuration (CPU, memory, disk I/O, network I/O, etc.).\n2. **Collect Metrics**: The plugin collects real-time metrics for the identified resources, providing a snapshot of current resource consumption.\n3. **Analyze Data**: The skill analyzes the collected data to identify performance bottlenecks, resource imbalances, and potential optimization opportunities.\n4. **Provide Recommendations**: Based on the analysis, the skill provides specific recommendations for optimizing resource allocation, right-sizing instances, and reducing costs.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify performance bottlenecks in an application.\n- Optimize resource allocation to improve efficiency.\n- Reduce cloud infrastructure costs by right-sizing instances.\n- Monitor resource usage in real-time to detect anomalies.\n- Track the impact of code changes on resource consumption.\n\n## Examples\n\n### Example 1: Identifying Memory Leaks\n\nUser request: \"Track memory usage and identify potential memory leaks.\"\n\nThe skill will:\n1. Activate the resource-usage-tracker plugin to monitor memory usage (heap, stack, RSS).\n2. Analyze the memory usage data over time to detect patterns indicative of memory leaks.\n3. Provide recommendations for identifying and resolving the memory leaks.\n\n### Example 2: Optimizing Database Connection Pool\n\nUser request: \"Optimize database connection pool utilization.\"\n\nThe skill will:\n1. Activate the resource-usage-tracker plugin to monitor database connection pool metrics.\n2. Analyze the connection pool utilization data to identify periods of high contention or underutilization.\n3. Provide recommendations for adjusting the connection pool size to optimize performance and resource consumption.\n\n## Best Practices\n\n- **Granularity**: Track resource usage at a granular level (e.g., process-level CPU usage) to identify specific bottlenecks.\n- **Historical Data**: Analyze historical resource usage data to identify trends and predict future resource needs.\n- **Alerting**: Configure alerts to notify you when resource usage exceeds predefined thresholds.\n\n## Integration\n\nThis skill can be integrated with other monitoring and alerting tools to provide a comprehensive view of application performance. It can also be used in conjunction with deployment automation tools to automatically right-size instances based on resource usage patterns.\n\n## Prerequisites\n\n- Access to system monitoring tools (top, ps, vmstat, iostat)\n- Resource metrics collection infrastructure\n- Historical usage data in {baseDir}/metrics/resources/\n- Performance baseline definitions\n\n## Instructions\n\n1. Identify resources to track (CPU, memory, disk, network)\n2. Collect real-time metrics using system tools\n3. Analyze data for bottlenecks and patterns\n4. Compare against historical baselines\n5. Generate optimization recommendations\n6. Provide right-sizing and cost reduction strategies\n\n## Output\n\n- Resource usage reports with trends\n- Bottleneck identification and analysis\n- Right-sizing recommendations for instances\n- Cost optimization suggestions\n- Alert configurations for thresholds\n\n## Error Handling\n\nIf resource tracking fails:\n- Verify system monitoring tool permissions\n- Check metrics collection daemon status\n- Validate data storage availability\n- Ensure network access to monitoring endpoints\n- Review baseline data completeness\n\n## Resources\n\n- System performance monitoring guides\n- Cloud resource optimization best practices\n- CPU and memory profiling techniques\n- Infrastructure cost optimization strategies",
      "parentPlugin": {
        "name": "resource-usage-tracker",
        "category": "performance",
        "path": "plugins/performance/resource-usage-tracker",
        "version": "1.0.0",
        "description": "Track and optimize resource usage across the stack"
      },
      "filePath": "plugins/performance/resource-usage-tracker/skills/tracking-resource-usage/SKILL.md"
    },
    {
      "slug": "tracking-service-reliability",
      "name": "tracking-service-reliability",
      "description": "Define and track SLAs, SLIs, and SLOs for service reliability including availability, latency, and error rates. Use when establishing reliability targets or monitoring service health. Trigger with phrases like \"define SLOs\", \"track SLI metrics\", or \"calculate error budget\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(monitoring:*)",
        "Bash(metrics:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Sla Sli Tracker\n\nThis skill provides automated assistance for sla sli tracker tasks.\n\n## Overview\n\nThis skill provides a structured approach to defining and tracking SLAs, SLIs, and SLOs, which are essential for ensuring service reliability. It automates the process of setting performance targets and monitoring actual performance, enabling proactive identification and resolution of potential issues.\n\n## How It Works\n\n1. **SLI Definition**: The skill guides the user to define Service Level Indicators (SLIs) such as availability, latency, error rate, and throughput.\n2. **SLO Target Setting**: The skill assists in setting Service Level Objectives (SLOs) by establishing target values for the defined SLIs (e.g., 99.9% availability).\n3. **SLA Establishment**: The skill helps in formalizing Service Level Agreements (SLAs), which are customer-facing commitments based on the defined SLOs.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Define SLAs, SLIs, and SLOs for a service.\n- Track service performance against defined objectives.\n- Calculate error budgets based on SLOs.\n\n## Examples\n\n### Example 1: Defining SLOs for a New Service\n\nUser request: \"Create SLOs for our new payment processing service.\"\n\nThe skill will:\n1. Prompt the user to define SLIs (e.g., latency, error rate).\n2. Assist in setting target values for each SLI (e.g., p99 latency < 100ms, error rate < 0.01%).\n\n### Example 2: Tracking Availability\n\nUser request: \"Track the availability SLI for the database service.\"\n\nThe skill will:\n1. Guide the user in setting up the tracking of the availability SLI.\n2. Visualize availability performance against the defined SLO.\n\n## Best Practices\n\n- **Granularity**: Define SLIs that are specific and measurable.\n- **Realism**: Set SLOs that are challenging but achievable.\n- **Alignment**: Ensure SLAs align with the defined SLOs and business requirements.\n\n## Integration\n\nThis skill can be integrated with monitoring tools to automatically collect SLI data and track performance against SLOs. It can also be used in conjunction with alerting systems to trigger notifications when SLO violations occur.\n\n## Prerequisites\n\n- SLI definitions stored in {baseDir}/slos/sli-definitions.yaml\n- Access to monitoring and metrics systems\n- Historical performance data for baseline\n- Business requirements for service reliability\n\n## Instructions\n\n1. Define Service Level Indicators (availability, latency, error rate, throughput)\n2. Set Service Level Objectives with target values (e.g., 99.9% availability)\n3. Formalize Service Level Agreements with customer commitments\n4. Configure automated SLI data collection\n5. Calculate error budgets based on SLOs\n6. Track performance and alert on SLO violations\n\n## Output\n\n- SLI/SLO/SLA definition documents\n- Real-time SLI metric dashboards\n- Error budget calculations and burn rate\n- SLO compliance reports\n- Alerting configurations for violations\n\n## Error Handling\n\nIf SLI/SLO tracking fails:\n- Verify SLI definition completeness\n- Check metric collection infrastructure\n- Validate data accuracy and granularity\n- Ensure alerting system connectivity\n- Review error budget calculation logic\n\n## Resources\n\n- Google SRE book on SLIs and SLOs\n- Error budget implementation guides\n- Service reliability engineering practices\n- SLO definition templates and examples",
      "parentPlugin": {
        "name": "sla-sli-tracker",
        "category": "performance",
        "path": "plugins/performance/sla-sli-tracker",
        "version": "1.0.0",
        "description": "Track SLAs, SLIs, and SLOs for service reliability"
      },
      "filePath": "plugins/performance/sla-sli-tracker/skills/tracking-service-reliability/SKILL.md"
    },
    {
      "slug": "tracking-token-launches",
      "name": "tracking-token-launches",
      "description": "Monitor new token launches, IDOs, and fair launches with contract verification. Use when discovering new token launches. Trigger with phrases like \"track launches\", \"find new tokens\", or \"monitor IDOs\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:launch-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Tracking Token Launches\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n1. Use Bash(crypto:launch-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n- Transaction count and network activity\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n- ethers.js for Ethereum smart contract interaction",
      "parentPlugin": {
        "name": "token-launch-tracker",
        "category": "crypto",
        "path": "plugins/crypto/token-launch-tracker",
        "version": "1.0.0",
        "description": "Track new token launches, detect rugpulls, and analyze contract security for early-stage crypto projects"
      },
      "filePath": "plugins/crypto/token-launch-tracker/skills/tracking-token-launches/SKILL.md"
    },
    {
      "slug": "training-machine-learning-models",
      "name": "training-machine-learning-models",
      "description": "Build train machine learning models with automated workflows. Analyzes datasets, selects model types (classification, regression), configures parameters, trains with cross-validation, and saves model artifacts. Use when asked to \"train model\" or \"evalua... Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Ml Model Trainer\n\nThis skill provides automated assistance for ml model trainer tasks.\n\n## Overview\n\nThis skill empowers Claude to automatically train and evaluate machine learning models. It streamlines the model development process by handling data analysis, model selection, training, and evaluation, ultimately providing a persisted model artifact.\n\n## How It Works\n\n1. **Data Analysis and Preparation**: The skill analyzes the provided dataset and identifies the target variable, determining the appropriate model type (classification, regression, etc.).\n2. **Model Selection and Training**: Based on the data analysis, the skill selects a suitable machine learning model and configures the training parameters. It then trains the model using cross-validation techniques.\n3. **Performance Evaluation and Persistence**: After training, the skill generates performance metrics to evaluate the model's effectiveness. Finally, it saves the trained model artifact for future use.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Train a machine learning model on a given dataset.\n- Evaluate the performance of a machine learning model.\n- Automate the machine learning model training process.\n\n## Examples\n\n### Example 1: Training a Classification Model\n\nUser request: \"Train a classification model on this dataset of customer churn data.\"\n\nThe skill will:\n1. Analyze the customer churn data, identify the churn status as the target variable, and determine that a classification model is appropriate.\n2. Select a suitable classification algorithm (e.g., Logistic Regression, Random Forest), train the model using cross-validation, and generate performance metrics such as accuracy, precision, and recall.\n\n### Example 2: Training a Regression Model\n\nUser request: \"Train a regression model to predict house prices based on features like size, location, and number of bedrooms.\"\n\nThe skill will:\n1. Analyze the house price data, identify the price as the target variable, and determine that a regression model is appropriate.\n2. Select a suitable regression algorithm (e.g., Linear Regression, Support Vector Regression), train the model using cross-validation, and generate performance metrics such as Mean Squared Error (MSE) and R-squared.\n\n## Best Practices\n\n- **Data Quality**: Ensure the dataset is clean and properly formatted before training the model.\n- **Feature Engineering**: Consider feature engineering techniques to improve model performance.\n- **Hyperparameter Tuning**: Experiment with different hyperparameter settings to optimize model performance.\n\n## Integration\n\nThis skill can be used in conjunction with other data analysis and manipulation tools to prepare data for training. It can also integrate with model deployment tools to deploy the trained model to production.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "ml-model-trainer",
        "category": "ai-ml",
        "path": "plugins/ai-ml/ml-model-trainer",
        "version": "1.0.0",
        "description": "Train and optimize machine learning models with automated workflows"
      },
      "filePath": "plugins/ai-ml/ml-model-trainer/skills/training-machine-learning-models/SKILL.md"
    },
    {
      "slug": "tuning-hyperparameters",
      "name": "tuning-hyperparameters",
      "description": "Optimize machine learning model hyperparameters using grid search, random search, or Bayesian optimization. Finds best parameter configurations to maximize performance. Use when asked to \"tune hyperparameters\" or \"optimize model\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Hyperparameter Tuner\n\nThis skill provides automated assistance for hyperparameter tuner tasks.\n\n## Overview\n\nThis skill empowers Claude to fine-tune machine learning models by automatically searching for the optimal hyperparameter configurations. It leverages different search strategies (grid, random, Bayesian) to efficiently explore the hyperparameter space and identify settings that maximize model performance.\n\n## How It Works\n\n1. **Analyzing Requirements**: Claude analyzes the user's request to determine the model, the hyperparameters to tune, the search strategy, and the evaluation metric.\n2. **Generating Code**: Claude generates Python code using appropriate ML libraries (e.g., scikit-learn, Optuna) to implement the specified hyperparameter search. The code includes data loading, preprocessing, model training, and evaluation.\n3. **Executing Search**: The generated code is executed to perform the hyperparameter search. The plugin iterates through different hyperparameter combinations, trains the model with each combination, and evaluates its performance.\n4. **Reporting Results**: Claude reports the best hyperparameter configuration found during the search, along with the corresponding performance metrics. It also provides insights into the search process and potential areas for further optimization.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Optimize the performance of a machine learning model.\n- Automatically search for the best hyperparameter settings.\n- Compare different hyperparameter search strategies.\n- Improve model accuracy, precision, recall, or other relevant metrics.\n\n## Examples\n\n### Example 1: Optimizing a Random Forest Model\n\nUser request: \"Tune hyperparameters of a Random Forest model using grid search to maximize accuracy on the iris dataset. Consider n_estimators and max_depth.\"\n\nThe skill will:\n1. Generate code to perform a grid search over the specified hyperparameters (n_estimators, max_depth) of a Random Forest model using the iris dataset.\n2. Execute the grid search and report the best hyperparameter combination and the corresponding accuracy score.\n\n### Example 2: Using Bayesian Optimization\n\nUser request: \"Optimize a Gradient Boosting model using Bayesian optimization with Optuna to minimize the root mean squared error on the Boston housing dataset.\"\n\nThe skill will:\n1. Generate code to perform Bayesian optimization using Optuna to find the best hyperparameters for a Gradient Boosting model on the Boston housing dataset.\n2. Execute the optimization and report the best hyperparameter combination and the corresponding RMSE.\n\n## Best Practices\n\n- **Define Search Space**: Clearly define the range and type of values for each hyperparameter to be tuned.\n- **Choose Appropriate Strategy**: Select the hyperparameter search strategy (grid, random, Bayesian) based on the complexity of the hyperparameter space and the available computational resources. Bayesian optimization is generally more efficient for complex spaces.\n- **Use Cross-Validation**: Implement cross-validation to ensure the robustness of the evaluation metric and prevent overfitting.\n\n## Integration\n\nThis skill integrates seamlessly with other Claude Code plugins that involve machine learning tasks, such as data analysis, model training, and deployment. It can be used in conjunction with data visualization tools to gain insights into the impact of different hyperparameter settings on model performance.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "hyperparameter-tuner",
        "category": "ai-ml",
        "path": "plugins/ai-ml/hyperparameter-tuner",
        "version": "1.0.0",
        "description": "Optimize hyperparameters using grid/random/bayesian search"
      },
      "filePath": "plugins/ai-ml/hyperparameter-tuner/skills/tuning-hyperparameters/SKILL.md"
    },
    {
      "slug": "validating-ai-ethics-and-fairness",
      "name": "validating-ai-ethics-and-fairness",
      "description": "Validate AI/ML models and datasets for bias, fairness, and ethical concerns. Use when auditing AI systems for ethical compliance, fairness assessment, or bias detection. Trigger with phrases like \"evaluate model fairness\", \"check for bias\", or \"validate AI ethics\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(python:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Ai Ethics Validator\n\nThis skill provides automated assistance for ai ethics validator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to the AI model or dataset requiring validation\n- Model predictions or training data available for analysis\n- Understanding of demographic attributes relevant to fairness evaluation\n- Python environment with fairness assessment libraries (e.g., Fairlearn, AIF360)\n- Appropriate permissions to analyze sensitive data attributes\n\n## Instructions\n\n### Step 1: Identify Validation Scope\nDetermine which aspects of the AI system require ethical validation:\n- Model predictions across demographic groups\n- Training dataset representation and balance\n- Feature selection and potential proxy variables\n- Output disparities and fairness metrics\n\n### Step 2: Analyze for Bias\nUse the skill to examine the AI system:\n1. Load model predictions or dataset using Read tool\n2. Identify sensitive attributes (age, gender, race, etc.)\n3. Calculate fairness metrics (demographic parity, equalized odds, etc.)\n4. Detect statistical disparities across groups\n\n### Step 3: Generate Validation Report\nThe skill produces a comprehensive report including:\n- Identified biases and their severity\n- Fairness metric calculations with thresholds\n- Representation analysis across demographic groups\n- Recommended mitigation strategies\n- Compliance assessment against ethical guidelines\n\n### Step 4: Implement Mitigations\nBased on findings, apply recommended strategies:\n- Rebalance training data using sampling techniques\n- Apply algorithmic fairness constraints during training\n- Adjust decision thresholds for specific groups\n- Document ethical considerations and trade-offs\n\n## Output\n\nThe skill generates structured reports containing:\n\n### Bias Detection Results\n- Statistical disparities identified across groups\n- Severity classification (low, medium, high, critical)\n- Affected demographic segments with quantified impact\n\n### Fairness Metrics\n- Demographic parity ratios\n- Equal opportunity differences\n- Predictive parity measurements\n- Calibration scores across groups\n\n### Mitigation Recommendations\n- Specific technical approaches to reduce bias\n- Data augmentation or resampling strategies\n- Model constraint adjustments\n- Monitoring and continuous evaluation plans\n\n### Compliance Assessment\n- Alignment with ethical AI guidelines\n- Regulatory compliance status\n- Documentation requirements for audit trails\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Insufficient Data**\n- Error: Cannot calculate fairness metrics with small sample sizes\n- Solution: Aggregate related groups or collect additional data for underrepresented segments\n\n**Missing Sensitive Attributes**\n- Error: Demographic information not available in dataset\n- Solution: Use proxy detection methods or request access to protected attributes under appropriate governance\n\n**Conflicting Fairness Criteria**\n- Error: Multiple fairness metrics show contradictory results\n- Solution: Document trade-offs and prioritize metrics based on use case context and stakeholder input\n\n**Data Quality Issues**\n- Error: Inconsistent or corrupted attribute values\n- Solution: Perform data cleaning, standardization, and validation before bias analysis\n\n## Resources\n\n### Fairness Assessment Frameworks\n- Fairlearn library for bias detection and mitigation\n- AI Fairness 360 (AIF360) toolkit for comprehensive fairness analysis\n- Google What-If Tool for interactive fairness exploration\n\n### Ethical AI Guidelines\n- IEEE Ethically Aligned Design principles\n- EU Ethics Guidelines for Trustworthy AI\n- ACM Code of Ethics for AI practitioners\n\n### Fairness Metrics Documentation\n- Demographic parity and statistical parity definitions\n- Equalized odds and equal opportunity metrics\n- Individual fairness and calibration measures\n\n### Best Practices\n- Involve diverse stakeholders in fairness criteria selection\n- Document all ethical decisions and trade-offs\n- Implement continuous monitoring for fairness drift\n- Maintain transparency in model limitations and biases\n\n## Overview\n\n\nThis skill provides automated assistance for ai ethics validator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "ai-ethics-validator",
        "category": "ai-ml",
        "path": "plugins/ai-ml/ai-ethics-validator",
        "version": "1.0.0",
        "description": "AI ethics and fairness validation"
      },
      "filePath": "plugins/ai-ml/ai-ethics-validator/skills/validating-ai-ethics-and-fairness/SKILL.md"
    },
    {
      "slug": "validating-api-contracts",
      "name": "validating-api-contracts",
      "description": "Validate API contracts using consumer-driven contract testing (Pact, Spring Cloud Contract). Use when performing specialized testing. Trigger with phrases like \"validate API contract\", \"run contract tests\", or \"check consumer contracts\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:contract-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Contract Test Validator\n\nThis skill provides automated assistance for contract test validator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:contract-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for contract test validator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "contract-test-validator",
        "category": "testing",
        "path": "plugins/testing/contract-test-validator",
        "version": "1.0.0",
        "description": "API contract testing with Pact, OpenAPI validation, and consumer-driven contract verification"
      },
      "filePath": "plugins/testing/contract-test-validator/skills/validating-api-contracts/SKILL.md"
    },
    {
      "slug": "validating-api-responses",
      "name": "validating-api-responses",
      "description": "Validate API responses against schemas to ensure contract compliance and data integrity. Use when ensuring API response correctness. Trigger with phrases like \"validate responses\", \"check API responses\", or \"verify response format\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:validate-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Validating Api Responses\n\n## Overview\n\n\nThis skill provides automated assistance for api response validator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:validate-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-response-validator",
        "category": "api-development",
        "path": "plugins/api-development/api-response-validator",
        "version": "1.0.0",
        "description": "Validate API responses against schemas and contracts"
      },
      "filePath": "plugins/api-development/api-response-validator/skills/validating-api-responses/SKILL.md"
    },
    {
      "slug": "validating-api-schemas",
      "name": "validating-api-schemas",
      "description": "Validate API schemas against OpenAPI, JSON Schema, and GraphQL specifications. Use when validating API schemas and contracts. Trigger with phrases like \"validate API schema\", \"check OpenAPI spec\", or \"verify schema\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:schema-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Validating Api Schemas\n\n## Overview\n\n\nThis skill provides automated assistance for api schema validator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:schema-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-schema-validator",
        "category": "api-development",
        "path": "plugins/api-development/api-schema-validator",
        "version": "1.0.0",
        "description": "Validate API schemas with JSON Schema, Joi, Yup, or Zod"
      },
      "filePath": "plugins/api-development/api-schema-validator/skills/validating-api-schemas/SKILL.md"
    },
    {
      "slug": "validating-authentication-implementations",
      "name": "validating-authentication-implementations",
      "description": "Validate authentication mechanisms for security weaknesses and compliance. Use when reviewing login systems or auth flows. Trigger with 'validate authentication', 'check auth security', or 'review login'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Authentication Validator\n\nThis skill provides automated assistance for authentication validator tasks.\n\n## Overview\n\nThis skill allows Claude to assess the security of authentication mechanisms in a system or application. It provides a detailed report highlighting potential vulnerabilities and offering recommendations for improvement based on established security principles.\n\n## How It Works\n\n1. **Initiate Validation**: Upon receiving a trigger phrase, the skill activates the `authentication-validator` plugin.\n2. **Analyze Authentication Methods**: The plugin examines the implemented authentication methods, such as JWT, OAuth, session-based, or API keys.\n3. **Generate Security Report**: The plugin generates a comprehensive report outlining potential vulnerabilities and recommended fixes related to password security, session management, token security (JWT), multi-factor authentication, and account security.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Assess the security of an application's authentication implementation.\n- Identify vulnerabilities in password policies and session management.\n- Evaluate the security of JWT tokens and MFA implementation.\n- Ensure compliance with security best practices and industry standards.\n\n## Examples\n\n### Example 1: Assessing JWT Security\n\nUser request: \"validate authentication for jwt implementation\"\n\nThe skill will:\n1. Activate the `authentication-validator` plugin.\n2. Analyze the JWT implementation, checking for strong signing algorithms, proper expiration claims, and audience/issuer validation.\n3. Generate a report highlighting any vulnerabilities and recommending best practices for JWT security.\n\n### Example 2: Checking Session Security\n\nUser request: \"authcheck session cookies\"\n\nThe skill will:\n1. Activate the `authentication-validator` plugin.\n2. Analyze the session cookie settings, including HttpOnly, Secure, and SameSite attributes.\n3. Generate a report outlining any potential session fixation or CSRF vulnerabilities and recommending appropriate countermeasures.\n\n## Best Practices\n\n- **Password Hashing**: Always use strong hashing algorithms like bcrypt or Argon2 with appropriate salt generation.\n- **Token Expiration**: Implement short-lived access tokens and refresh token rotation for enhanced security.\n- **Multi-Factor Authentication**: Encourage or enforce MFA to mitigate the risk of password compromise.\n\n## Integration\n\nThis skill can be used in conjunction with other security-related plugins to provide a comprehensive security assessment of an application. For example, it can be used alongside a code analysis plugin to identify potential code-level vulnerabilities related to authentication.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "authentication-validator",
        "category": "security",
        "path": "plugins/security/authentication-validator",
        "version": "1.0.0",
        "description": "Validate authentication implementations"
      },
      "filePath": "plugins/security/authentication-validator/skills/validating-authentication-implementations/SKILL.md"
    },
    {
      "slug": "validating-cors-policies",
      "name": "validating-cors-policies",
      "description": "Validate CORS policies for security issues and misconfigurations. Use when reviewing cross-origin resource sharing. Trigger with 'validate CORS', 'check CORS policy', or 'review cross-origin'.",
      "allowedTools": [
        "\"Read",
        "WebFetch",
        "WebSearch",
        "Grep\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Cors Policy Validator\n\nThis skill provides automated assistance for cors policy validator tasks.\n\n## Overview\n\nThis skill empowers Claude to assess the security and correctness of CORS policies. By leveraging the cors-policy-validator plugin, it identifies misconfigurations and potential vulnerabilities in CORS settings, helping developers build more secure web applications.\n\n## How It Works\n\n1. **Analyze CORS Configuration**: The skill receives the CORS configuration details, such as headers or policy files.\n2. **Validate Policy**: It utilizes the cors-policy-validator plugin to analyze the provided configuration against established security best practices.\n3. **Report Findings**: The skill presents a detailed report outlining any identified vulnerabilities or misconfigurations in the CORS policy.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Validate a CORS policy for a web application.\n- Check the CORS configuration of an API endpoint.\n- Identify potential security vulnerabilities in existing CORS implementations.\n\n## Examples\n\n### Example 1: Validating a CORS Policy File\n\nUser request: \"Validate the CORS policy in `cors_policy.json`\"\n\nThe skill will:\n1. Read the `cors_policy.json` file.\n2. Use the cors-policy-validator plugin to analyze the CORS configuration.\n3. Output a report detailing any identified vulnerabilities or misconfigurations.\n\n### Example 2: Checking CORS Headers for an API Endpoint\n\nUser request: \"Check CORS headers for the API endpoint at `https://example.com/api`\"\n\nThe skill will:\n1. Fetch the CORS headers from the specified API endpoint.\n2. Use the cors-policy-validator plugin to analyze the headers.\n3. Output a report summarizing the CORS configuration and any potential issues.\n\n## Best Practices\n\n- **Configuration Source**: Always specify the source of the CORS configuration (e.g., file path, URL) for accurate validation.\n- **Regular Validation**: Regularly validate CORS policies, especially after making changes to the application or API.\n- **Heuristic Analysis**: Consider supplementing validation with manual review and heuristic analysis to catch subtle vulnerabilities.\n\n## Integration\n\nThis skill can be integrated with other security-related plugins to provide a more comprehensive security assessment. For example, it can be used in conjunction with vulnerability scanning tools to identify potential cross-site scripting (XSS) vulnerabilities related to CORS misconfigurations.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "cors-policy-validator",
        "category": "security",
        "path": "plugins/security/cors-policy-validator",
        "version": "1.0.0",
        "description": "Validate CORS policies"
      },
      "filePath": "plugins/security/cors-policy-validator/skills/validating-cors-policies/SKILL.md"
    },
    {
      "slug": "validating-csrf-protection",
      "name": "validating-csrf-protection",
      "description": "Validate CSRF protection implementations for security gaps. Use when reviewing form security or state-changing operations. Trigger with 'validate CSRF', 'check CSRF protection', or 'review token security'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Csrf Protection Validator\n\nThis skill provides automated assistance for csrf protection validator tasks.\n\n## Overview\n\nThis skill empowers Claude to analyze web applications for CSRF vulnerabilities. It assesses the effectiveness of implemented CSRF protection mechanisms, providing insights into potential weaknesses and recommendations for remediation.\n\n## How It Works\n\n1. **Analyze Endpoints**: The plugin examines application endpoints to identify those lacking CSRF protection.\n2. **Assess Protection Mechanisms**: It validates the implementation of CSRF protection mechanisms, including token validation, double-submit cookies, SameSite attributes, and origin validation.\n3. **Generate Report**: A detailed report is generated, highlighting vulnerable endpoints, potential attack scenarios, and recommended fixes.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Validate existing CSRF protection measures.\n- Identify CSRF vulnerabilities in a web application.\n- Assess the risk associated with unprotected endpoints.\n- Generate a report outlining CSRF vulnerabilities and recommended fixes.\n\n## Examples\n\n### Example 1: Identifying Unprotected API Endpoints\n\nUser request: \"validate csrf\"\n\nThe skill will:\n1. Analyze the application's API endpoints.\n2. Identify endpoints lacking CSRF protection, such as those handling sensitive data modifications.\n3. Generate a report outlining vulnerable endpoints and potential attack vectors.\n\n### Example 2: Checking SameSite Cookie Attributes\n\nUser request: \"Check for csrf vulnerabilities in my application\"\n\nThe skill will:\n1. Analyze the application's cookie settings.\n2. Verify that SameSite attributes are properly configured to mitigate CSRF attacks.\n3. Report any cookies lacking the SameSite attribute or using an insecure setting.\n\n## Best Practices\n\n- **Regular Validation**: Regularly validate CSRF protection mechanisms as part of the development lifecycle.\n- **Comprehensive Coverage**: Ensure all state-changing operations are protected against CSRF attacks.\n- **Secure Configuration**: Use secure configurations for CSRF protection mechanisms, such as strong token generation and proper SameSite attribute settings.\n\n## Integration\n\nThis skill can be used in conjunction with other security plugins to provide a comprehensive security assessment of web applications. For example, it can be combined with a vulnerability scanner to identify other potential vulnerabilities in addition to CSRF weaknesses.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "csrf-protection-validator",
        "category": "security",
        "path": "plugins/security/csrf-protection-validator",
        "version": "1.0.0",
        "description": "Validate CSRF protection"
      },
      "filePath": "plugins/security/csrf-protection-validator/skills/validating-csrf-protection/SKILL.md"
    },
    {
      "slug": "validating-database-integrity",
      "name": "validating-database-integrity",
      "description": "Process use when you need to ensure database integrity through comprehensive data validation. This skill validates data types, ranges, formats, referential integrity, and business rules. Trigger with phrases like \"validate database data\", \"implement data validation rules\", \"enforce data integrity constraints\", or \"validate data formats\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Data Validation Engine\n\nThis skill provides automated assistance for data validation engine tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Database connection credentials are available\n- Appropriate database permissions for schema modifications\n- Backup of production databases before applying constraints\n- Understanding of existing data that may violate new constraints\n- Access to database documentation for column specifications\n\n## Instructions\n\n### Step 1: Analyze Validation Requirements\n1. Review database schema and identify columns requiring validation\n2. Determine validation types needed (data type, range, format, referential)\n3. Document existing data patterns that may conflict with new rules\n4. Prioritize validation rules by business criticality\n\n### Step 2: Define Validation Rules\n1. Create validation rule definitions for each column\n2. Specify data types, constraints, and acceptable ranges\n3. Define regular expressions for format validation\n4. Map foreign key relationships for referential integrity\n5. Document business rule logic for complex validations\n\n### Step 3: Implement Database Constraints\n1. Generate SQL constraints for data type validation\n2. Add CHECK constraints for range and format validation\n3. Create foreign key constraints for referential integrity\n4. Implement triggers for complex business rule validation\n5. Test constraints with valid and invalid sample data\n\n### Step 4: Validate Existing Data\n1. Query existing data to identify constraint violations\n2. Generate reports of data that would fail new constraints\n3. Create data cleanup scripts to fix violations\n4. Execute cleanup scripts in staging environment first\n5. Re-validate cleaned data before applying constraints\n\n### Step 5: Apply Validation Rules\n1. Apply constraints to staging database first\n2. Monitor for any application errors or failures\n3. Validate that legitimate operations still function\n4. Apply constraints to production database during maintenance window\n5. Monitor database logs for constraint violation attempts\n\n## Output\n\nThis skill produces:\n\n**Database Constraints**: SQL DDL statements with CHECK, FOREIGN KEY, and NOT NULL constraints\n\n**Validation Reports**: Analysis of existing data showing constraint violations with counts and examples\n\n**Data Cleanup Scripts**: SQL UPDATE/DELETE statements to fix existing data that violates new constraints\n\n**Test Results**: Documentation of constraint testing with valid/invalid data samples and outcomes\n\n**Implementation Log**: Timestamped record of constraint application with success/failure status\n\n## Error Handling\n\n**Constraint Violation Errors**:\n- Review existing data that violates the constraint\n- Create data cleanup scripts to fix violations\n- Re-run constraint application after cleanup\n- Document exceptions that require manual review\n\n**Permission Errors**:\n- Verify database user has ALTER TABLE privileges\n- Request elevated permissions from database administrator\n- Use separate admin connection for schema changes\n- Document permission requirements for future deployments\n\n**Circular Dependency Errors**:\n- Map all foreign key relationships before implementation\n- Apply constraints in dependency order (referenced tables first)\n- Use ALTER TABLE ADD CONSTRAINT for deferred constraint creation\n- Consider disabling foreign key checks temporarily during bulk operations\n\n**Performance Degradation**:\n- Analyze constraint checking overhead with EXPLAIN ANALYZE\n- Add appropriate indexes to support constraint validation\n- Consider batch validation for large data updates\n- Monitor query performance after constraint implementation\n\n## Resources\n\n**Database-Specific Constraint Syntax**:\n- PostgreSQL: `{baseDir}/docs/postgresql-constraints.md`\n- MySQL: `{baseDir}/docs/mysql-constraints.md`\n- SQL Server: `{baseDir}/docs/sqlserver-constraints.md`\n\n**Validation Rule Templates**: `{baseDir}/templates/validation-rules/`\n- Email format validation\n- Phone number validation\n- Date range validation\n- Numeric range validation\n- Custom business rules\n\n**Testing Guidelines**: `{baseDir}/docs/validation-testing.md`\n**Constraint Performance Analysis**: `{baseDir}/docs/constraint-performance.md`\n**Data Cleanup Procedures**: `{baseDir}/docs/data-cleanup-procedures.md`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "data-validation-engine",
        "category": "database",
        "path": "plugins/database/data-validation-engine",
        "version": "1.0.0",
        "description": "Database plugin for data-validation-engine"
      },
      "filePath": "plugins/database/data-validation-engine/skills/validating-database-integrity/SKILL.md"
    },
    {
      "slug": "validating-pci-dss-compliance",
      "name": "validating-pci-dss-compliance",
      "description": "Validate PCI-DSS compliance for payment card data security. Use when auditing payment systems. Trigger with 'validate PCI-DSS', 'check payment security', or 'audit card data'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Pci Dss Validator\n\nThis skill provides automated assistance for pci dss validator tasks.\n\n## Overview\n\nThis skill streamlines PCI DSS compliance checks by automatically analyzing code and configurations. It flags potential issues, allowing for proactive remediation and improved security posture. It is particularly useful for developers, security engineers, and compliance officers.\n\n## How It Works\n\n1. **Analyze the Target**: The skill identifies the codebase, configuration files, or infrastructure resources to be evaluated.\n2. **Run PCI DSS Validation**: The pci-dss-validator plugin scans the target for potential PCI DSS violations.\n3. **Generate Report**: The skill compiles a report detailing any identified vulnerabilities or non-compliant configurations, along with remediation recommendations.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Evaluate a new application or system for PCI DSS compliance before deployment.\n- Periodically assess existing systems to maintain PCI DSS compliance.\n- Investigate potential security vulnerabilities related to PCI DSS.\n\n## Examples\n\n### Example 1: Validating a Web Application\n\nUser request: \"Validate PCI compliance for my e-commerce web application.\"\n\nThe skill will:\n1. Identify the source code repository for the web application.\n2. Run the pci-dss-validator plugin against the codebase.\n3. Generate a report highlighting any PCI DSS violations found in the code.\n\n### Example 2: Checking Infrastructure Configuration\n\nUser request: \"Check PCI DSS compliance of my AWS infrastructure.\"\n\nThe skill will:\n1. Access the AWS configuration files (e.g., Terraform, CloudFormation).\n2. Execute the pci-dss-validator plugin against the infrastructure configuration.\n3. Produce a report outlining any non-compliant configurations in the AWS environment.\n\n## Best Practices\n\n- **Scope Definition**: Clearly define the scope of the PCI DSS assessment to ensure accurate and relevant results.\n- **Regular Assessments**: Conduct regular PCI DSS assessments to maintain continuous compliance.\n- **Remediation Tracking**: Track and document all remediation efforts to demonstrate ongoing commitment to security.\n\n## Integration\n\nThis skill can be integrated with other security tools and plugins to provide a comprehensive security assessment. For example, it can be used in conjunction with static analysis tools to identify vulnerabilities in code before it is deployed. It can also be integrated with infrastructure-as-code tools to ensure that infrastructure is compliant with PCI DSS from the start.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "pci-dss-validator",
        "category": "security",
        "path": "plugins/security/pci-dss-validator",
        "version": "1.0.0",
        "description": "Validate PCI DSS compliance"
      },
      "filePath": "plugins/security/pci-dss-validator/skills/validating-pci-dss-compliance/SKILL.md"
    },
    {
      "slug": "validating-performance-budgets",
      "name": "validating-performance-budgets",
      "description": "Validate application performance against defined budgets to identify regressions early. Use when checking page load times, bundle sizes, or API response times against thresholds. Trigger with phrases like \"validate performance budget\", \"check performance metrics\", or \"detect performance regression\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(lighthouse:*)",
        "Bash(webpack:*)",
        "Bash(performance:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Performance Budget Validator\n\nThis skill provides automated assistance for performance budget validator tasks.\n\n## Overview\n\nThis skill allows Claude to automatically validate your application's performance against predefined budgets. It helps identify performance regressions and ensures your application maintains optimal performance characteristics.\n\n## How It Works\n\n1. **Analyze Performance Metrics**: Claude analyzes current performance metrics, such as page load times, bundle sizes, and API response times.\n2. **Validate Against Budget**: The plugin validates these metrics against predefined performance budget thresholds.\n3. **Report Violations**: If any metrics exceed the defined budget, the skill reports violations and provides details on the exceeded thresholds.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Validate performance against predefined budgets.\n- Identify performance regressions in your application.\n- Integrate performance budget validation into your CI/CD pipeline.\n\n## Examples\n\n### Example 1: Preventing Performance Regressions\n\nUser request: \"Validate performance budget for the homepage.\"\n\nThe skill will:\n1. Analyze the homepage's performance metrics (load time, bundle size).\n2. Compare these metrics against the defined budget.\n3. Report any violations, such as exceeding the load time budget.\n\n### Example 2: Integrating with CI/CD\n\nUser request: \"Run performance budget validation as part of the build process.\"\n\nThe skill will:\n1. Execute the performance budget validation command.\n2. Check all defined performance metrics against their budgets.\n3. Report any violations that would cause the build to fail.\n\n## Best Practices\n\n- **Budget Definition**: Define realistic and achievable performance budgets based on current application performance and user expectations.\n- **Metric Selection**: Choose relevant performance metrics that directly impact user experience, such as page load times and API response times.\n- **CI/CD Integration**: Integrate performance budget validation into your CI/CD pipeline to automatically detect and prevent performance regressions.\n\n## Integration\n\nThis skill can be integrated with other plugins that provide performance metrics, such as website speed test tools or API monitoring services. It can also be used in conjunction with alerting plugins to notify developers of performance budget violations.\n\n## Prerequisites\n\n- Performance budget definitions in {baseDir}/performance-budgets.json\n- Access to performance testing tools (Lighthouse, WebPageTest)\n- Build output directory for bundle analysis\n- Historical performance metrics for comparison\n\n## Instructions\n\n1. Load performance budget configuration\n2. Collect current performance metrics (load time, bundle size, API latency)\n3. Compare metrics against defined budget thresholds\n4. Identify budget violations and severity\n5. Generate detailed violation report\n6. Provide remediation recommendations\n\n## Output\n\n- Performance budget validation report\n- List of metrics exceeding budget thresholds\n- Comparison with previous measurements\n- Detailed breakdown by metric category\n- Actionable recommendations for fixes\n\n## Error Handling\n\nIf budget validation fails:\n- Verify budget configuration file exists\n- Check performance testing tool availability\n- Validate metric collection permissions\n- Ensure network access to test endpoints\n- Review budget threshold definitions\n\n## Resources\n\n- Performance budget best practices\n- Lighthouse performance scoring guide\n- Bundle size optimization techniques\n- CI/CD integration patterns for performance testing",
      "parentPlugin": {
        "name": "performance-budget-validator",
        "category": "performance",
        "path": "plugins/performance/performance-budget-validator",
        "version": "1.0.0",
        "description": "Validate application against performance budgets"
      },
      "filePath": "plugins/performance/performance-budget-validator/skills/validating-performance-budgets/SKILL.md"
    },
    {
      "slug": "validator-expert",
      "name": "validator-expert",
      "description": "Validate production readiness of Vertex AI Agent Engine deployments across security, monitoring, performance, compliance, and best practices. Generates weighted scores (0-100%) with actionable recommendations. Use when asked to \"validate deploymen... Trigger with phrases like 'validate', 'check', or 'verify'. allowed-tools: Read, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Validator Expert\n\nThis skill provides automated assistance for validator expert tasks.\n\n## What This Skill Does\n\nProduction validator for Vertex AI deployments. Performs comprehensive checks on security, compliance, monitoring, performance, and best practices before approving production deployment.\n\n## When This Skill Activates\n\nTriggers: \"validate deployment\", \"production readiness\", \"security audit vertex ai\", \"check compliance\", \"validate adk agent\"\n\n## Validation Checklist\n\n### Security Validation\n- ✅ IAM roles follow least privilege\n- ✅ VPC Service Controls enabled\n- ✅ Encryption at rest configured\n- ✅ No hardcoded secrets\n- ✅ Service accounts properly configured\n- ✅ Model Armor enabled (for ADK)\n\n### Monitoring Validation\n- ✅ Cloud Monitoring dashboards configured\n- ✅ Alerting policies set\n- ✅ Token usage tracking enabled\n- ✅ Error rate monitoring active\n- ✅ Latency SLOs defined\n\n### Performance Validation\n- ✅ Auto-scaling configured\n- ✅ Resource limits appropriate\n- ✅ Caching strategy implemented\n- ✅ Code Execution sandbox TTL set\n- ✅ Memory Bank retention configured\n\n### Compliance Validation\n- ✅ Audit logging enabled\n- ✅ Data residency requirements met\n- ✅ Privacy policies implemented\n- ✅ Backup/disaster recovery configured\n\n## Tool Permissions\n\nRead, Grep, Glob, Bash - Read-only analysis for security\n\n## References\n\n- Vertex AI Security: https://cloud.google.com/vertex-ai/docs/security\n\n## Overview\n\n\nThis skill provides automated assistance for validator expert tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "jeremy-vertex-validator",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-vertex-validator",
        "version": "1.0.0",
        "description": "Production readiness validator for Vertex AI deployments and configurations"
      },
      "filePath": "plugins/ai-ml/jeremy-vertex-validator/skills/validator-expert/SKILL.md"
    },
    {
      "slug": "vercel-advanced-troubleshooting",
      "name": "vercel-advanced-troubleshooting",
      "description": "Execute apply Vercel advanced debugging techniques for hard-to-diagnose issues. Use when standard troubleshooting fails, investigating complex race conditions, or preparing evidence bundles for Vercel support escalation. Trigger with phrases like \"vercel hard bug\", \"vercel mystery error\", \"vercel impossible to debug\", \"difficult vercel issue\", \"vercel deep debug\". allowed-tools: Read, Grep, Bash(kubectl:*), Bash(curl:*), Bash(tcpdump:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Advanced Troubleshooting\n\n## Prerequisites\n- Access to production logs and metrics\n- kubectl access to clusters\n- Network capture tools available\n- Understanding of distributed tracing\n\n## Instructions\n\n### Step 1: Collect Evidence Bundle\nRun the comprehensive debug script to gather all relevant data.\n\n### Step 2: Systematic Isolation\nTest each layer independently to identify the failure point.\n\n### Step 3: Create Minimal Reproduction\nStrip down to the simplest failing case.\n\n### Step 4: Escalate with Evidence\nUse the support template with all collected evidence.\n\n## Output\n- Comprehensive debug bundle collected\n- Failure layer identified\n- Minimal reproduction created\n- Support escalation submitted\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel Support Portal](https://support.vercel.com)\n- [Vercel Status Page](https://www.vercel-status.com)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-advanced-troubleshooting/SKILL.md"
    },
    {
      "slug": "vercel-architecture-variants",
      "name": "vercel-architecture-variants",
      "description": "Execute choose and implement Vercel validated architecture blueprints for different scales. Use when designing new Vercel integrations, choosing between monolith/service/microservice architectures, or planning migration paths for Vercel applications. Trigger with phrases like \"vercel architecture\", \"vercel blueprint\", \"how to structure vercel\", \"vercel project layout\", \"vercel microservice\". allowed-tools: Read, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Architecture Variants\n\n## Prerequisites\n- Understanding of team size and DAU requirements\n- Knowledge of deployment infrastructure\n- Clear SLA requirements\n- Growth projections available\n\n## Instructions\n\n### Step 1: Assess Requirements\nUse the decision matrix to identify appropriate variant.\n\n### Step 2: Choose Architecture\nSelect Monolith, Service Layer, or Microservice based on needs.\n\n### Step 3: Implement Structure\nSet up project layout following the chosen blueprint.\n\n### Step 4: Plan Migration Path\nDocument upgrade path for future scaling.\n\n## Output\n- Architecture variant selected\n- Project structure implemented\n- Migration path documented\n- Appropriate patterns applied\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Monolith First](https://martinfowler.com/bliki/MonolithFirst.html)\n- [Microservices Guide](https://martinfowler.com/microservices/)\n- [Vercel Architecture Guide](https://vercel.com/docs/architecture)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-architecture-variants/SKILL.md"
    },
    {
      "slug": "vercel-ci-integration",
      "name": "vercel-ci-integration",
      "description": "Configure Vercel CI/CD integration with GitHub Actions and testing. Use when setting up automated testing, configuring CI pipelines, or integrating Vercel tests into your build process. Trigger with phrases like \"vercel CI\", \"vercel GitHub Actions\", \"vercel automated tests\", \"CI vercel\". allowed-tools: Read, Write, Edit, Bash(gh:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Ci Integration\n\n## Prerequisites\n- GitHub repository with Actions enabled\n- Vercel test API key\n- npm/pnpm project configured\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n- Automated test pipeline\n- PR checks configured\n- Coverage reports uploaded\n- Release workflow ready\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [GitHub Actions Documentation](https://docs.github.com/en/actions)\n- [Vercel CI Guide](https://vercel.com/docs/ci)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-ci-integration/SKILL.md"
    },
    {
      "slug": "vercel-common-errors",
      "name": "vercel-common-errors",
      "description": "Execute diagnose and fix Vercel common errors and exceptions. Use when encountering Vercel errors, debugging failed requests, or troubleshooting integration issues. Trigger with phrases like \"vercel error\", \"fix vercel\", \"vercel not working\", \"debug vercel\". allowed-tools: Read, Grep, Bash(curl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Common Errors\n\n## Prerequisites\n- Vercel SDK installed\n- API credentials configured\n- Access to error logs\n\n## Instructions\n\n### Step 1: Identify the Error\nCheck error message and code in your logs or console.\n\n### Step 2: Find Matching Error Below\nMatch your error to one of the documented cases.\n\n### Step 3: Apply Solution\nFollow the solution steps for your specific error.\n\n## Output\n- Identified error cause\n- Applied fix\n- Verified resolution\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel Status Page](https://www.vercel-status.com)\n- [Vercel Support](https://vercel.com/docs/support)\n- [Vercel Error Codes](https://vercel.com/docs/errors)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-common-errors/SKILL.md"
    },
    {
      "slug": "vercel-cost-tuning",
      "name": "vercel-cost-tuning",
      "description": "Optimize Vercel costs through tier selection, sampling, and usage monitoring. Use when analyzing Vercel billing, reducing API costs, or implementing usage monitoring and budget alerts. Trigger with phrases like \"vercel cost\", \"vercel billing\", \"reduce vercel costs\", \"vercel pricing\", \"vercel expensive\", \"vercel budget\". allowed-tools: Read, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Cost Tuning\n\n## Prerequisites\n- Access to Vercel billing dashboard\n- Understanding of current usage patterns\n- Database for usage tracking (optional)\n- Alerting system configured (optional)\n\n## Instructions\n\n### Step 1: Analyze Current Usage\nReview Vercel dashboard for usage patterns and costs.\n\n### Step 2: Select Optimal Tier\nUse the cost estimation function to find the right tier.\n\n### Step 3: Implement Monitoring\nAdd usage tracking to catch budget overruns early.\n\n### Step 4: Apply Optimizations\nEnable batching, caching, and sampling where appropriate.\n\n## Output\n- Optimized tier selection\n- Usage monitoring implemented\n- Budget alerts configured\n- Cost reduction strategies applied\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel Pricing](https://vercel.com/pricing)\n- [Vercel Billing Dashboard](https://dashboard.vercel.com/billing)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-cost-tuning/SKILL.md"
    },
    {
      "slug": "vercel-data-handling",
      "name": "vercel-data-handling",
      "description": "Implement Vercel PII handling, data retention, and GDPR/CCPA compliance patterns. Use when handling sensitive data, implementing data redaction, configuring retention policies, or ensuring compliance with privacy regulations for Vercel integrations. Trigger with phrases like \"vercel data\", \"vercel PII\", \"vercel GDPR\", \"vercel data retention\", \"vercel privacy\", \"vercel CCPA\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Data Handling\n\n## Overview\nHandle sensitive data correctly when integrating with Vercel.\n\n## Prerequisites\n- Understanding of GDPR/CCPA requirements\n- Vercel SDK with data export capabilities\n- Database for audit logging\n- Scheduled job infrastructure for cleanup\n\n## Data Classification\n\n| Category | Examples | Handling |\n|----------|----------|----------|\n| PII | Email, name, phone | Encrypt, minimize |\n| Sensitive | API keys, tokens | Never log, rotate |\n| Business | Usage metrics | Aggregate when possible |\n| Public | Product names | Standard handling |\n\n## PII Detection\n\n```typescript\nconst PII_PATTERNS = [\n  { type: 'email', regex: /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}/g },\n  { type: 'phone', regex: /\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b/g },\n  { type: 'ssn', regex: /\\b\\d{3}-\\d{2}-\\d{4}\\b/g },\n  { type: 'credit_card', regex: /\\b\\d{4}[- ]?\\d{4}[- ]?\\d{4}[- ]?\\d{4}\\b/g },\n];\n\nfunction detectPII(text: string): { type: string; match: string }[] {\n  const findings: { type: string; match: string }[] = [];\n\n  for (const pattern of PII_PATTERNS) {\n    const matches = text.matchAll(pattern.regex);\n    for (const match of matches) {\n\n## Detailed Reference\n\nSee `{baseDir}/references/implementation.md` for complete data handling guide.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-data-handling/SKILL.md"
    },
    {
      "slug": "vercel-debug-bundle",
      "name": "vercel-debug-bundle",
      "description": "Execute collect Vercel debug evidence for support tickets and troubleshooting. Use when encountering persistent issues, preparing support tickets, or collecting diagnostic information for Vercel problems. Trigger with phrases like \"vercel debug\", \"vercel support bundle\", \"collect vercel logs\", \"vercel diagnostic\". allowed-tools: Read, Bash(grep:*), Bash(curl:*), Bash(tar:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Debug Bundle\n\n## Prerequisites\n- Vercel SDK installed\n- Access to application logs\n- Permission to collect environment info\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n- `vercel-debug-YYYYMMDD-HHMMSS.tar.gz` archive containing:\n  - `summary.txt` - Environment and SDK info\n  - `logs.txt` - Recent redacted logs\n  - `config-redacted.txt` - Configuration (secrets removed)\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel Support](https://vercel.com/docs/support)\n- [Vercel Status](https://www.vercel-status.com)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-debug-bundle/SKILL.md"
    },
    {
      "slug": "vercel-deploy-integration",
      "name": "vercel-deploy-integration",
      "description": "Deploy Vercel integrations to Vercel, Fly.io, and Cloud Run platforms. Use when deploying Vercel-powered applications to production, configuring platform-specific secrets, or setting up deployment pipelines. Trigger with phrases like \"deploy vercel\", \"vercel Vercel\", \"vercel production deploy\", \"vercel Cloud Run\", \"vercel Fly.io\". allowed-tools: Read, Write, Edit, Bash(vercel:*), Bash(fly:*), Bash(gcloud:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Deploy Integration\n\n## Prerequisites\n- Vercel API keys for production environment\n- Platform CLI installed (vercel, fly, or gcloud)\n- Application code ready for deployment\n- Environment variables documented\n\n## Instructions\n\n### Step 1: Choose Deployment Platform\nSelect the platform that best fits your infrastructure needs and follow the platform-specific guide below.\n\n### Step 2: Configure Secrets\nStore Vercel API keys securely using the platform's secrets management.\n\n### Step 3: Deploy Application\nUse the platform CLI to deploy your application with Vercel integration.\n\n### Step 4: Verify Health\nTest the health check endpoint to confirm Vercel connectivity.\n\n## Output\n- Application deployed to production\n- Vercel secrets securely configured\n- Health check endpoint functional\n- Environment-specific configuration in place\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel Documentation](https://vercel.com/docs)\n- [Fly.io Documentation](https://fly.io/docs)\n- [Cloud Run Documentation](https://cloud.google.com/run/docs)\n- [Vercel Deploy Guide](https://vercel.com/docs/deploy)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-deploy-integration/SKILL.md"
    },
    {
      "slug": "vercel-deploy-preview",
      "name": "vercel-deploy-preview",
      "description": "Execute Vercel primary workflow: Deploy Preview. Use when Deploying a preview for a pull request, Testing changes before production, or Sharing preview URLs with stakeholders. Trigger with phrases like \"vercel deploy preview\", \"create preview deployment with vercel\". allowed-tools: Read, Write, Edit, Bash(npm:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Deploy Preview\n\n## Overview\nDeploy preview environments for pull requests and branches.\nThis is the primary workflow for Vercel - instant previews for every commit.\n\n\n## Prerequisites\n- Completed `vercel-install-auth` setup\n- Understanding of Vercel core concepts\n- Valid API credentials configured\n\n## Instructions\n\n### Step 1: Initialize\n```typescript\n// Step 1 implementation\n```\n\n### Step 2: Execute\n```typescript\n// Step 2 implementation\n```\n\n### Step 3: Finalize\n```typescript\n// Step 3 implementation\n```\n\n## Output\n- Completed Deploy Preview execution\n- Expected results from Vercel API\n- Success confirmation or error details\n\n## Error Handling\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Error 1 | Cause | Solution |\n| Error 2 | Cause | Solution |\n\n## Examples\n\n### Complete Workflow\n```typescript\n// Complete workflow example\n```\n\n### Common Variations\n- Variation 1: Description\n- Variation 2: Description\n\n## Resources\n- [Vercel Documentation](https://vercel.com/docs)\n- [Vercel API Reference](https://vercel.com/docs/api)\n\n## Next Steps\nFor secondary workflow, see `vercel-edge-functions`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-deploy-preview/SKILL.md"
    },
    {
      "slug": "vercel-edge-functions",
      "name": "vercel-edge-functions",
      "description": "Execute Vercel secondary workflow: Edge Functions. Use when API routes with minimal latency, or complementing primary workflow. Trigger with phrases like \"vercel edge function\", \"deploy edge function with vercel\". allowed-tools: Read, Write, Edit, Bash(npm:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Edge Functions\n\n## Overview\nBuild and deploy Edge Functions for ultra-low latency at the edge.\nServerless functions that run close to users worldwide.\n\n\n## Prerequisites\n- Completed `vercel-install-auth` setup\n- Familiarity with `vercel-deploy-preview`\n- Valid API credentials configured\n\n## Instructions\n\n### Step 1: Setup\n```typescript\n// Step 1 implementation\n```\n\n### Step 2: Process\n```typescript\n// Step 2 implementation\n```\n\n### Step 3: Complete\n```typescript\n// Step 3 implementation\n```\n\n## Output\n- Completed Edge Functions execution\n- Results from Vercel API\n- Success confirmation or error details\n\n## Error Handling\n| Aspect | Deploy Preview | Edge Functions |\n|--------|------------|------------|\n| Use Case | Deploying a preview for a pull request | API routes with minimal latency |\n| Complexity | Medium | Medium |\n| Performance | Standard | Ultra-fast (<50ms) |\n\n## Examples\n\n### Complete Workflow\n```typescript\n// Complete workflow example\n```\n\n### Error Recovery\n```typescript\n// Error handling code\n```\n\n## Resources\n- [Vercel Documentation](https://vercel.com/docs)\n- [Vercel API Reference](https://vercel.com/docs/api)\n\n## Next Steps\nFor common errors, see `vercel-common-errors`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-edge-functions/SKILL.md"
    },
    {
      "slug": "vercel-enterprise-rbac",
      "name": "vercel-enterprise-rbac",
      "description": "Configure Vercel enterprise SSO, role-based access control, and organization management. Use when implementing SSO integration, configuring role-based permissions, or setting up organization-level controls for Vercel. Trigger with phrases like \"vercel SSO\", \"vercel RBAC\", \"vercel enterprise\", \"vercel roles\", \"vercel permissions\", \"vercel SAML\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Enterprise Rbac\n\n## Prerequisites\n- Vercel Enterprise tier subscription\n- Identity Provider (IdP) with SAML/OIDC support\n- Understanding of role-based access patterns\n- Audit logging infrastructure\n\n## Instructions\n\n### Step 1: Define Roles\nMap organizational roles to Vercel permissions.\n\n### Step 2: Configure SSO\nSet up SAML or OIDC integration with your IdP.\n\n### Step 3: Implement Middleware\nAdd permission checks to API endpoints.\n\n### Step 4: Enable Audit Logging\nTrack all access for compliance.\n\n## Output\n- Role definitions implemented\n- SSO integration configured\n- Permission middleware active\n- Audit trail enabled\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel Enterprise Guide](https://vercel.com/docs/enterprise)\n- [SAML 2.0 Specification](https://wiki.oasis-open.org/security/FrontPage)\n- [OpenID Connect Spec](https://openid.net/specs/openid-connect-core-1_0.html)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-enterprise-rbac/SKILL.md"
    },
    {
      "slug": "vercel-hello-world",
      "name": "vercel-hello-world",
      "description": "Create a minimal working Vercel example. Use when starting a new Vercel integration, testing your setup, or learning basic Vercel API patterns. Trigger with phrases like \"vercel hello world\", \"vercel example\", \"vercel quick start\", \"simple vercel code\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Hello World\n\n## Overview\nMinimal working example demonstrating core Vercel functionality.\n\n## Prerequisites\n- Completed `vercel-install-auth` setup\n- Valid API credentials configured\n- Development environment ready\n\n## Instructions\n\n### Step 1: Create Entry File\nCreate a new file for your hello world example.\n\n### Step 2: Import and Initialize Client\n```typescript\nimport { VercelClient } from 'vercel';\n\nconst client = new VercelClient({\n  apiKey: process.env.VERCEL_API_KEY,\n});\n```\n\n### Step 3: Make Your First API Call\n```typescript\nasync function main() {\n  const projects = await vercel.projects.list(); console.log('Projects:', projects.map(p => p.name));\n}\n\nmain().catch(console.error);\n```\n\n## Output\n- Working code file with Vercel client initialization\n- Successful API response confirming connection\n- Console output showing:\n```\nSuccess! Your Vercel connection is working.\n```\n\n## Error Handling\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Import Error | SDK not installed | Verify with `npm list` or `pip show` |\n| Auth Error | Invalid credentials | Check environment variable is set |\n| Timeout | Network issues | Increase timeout or check connectivity |\n| Rate Limit | Too many requests | Wait and retry with exponential backoff |\n\n## Examples\n\n### TypeScript Example\n```typescript\nimport { VercelClient } from 'vercel';\n\nconst client = new VercelClient({\n  apiKey: process.env.VERCEL_API_KEY,\n});\n\nasync function main() {\n  const projects = await vercel.projects.list(); console.log('Projects:', projects.map(p => p.name));\n}\n\nmain().catch(console.error);\n```\n\n### Python Example\n```python\nfrom None import VercelClient\n\nclient = VercelClient()\n\nNone\n```\n\n## Resources\n- [Vercel Getting Started](https://vercel.com/docs/getting-started)\n- [Vercel API Reference](https://vercel.com/docs/api)\n- [Vercel Examples](https://vercel.com/docs/examples)\n\n## Next Steps\nProceed to `vercel-local-dev-loop` for development workflow setup.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-hello-world/SKILL.md"
    },
    {
      "slug": "vercel-incident-runbook",
      "name": "vercel-incident-runbook",
      "description": "Execute Vercel incident response procedures with triage, mitigation, and postmortem. Use when responding to Vercel-related outages, investigating errors, or running post-incident reviews for Vercel integration failures. Trigger with phrases like \"vercel incident\", \"vercel outage\", \"vercel down\", \"vercel on-call\", \"vercel emergency\", \"vercel broken\". allowed-tools: Read, Grep, Bash(kubectl:*), Bash(curl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Incident Runbook\n\n## Prerequisites\n- Access to Vercel dashboard and status page\n- kubectl access to production cluster\n- Prometheus/Grafana access\n- Communication channels (Slack, PagerDuty)\n\n## Instructions\n\n### Step 1: Quick Triage\nRun the triage commands to identify the issue source.\n\n### Step 2: Follow Decision Tree\nDetermine if the issue is Vercel-side or internal.\n\n### Step 3: Execute Immediate Actions\nApply the appropriate remediation for the error type.\n\n### Step 4: Communicate Status\nUpdate internal and external stakeholders.\n\n## Output\n- Issue identified and categorized\n- Remediation applied\n- Stakeholders notified\n- Evidence collected for postmortem\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel Status Page](https://www.vercel-status.com)\n- [Vercel Support](https://support.vercel.com)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-incident-runbook/SKILL.md"
    },
    {
      "slug": "vercel-install-auth",
      "name": "vercel-install-auth",
      "description": "Install and configure Vercel SDK/CLI authentication. Use when setting up a new Vercel integration, configuring API keys, or initializing Vercel in your project. Trigger with phrases like \"install vercel\", \"setup vercel\", \"vercel auth\", \"configure vercel API key\". allowed-tools: Read, Write, Edit, Bash(npm:*), Bash(pip:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Install & Auth\n\n## Overview\nSet up Vercel SDK/CLI and configure authentication credentials.\n\n## Prerequisites\n- Node.js 18+ or Python 3.10+\n- Package manager (npm, pnpm, or pip)\n- Vercel account with API access\n- API key from Vercel dashboard\n\n## Instructions\n\n### Step 1: Install SDK\n```bash\n# Node.js\nnpm install vercel\n\n# Python\npip install None\n```\n\n### Step 2: Configure Authentication\n```bash\n# Set environment variable\nexport VERCEL_API_KEY=\"your-api-key\"\n\n# Or create .env file\necho 'VERCEL_API_KEY=your-api-key' >> .env\n```\n\n### Step 3: Verify Connection\n```typescript\nconst teams = await vercel.teams.list(); console.log(teams.length > 0 ? 'OK' : 'No teams');\n```\n\n## Output\n- Installed SDK package in node_modules or site-packages\n- Environment variable or .env file with API key\n- Successful connection verification output\n\n## Error Handling\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Invalid API Key | Incorrect or expired key | Verify key in Vercel dashboard |\n| Rate Limited | Exceeded quota | Check quota at https://vercel.com/docs |\n| Network Error | Firewall blocking | Ensure outbound HTTPS allowed |\n| Module Not Found | Installation failed | Run `npm install` or `pip install` again |\n\n## Examples\n\n### TypeScript Setup\n```typescript\nimport { VercelClient } from 'vercel';\n\nconst client = new VercelClient({\n  apiKey: process.env.VERCEL_API_KEY,\n});\n```\n\n### Python Setup\n```python\nfrom None import VercelClient\n\nclient = VercelClient(\n    api_key=os.environ.get('VERCEL_API_KEY')\n)\n```\n\n## Resources\n- [Vercel Documentation](https://vercel.com/docs)\n- [Vercel Dashboard](https://api.vercel.com)\n- [Vercel Status](https://www.vercel-status.com)\n\n## Next Steps\nAfter successful auth, proceed to `vercel-hello-world` for your first API call.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-install-auth/SKILL.md"
    },
    {
      "slug": "vercel-known-pitfalls",
      "name": "vercel-known-pitfalls",
      "description": "Execute identify and avoid Vercel anti-patterns and common integration mistakes. Use when reviewing Vercel code for issues, onboarding new developers, or auditing existing Vercel integrations for best practices violations. Trigger with phrases like \"vercel mistakes\", \"vercel anti-patterns\", \"vercel pitfalls\", \"vercel what not to do\", \"vercel code review\". allowed-tools: Read, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Known Pitfalls\n\n## Prerequisites\n- Access to Vercel codebase for review\n- Understanding of async/await patterns\n- Knowledge of security best practices\n- Familiarity with rate limiting concepts\n\n## Instructions\n\n### Step 1: Review for Anti-Patterns\nScan codebase for each pitfall pattern.\n\n### Step 2: Prioritize Fixes\nAddress security issues first, then performance.\n\n### Step 3: Implement Better Approach\nReplace anti-patterns with recommended patterns.\n\n### Step 4: Add Prevention\nSet up linting and CI checks to prevent recurrence.\n\n## Output\n- Anti-patterns identified\n- Fixes prioritized and implemented\n- Prevention measures in place\n- Code quality improved\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel Security Guide](https://vercel.com/docs/security)\n- [Vercel Best Practices](https://vercel.com/docs/best-practices)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-known-pitfalls/SKILL.md"
    },
    {
      "slug": "vercel-load-scale",
      "name": "vercel-load-scale",
      "description": "Implement Vercel load testing, auto-scaling, and capacity planning strategies. Use when running performance tests, configuring horizontal scaling, or planning capacity for Vercel integrations. Trigger with phrases like \"vercel load test\", \"vercel scale\", \"vercel performance test\", \"vercel capacity\", \"vercel k6\", \"vercel benchmark\". allowed-tools: Read, Write, Edit, Bash(k6:*), Bash(kubectl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Load Scale\n\n## Prerequisites\n- k6 load testing tool installed\n- Kubernetes cluster with HPA configured\n- Prometheus for metrics collection\n- Test environment API keys\n\n## Instructions\n\n### Step 1: Create Load Test Script\nWrite k6 test script with appropriate thresholds.\n\n### Step 2: Configure Auto-Scaling\nSet up HPA with CPU and custom metrics.\n\n### Step 3: Run Load Test\nExecute test and collect metrics.\n\n### Step 4: Analyze and Document\nRecord results in benchmark template.\n\n## Output\n- Load test script created\n- HPA configured\n- Benchmark results documented\n- Capacity recommendations defined\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [k6 Documentation](https://k6.io/docs/)\n- [Kubernetes HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)\n- [Vercel Rate Limits](https://vercel.com/docs/rate-limits)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-load-scale/SKILL.md"
    },
    {
      "slug": "vercel-local-dev-loop",
      "name": "vercel-local-dev-loop",
      "description": "Configure Vercel local development with hot reload and testing. Use when setting up a development environment, configuring test workflows, or establishing a fast iteration cycle with Vercel. Trigger with phrases like \"vercel dev setup\", \"vercel local development\", \"vercel dev environment\", \"develop with vercel\". allowed-tools: Read, Write, Edit, Bash(npm:*), Bash(pnpm:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Local Dev Loop\n\n## Prerequisites\n- Completed `vercel-install-auth` setup\n- Node.js 18+ with npm/pnpm\n- Code editor with TypeScript support\n- Git for version control\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n- Working development environment with hot reload\n- Configured test suite with mocking\n- Environment variable management\n- Fast iteration cycle for Vercel development\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel SDK Reference](https://vercel.com/docs/sdk)\n- [Vitest Documentation](https://vitest.dev/)\n- [tsx Documentation](https://github.com/esbuild-kit/tsx)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-local-dev-loop/SKILL.md"
    },
    {
      "slug": "vercel-migration-deep-dive",
      "name": "vercel-migration-deep-dive",
      "description": "Execute Vercel major re-architecture and migration strategies with strangler fig pattern. Use when migrating to or from Vercel, performing major version upgrades, or re-platforming existing integrations to Vercel. Trigger with phrases like \"migrate vercel\", \"vercel migration\", \"switch to vercel\", \"vercel replatform\", \"vercel upgrade major\". allowed-tools: Read, Write, Edit, Bash(npm:*), Bash(node:*), Bash(kubectl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Migration Deep Dive\n\n## Prerequisites\n- Current system documentation\n- Vercel SDK installed\n- Feature flag infrastructure\n- Rollback strategy tested\n\n## Instructions\n\n### Step 1: Assess Current State\nDocument existing implementation and data inventory.\n\n### Step 2: Build Adapter Layer\nCreate abstraction layer for gradual migration.\n\n### Step 3: Migrate Data\nRun batch data migration with error handling.\n\n### Step 4: Shift Traffic\nGradually route traffic to new Vercel integration.\n\n## Output\n- Migration assessment complete\n- Adapter layer implemented\n- Data migrated successfully\n- Traffic fully shifted to Vercel\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Strangler Fig Pattern](https://martinfowler.com/bliki/StranglerFigApplication.html)\n- [Vercel Migration Guide](https://vercel.com/docs/migration)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-migration-deep-dive/SKILL.md"
    },
    {
      "slug": "vercel-multi-env-setup",
      "name": "vercel-multi-env-setup",
      "description": "Configure Vercel across development, staging, and production environments. Use when setting up multi-environment deployments, configuring per-environment secrets, or implementing environment-specific Vercel configurations. Trigger with phrases like \"vercel environments\", \"vercel staging\", \"vercel dev prod\", \"vercel environment setup\", \"vercel config by env\". allowed-tools: Read, Write, Edit, Bash(aws:*), Bash(gcloud:*), Bash(vault:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Multi Env Setup\n\n## Prerequisites\n- Separate Vercel accounts or API keys per environment\n- Secret management solution (Vault, AWS Secrets Manager, etc.)\n- CI/CD pipeline with environment variables\n- Environment detection in application\n\n## Instructions\n\n### Step 1: Create Config Structure\nSet up the base and per-environment configuration files.\n\n### Step 2: Implement Environment Detection\nAdd logic to detect and load environment-specific config.\n\n### Step 3: Configure Secrets\nStore API keys securely using your secret management solution.\n\n### Step 4: Add Environment Guards\nImplement safeguards for production-only operations.\n\n## Output\n- Multi-environment config structure\n- Environment detection logic\n- Secure secret management\n- Production safeguards enabled\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel Environments Guide](https://vercel.com/docs/environments)\n- [12-Factor App Config](https://12factor.net/config)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-multi-env-setup/SKILL.md"
    },
    {
      "slug": "vercel-observability",
      "name": "vercel-observability",
      "description": "Execute set up comprehensive observability for Vercel integrations with metrics, traces, and alerts. Use when implementing monitoring for Vercel operations, setting up dashboards, or configuring alerting for Vercel integration health. Trigger with phrases like \"vercel monitoring\", \"vercel metrics\", \"vercel observability\", \"monitor vercel\", \"vercel alerts\", \"vercel tracing\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Observability\n\n## Prerequisites\n- Prometheus or compatible metrics backend\n- OpenTelemetry SDK installed\n- Grafana or similar dashboarding tool\n- AlertManager configured\n\n## Instructions\n\n### Step 1: Set Up Metrics Collection\nImplement Prometheus counters, histograms, and gauges for key operations.\n\n### Step 2: Add Distributed Tracing\nIntegrate OpenTelemetry for end-to-end request tracing.\n\n### Step 3: Configure Structured Logging\nSet up JSON logging with consistent field names.\n\n### Step 4: Create Alert Rules\nDefine Prometheus alerting rules for error rates and latency.\n\n## Output\n- Metrics collection enabled\n- Distributed tracing configured\n- Structured logging implemented\n- Alert rules deployed\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Prometheus Best Practices](https://prometheus.io/docs/practices/naming/)\n- [OpenTelemetry Documentation](https://opentelemetry.io/docs/)\n- [Vercel Observability Guide](https://vercel.com/docs/observability)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-observability/SKILL.md"
    },
    {
      "slug": "vercel-performance-tuning",
      "name": "vercel-performance-tuning",
      "description": "Optimize Vercel API performance with caching, batching, and connection pooling. Use when experiencing slow API responses, implementing caching strategies, or optimizing request throughput for Vercel integrations. Trigger with phrases like \"vercel performance\", \"optimize vercel\", \"vercel latency\", \"vercel caching\", \"vercel slow\", \"vercel batch\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Performance Tuning\n\n## Prerequisites\n- Vercel SDK installed\n- Understanding of async patterns\n- Redis or in-memory cache available (optional)\n- Performance monitoring in place\n\n## Instructions\n\n### Step 1: Establish Baseline\nMeasure current latency for critical Vercel operations.\n\n### Step 2: Implement Caching\nAdd response caching for frequently accessed data.\n\n### Step 3: Enable Batching\nUse DataLoader or similar for automatic request batching.\n\n### Step 4: Optimize Connections\nConfigure connection pooling with keep-alive.\n\n## Output\n- Reduced API latency\n- Caching layer implemented\n- Request batching enabled\n- Connection pooling configured\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel Performance Guide](https://vercel.com/docs/performance)\n- [DataLoader Documentation](https://github.com/graphql/dataloader)\n- [LRU Cache Documentation](https://github.com/isaacs/node-lru-cache)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-performance-tuning/SKILL.md"
    },
    {
      "slug": "vercel-policy-guardrails",
      "name": "vercel-policy-guardrails",
      "description": "Implement Vercel lint rules, policy enforcement, and automated guardrails. Use when setting up code quality rules for Vercel integrations, implementing pre-commit hooks, or configuring CI policy checks for Vercel best practices. Trigger with phrases like \"vercel policy\", \"vercel lint\", \"vercel guardrails\", \"vercel best practices check\", \"vercel eslint\". allowed-tools: Read, Write, Edit, Bash(npx:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Policy Guardrails\n\n## Prerequisites\n- ESLint configured in project\n- Pre-commit hooks infrastructure\n- CI/CD pipeline with policy checks\n- TypeScript for type enforcement\n\n## Instructions\n\n### Step 1: Create ESLint Rules\nImplement custom lint rules for Vercel patterns.\n\n### Step 2: Configure Pre-Commit Hooks\nSet up hooks to catch issues before commit.\n\n### Step 3: Add CI Policy Checks\nImplement policy-as-code in CI pipeline.\n\n### Step 4: Enable Runtime Guardrails\nAdd production safeguards for dangerous operations.\n\n## Output\n- ESLint plugin with Vercel rules\n- Pre-commit hooks blocking secrets\n- CI policy checks passing\n- Runtime guardrails active\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [ESLint Plugin Development](https://eslint.org/docs/latest/extend/plugins)\n- [Pre-commit Framework](https://pre-commit.com/)\n- [Open Policy Agent](https://www.openpolicyagent.org/)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-policy-guardrails/SKILL.md"
    },
    {
      "slug": "vercel-prod-checklist",
      "name": "vercel-prod-checklist",
      "description": "Execute Vercel production deployment checklist and rollback procedures. Use when deploying Vercel integrations to production, preparing for launch, or implementing go-live procedures. Trigger with phrases like \"vercel production\", \"deploy vercel\", \"vercel go-live\", \"vercel launch checklist\". allowed-tools: Read, Bash(kubectl:*), Bash(curl:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Prod Checklist\n\n## Prerequisites\n- Staging environment tested and verified\n- Production API keys available\n- Deployment pipeline configured\n- Monitoring and alerting ready\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n- Deployed Vercel integration\n- Health checks passing\n- Monitoring active\n- Rollback procedure documented\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel Status](https://www.vercel-status.com)\n- [Vercel Support](https://vercel.com/docs/support)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-prod-checklist/SKILL.md"
    },
    {
      "slug": "vercel-rate-limits",
      "name": "vercel-rate-limits",
      "description": "Implement Vercel rate limiting, backoff, and idempotency patterns. Use when handling rate limit errors, implementing retry logic, or optimizing API request throughput for Vercel. Trigger with phrases like \"vercel rate limit\", \"vercel throttling\", \"vercel 429\", \"vercel retry\", \"vercel backoff\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Rate Limits\n\n## Prerequisites\n- Vercel SDK installed\n- Understanding of async/await patterns\n- Access to rate limit headers\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n- Reliable API calls with automatic retry\n- Idempotent requests preventing duplicates\n- Rate limit headers properly handled\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel Rate Limits](https://vercel.com/docs/rate-limits)\n- [p-queue Documentation](https://github.com/sindresorhus/p-queue)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-rate-limits/SKILL.md"
    },
    {
      "slug": "vercel-reference-architecture",
      "name": "vercel-reference-architecture",
      "description": "Implement Vercel reference architecture with best-practice project layout. Use when designing new Vercel integrations, reviewing project structure, or establishing architecture standards for Vercel applications. Trigger with phrases like \"vercel architecture\", \"vercel best practices\", \"vercel project structure\", \"how to organize vercel\", \"vercel layout\". allowed-tools: Read, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Reference Architecture\n\n## Prerequisites\n- Understanding of layered architecture\n- Vercel SDK knowledge\n- TypeScript project setup\n- Testing framework configured\n\n## Instructions\n\n### Step 1: Create Directory Structure\nSet up the project layout following the reference structure above.\n\n### Step 2: Implement Client Wrapper\nCreate the singleton client with caching and monitoring.\n\n### Step 3: Add Error Handling\nImplement custom error classes for Vercel operations.\n\n### Step 4: Configure Health Checks\nAdd health check endpoint for Vercel connectivity.\n\n## Output\n- Structured project layout\n- Client wrapper with caching\n- Error boundary implemented\n- Health checks configured\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel SDK Documentation](https://vercel.com/docs/sdk)\n- [Vercel Best Practices](https://vercel.com/docs/best-practices)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-reference-architecture/SKILL.md"
    },
    {
      "slug": "vercel-reliability-patterns",
      "name": "vercel-reliability-patterns",
      "description": "Implement Vercel reliability patterns including circuit breakers, idempotency, and graceful degradation. Use when building fault-tolerant Vercel integrations, implementing retry strategies, or adding resilience to production Vercel services. Trigger with phrases like \"vercel reliability\", \"vercel circuit breaker\", \"vercel idempotent\", \"vercel resilience\", \"vercel fallback\", \"vercel bulkhead\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Reliability Patterns\n\n## Prerequisites\n- Understanding of circuit breaker pattern\n- opossum or similar library installed\n- Queue infrastructure for DLQ\n- Caching layer for fallbacks\n\n## Instructions\n\n### Step 1: Implement Circuit Breaker\nWrap Vercel calls with circuit breaker.\n\n### Step 2: Add Idempotency Keys\nGenerate deterministic keys for operations.\n\n### Step 3: Configure Bulkheads\nSeparate queues for different priorities.\n\n### Step 4: Set Up Dead Letter Queue\nHandle permanent failures gracefully.\n\n## Output\n- Circuit breaker protecting Vercel calls\n- Idempotency preventing duplicates\n- Bulkhead isolation implemented\n- DLQ for failed operations\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Circuit Breaker Pattern](https://martinfowler.com/bliki/CircuitBreaker.html)\n- [Opossum Documentation](https://nodeshift.dev/opossum/)\n- [Vercel Reliability Guide](https://vercel.com/docs/reliability)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-reliability-patterns/SKILL.md"
    },
    {
      "slug": "vercel-sdk-patterns",
      "name": "vercel-sdk-patterns",
      "description": "Execute apply production-ready Vercel SDK patterns for TypeScript and Python. Use when implementing Vercel integrations, refactoring SDK usage, or establishing team coding standards for Vercel. Trigger with phrases like \"vercel SDK patterns\", \"vercel best practices\", \"vercel code patterns\", \"idiomatic vercel\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Sdk Patterns\n\n## Prerequisites\n- Completed `vercel-install-auth` setup\n- Familiarity with async/await patterns\n- Understanding of error handling best practices\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n- Type-safe client singleton\n- Robust error handling with structured logging\n- Automatic retry with exponential backoff\n- Runtime validation for API responses\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel SDK Reference](https://vercel.com/docs/sdk)\n- [Vercel API Types](https://vercel.com/docs/types)\n- [Zod Documentation](https://zod.dev/)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-sdk-patterns/SKILL.md"
    },
    {
      "slug": "vercel-security-basics",
      "name": "vercel-security-basics",
      "description": "Execute apply Vercel security best practices for secrets and access control. Use when securing API keys, implementing least privilege access, or auditing Vercel security configuration. Trigger with phrases like \"vercel security\", \"vercel secrets\", \"secure vercel\", \"vercel API key security\". allowed-tools: Read, Write, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Security Basics\n\n## Prerequisites\n- Vercel SDK installed\n- Understanding of environment variables\n- Access to Vercel dashboard\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n- Secure API key storage\n- Environment-specific access controls\n- Audit logging enabled\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel Security Guide](https://vercel.com/docs/security)\n- [Vercel API Scopes](https://vercel.com/docs/scopes)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-security-basics/SKILL.md"
    },
    {
      "slug": "vercel-upgrade-migration",
      "name": "vercel-upgrade-migration",
      "description": "Execute analyze, plan, and execute Vercel SDK upgrades with breaking change detection. Use when upgrading Vercel SDK versions, detecting deprecations, or migrating to new API versions. Trigger with phrases like \"upgrade vercel\", \"vercel migration\", \"vercel breaking changes\", \"update vercel SDK\", \"analyze vercel version\". allowed-tools: Read, Write, Edit, Bash(npm:*), Bash(git:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Upgrade Migration\n\n## Prerequisites\n- Current Vercel SDK installed\n- Git for version control\n- Test suite available\n- Staging environment\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n- Updated SDK version\n- Fixed breaking changes\n- Passing test suite\n- Documented rollback procedure\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel Changelog](https://github.com/vercel/vercel/releases)\n- [Vercel Migration Guide](https://vercel.com/docs/migration)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-upgrade-migration/SKILL.md"
    },
    {
      "slug": "vercel-webhooks-events",
      "name": "vercel-webhooks-events",
      "description": "Implement Vercel webhook signature validation and event handling. Use when setting up webhook endpoints, implementing signature verification, or handling Vercel event notifications securely. Trigger with phrases like \"vercel webhook\", \"vercel events\", \"vercel webhook signature\", \"handle vercel events\", \"vercel notifications\". allowed-tools: Read, Write, Edit, Bash(curl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Webhooks Events\n\n## Prerequisites\n- Vercel webhook secret configured\n- HTTPS endpoint accessible from internet\n- Understanding of cryptographic signatures\n- Redis or database for idempotency (optional)\n\n## Instructions\n\n### Step 1: Register Webhook Endpoint\nConfigure your webhook URL in the Vercel dashboard.\n\n### Step 2: Implement Signature Verification\nUse the signature verification code to validate incoming webhooks.\n\n### Step 3: Handle Events\nImplement handlers for each event type your application needs.\n\n### Step 4: Add Idempotency\nPrevent duplicate processing with event ID tracking.\n\n## Output\n- Secure webhook endpoint\n- Signature validation enabled\n- Event handlers implemented\n- Replay attack protection active\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n- [Vercel Webhooks Guide](https://vercel.com/docs/webhooks)\n- [Webhook Security Best Practices](https://vercel.com/docs/webhooks/security)",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-webhooks-events/SKILL.md"
    },
    {
      "slug": "version-bumper",
      "name": "version-bumper",
      "description": "Execute automatically handles semantic version updates across plugin.json and marketplace catalog when user mentions version bump, update version, or release. ensures version consistency in AI assistant-code-plugins repository. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <plugins@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Version Bumper\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe output is a concrete, repo-ready version bump plan and execution summary, including the computed `old_version → new_version`, the exact files updated (plugin `.claude-plugin/plugin.json`, `.claude-plugin/marketplace.extended.json`, regenerated `.claude-plugin/marketplace.json` when applicable), and the next validation commands to run.\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "jeremy-plugin-tool",
        "category": "examples",
        "path": "plugins/examples/jeremy-plugin-tool",
        "version": "2.0.0",
        "description": "Ultimate plugin management toolkit with 4 auto-invoked Skills for creating, validating, auditing, and versioning plugins in the claude-code-plugins marketplace"
      },
      "filePath": "plugins/examples/jeremy-plugin-tool/skills/version-bumper/SKILL.md"
    },
    {
      "slug": "versioning-apis",
      "name": "versioning-apis",
      "description": "Implement API versioning with backward compatibility, deprecation notices, and migration paths. Use when managing API versions and backward compatibility. Trigger with phrases like \"version the API\", \"manage API versions\", or \"handle API versioning\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:version-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Versioning Apis\n\n## Overview\n\n\nThis skill provides automated assistance for api versioning manager tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n1. Generate boilerplate code using Bash(api:version-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n1. Write integration tests covering all endpoints\n\n\nSee `{baseDir}/references/implementation.md` for detailed implementation guide.\n\n## Output\n\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n- OpenAPI 3.0 specification with complete endpoint definitions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n- OpenAPI Specification 3.0+ for API documentation",
      "parentPlugin": {
        "name": "api-versioning-manager",
        "category": "api-development",
        "path": "plugins/api-development/api-versioning-manager",
        "version": "1.0.0",
        "description": "Manage API versions with migration strategies and backward compatibility"
      },
      "filePath": "plugins/api-development/api-versioning-manager/skills/versioning-apis/SKILL.md"
    },
    {
      "slug": "vertex-agent-builder",
      "name": "vertex-agent-builder",
      "description": "Build and deploy production-ready generative AI agents using Vertex AI, Gemini models, and Google Cloud infrastructure with RAG, function calling, and multi-modal capabilities. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Vertex AI Agent Builder\n\nBuild and deploy production-ready agents on Vertex AI with Gemini models, retrieval (RAG), function calling, and operational guardrails (validation, monitoring, cost controls).\n\n## Overview\n\n- Produces an agent scaffold aligned with Vertex AI Agent Engine deployment patterns.\n- Helps choose models/regions, design tool/function interfaces, and wire up retrieval.\n- Includes an evaluation + smoke-test checklist so deployments don’t regress.\n\n## Prerequisites\n\n- Google Cloud project with Vertex AI API enabled\n- Permissions to deploy/operate Agent Engine runtimes (or a local-only build target)\n- If using RAG: a document source (GCS/BigQuery/Firestore/etc) and an embeddings/index strategy\n- Secrets handled via env vars or Secret Manager (never committed)\n\n## Instructions\n\n1. Clarify the agent’s job (user intents, inputs/outputs, latency and cost constraints).\n2. Choose model + region and define tool/function interfaces (schemas, error contracts).\n3. Implement retrieval (if needed): chunking, embeddings, index, and a “citation-first” response format.\n4. Add evaluation: golden prompts, offline checks, and a minimal online smoke test.\n5. Deploy (optional): provide the exact deployment command/config and verify endpoints + permissions.\n6. Add ops: logs/metrics, alerting, quota/cost guardrails, and rollback steps.\n\n## Output\n\n- A Vertex AI agent scaffold (code/config) with clear extension points\n- A retrieval plan (when applicable) and a validation/evaluation checklist\n- Optional: deployment commands and post-deploy health checks\n\n## Error Handling\n\n- Quota/region issues: detect the failing service/quota and propose a scoped fix.\n- Auth failures: identify the principal and missing role; prefer least-privilege remediation.\n- Retrieval failures: validate indexing/embedding dimensions and add fallback behavior.\n- Tool/function errors: enforce structured error responses and add regression tests.\n\n## Examples\n\n**Example: RAG support agent**\n- Request: “Deploy a support bot that answers from our docs with citations.”\n- Result: ingestion plan, retrieval wiring, evaluation prompts, and a smoke test that verifies citations.\n\n**Example: Multimodal intake agent**\n- Request: “Build an agent that extracts structured fields from PDFs/images and routes tasks.”\n- Result: schema-first extraction prompts, tool interface contracts, and validation examples.\n\n## Resources\n\n- Full detailed guide (kept for reference): `{baseDir}/references/SKILL.full.md`\n- Repo standards (source of truth):\n  - `000-docs/6767-a-SPEC-DR-STND-claude-code-plugins-standard.md`\n  - `000-docs/6767-b-SPEC-DR-STND-claude-skills-standard.md`\n- Vertex AI docs: https://cloud.google.com/vertex-ai/docs\n- Agent Engine docs: https://cloud.google.com/vertex-ai/docs/agent-engine",
      "parentPlugin": {
        "name": "jeremy-vertex-ai",
        "category": "jeremy-vertex-ai",
        "path": "plugins/jeremy-vertex-ai",
        "version": "1.0.0",
        "description": "Comprehensive Vertex AI integration plugin for building generative AI agents with Gemini, Vertex AI Studio, and production deployment on Google Cloud"
      },
      "filePath": "plugins/jeremy-vertex-ai/skills/vertex-agent-builder/SKILL.md"
    },
    {
      "slug": "vertex-ai-media-master",
      "name": "vertex-ai-media-master",
      "description": "Execute automatic activation for all google vertex ai multimodal operations operations. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(general:*), Bash(util:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vertex Ai Media Master\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Access to project files in {baseDir}/\n- Required tools and dependencies installed\n- Understanding of skill functionality\n- Permissions for file operations\n\n## Instructions\n\n1. Identify skill activation trigger and context\n2. Gather required inputs and parameters\n3. Execute skill workflow systematically\n4. Validate outputs meet requirements\n5. Handle errors and edge cases appropriately\n6. Provide clear results and next steps\n\n## Output\n\n- Primary deliverables based on skill purpose\n- Status indicators and success metrics\n- Generated files or configurations\n- Reports and summaries as applicable\n- Recommendations for follow-up actions\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Official documentation for related tools\n- Best practices guides\n- Example use cases and templates\n- Community forums and support channels",
      "parentPlugin": {
        "name": "003-jeremy-vertex-ai-media-master",
        "category": "productivity",
        "path": "plugins/productivity/003-jeremy-vertex-ai-media-master",
        "version": "1.0.0",
        "description": "Comprehensive Google Vertex AI multimodal mastery for Jeremy - video processing (6+ hours), audio generation, image creation with Gemini 2.0/2.5 and Imagen 4. Marketing campaign automation, content generation, and media asset production."
      },
      "filePath": "plugins/productivity/003-jeremy-vertex-ai-media-master/skills/vertex-ai-media-master/SKILL.md"
    },
    {
      "slug": "vertex-engine-inspector",
      "name": "vertex-engine-inspector",
      "description": "Execute inspect and validate Vertex AI Agent Engine deployments including Code Execution Sandbox, Memory Bank, A2A protocol compliance, and security posture. Generates production readiness scores. Use when asked to \"inspect agent engine\" or \"validate depl... Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vertex Engine Inspector\n\n## Overview\n\n\nThis skill provides automated assistance for vertex engine inspector tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "jeremy-vertex-engine",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-vertex-engine",
        "version": "1.0.0",
        "description": "Vertex AI Agent Engine deployment inspector and runtime validator"
      },
      "filePath": "plugins/ai-ml/jeremy-vertex-engine/skills/vertex-engine-inspector/SKILL.md"
    },
    {
      "slug": "vertex-infra-expert",
      "name": "vertex-infra-expert",
      "description": "Execute use when provisioning Vertex AI infrastructure with Terraform. Trigger with phrases like \"vertex ai terraform\", \"deploy gemini terraform\", \"model garden infrastructure\", \"vertex ai endpoints terraform\", or \"vector search terraform\". Provisions Model Garden models, Gemini endpoints, vector search indices, ML pipelines, and production AI services with encryption and auto-scaling. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(terraform:*), Bash(gcloud:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vertex Infra Expert\n\n## Overview\n\nProvision Vertex AI infrastructure with Terraform (endpoints, deployed models, vector search indices, pipelines) with production guardrails: encryption, autoscaling, IAM least privilege, and operational validation steps. Use this skill to generate a minimal working Terraform baseline and iterate toward enterprise-ready deployments.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Google Cloud project with Vertex AI API enabled\n- Terraform 1.0+ installed\n- gcloud CLI authenticated with appropriate permissions\n- Understanding of Vertex AI services and ML models\n- KMS keys created for encryption (if required)\n- GCS buckets for model artifacts and embeddings\n\n## Instructions\n\n1. **Define AI Services**: Identify required Vertex AI components (endpoints, vector search, pipelines)\n2. **Configure Terraform**: Set up backend and define project variables\n3. **Provision Endpoints**: Deploy Gemini or custom model endpoints with auto-scaling\n4. **Set Up Vector Search**: Create indices for embeddings with appropriate dimensions\n5. **Configure Encryption**: Apply KMS encryption to endpoints and data\n6. **Implement Monitoring**: Set up Cloud Monitoring for model performance\n7. **Apply IAM Policies**: Grant least privilege access to AI services\n8. **Validate Deployment**: Test endpoints and verify model availability\n\n## Output\n\n\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- Vertex AI Terraform: https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/vertex_ai_endpoint\n- Vertex AI documentation: https://cloud.google.com/vertex-ai/docs\n- Model Garden: https://cloud.google.com/model-garden\n- Vector Search guide: https://cloud.google.com/vertex-ai/docs/vector-search\n- Terraform examples in {baseDir}/vertex-examples/",
      "parentPlugin": {
        "name": "jeremy-vertex-terraform",
        "category": "devops",
        "path": "plugins/devops/jeremy-vertex-terraform",
        "version": "1.0.0",
        "description": "Terraform configurations for Vertex AI platform and Agent Engine"
      },
      "filePath": "plugins/devops/jeremy-vertex-terraform/skills/vertex-infra-expert/SKILL.md"
    },
    {
      "slug": "yaml-master",
      "name": "yaml-master",
      "description": "Execute proactive YAML intelligence: automatically activates when working with YAML files. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(general:*), Bash(util:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# YAML Master\n\nProactive YAML intelligence: validate syntax, enforce consistent formatting, and keep configuration files schema-correct (Kubernetes, GitHub Actions, Docker Compose, and similar).\n\n## Overview\n\nThis skill activates when working with `.yml`/`.yaml` files to detect structural issues early (indentation, anchors, type mismatches), and to produce safe, minimal edits that keep CI/config tooling happy.\n\n## Prerequisites\n\n- The YAML file(s) to inspect and their intended target (e.g., Kubernetes, GitHub Actions, Compose)\n- Any relevant schema or constraints (when available)\n- Permission to edit the file(s) (or to propose a patch)\n\n## Instructions\n\n1. Parse and validate YAML syntax (identify the first breaking error and its location).\n2. Normalize formatting (indentation, quoting) without changing semantics.\n3. Validate structure against the target system’s expectations (keys, types, required fields).\n4. Identify risky patterns (duplicate keys, ambiguous scalars, anchors used incorrectly).\n5. Output a minimal patch plus a short validation checklist (what to run next).\n\n## Output\n\n- Corrected YAML with minimal diffs\n- A concise list of issues found (syntax vs schema vs best practice)\n- Follow-up validation commands appropriate for the target (e.g., `kubectl apply --dry-run=client`, CI lint)\n\n## Error Handling\n\n- If the schema/target is unknown, ask for the target system and apply syntax-only fixes first.\n- If the YAML is valid but tooling still fails, surface the exact downstream error and reconcile expectations.\n\n## Examples\n\n**Example: Fix an indentation/syntax error**\n- Input: a workflow with a mis-indented `steps:` block.\n- Output: corrected indentation and a note on which job/step was affected.\n\n**Example: Convert JSON to YAML safely**\n- Input: a JSON config blob.\n- Output: YAML with explicit quoting where necessary to avoid type surprises.\n\n## Resources\n\n- Full detailed guide (kept for reference): `{baseDir}/references/SKILL.full.md`\n- YAML spec: https://yaml.org/spec/\n- GitHub Actions workflow syntax: https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions",
      "parentPlugin": {
        "name": "002-jeremy-yaml-master-agent",
        "category": "productivity",
        "path": "plugins/productivity/002-jeremy-yaml-master-agent",
        "version": "1.0.0",
        "description": "Intelligent YAML validation, generation, and transformation agent with schema inference, linting, and format conversion capabilities"
      },
      "filePath": "plugins/productivity/002-jeremy-yaml-master-agent/skills/yaml-master/SKILL.md"
    },
    {
      "slug": "zai-cli",
      "name": "zai-cli",
      "description": "Execute z.AI CLI providing vision, search, reader, and GitHub exploration via CLI and MCP. Use when user needs image/video analysis, OCR, UI-to-code conversion, error diagnosis, real-time web search, web page to markdown extraction, or GitHub code exploration. Trigger with phrases like \"analyze this image\", \"search the web for\", \"read this page\", \"explore this repo\", or \"use zai\". Requires Z_AI_API_KEY. allowed-tools: Read, Write, Edit, Bash(cmd:*), WebFetch version: 1.0.0 license: Apache-2.0 author: Numman Ali <numman.ali@gmail.com>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Numman Ali <numman.ali@gmail.com>",
      "license": "MIT",
      "content": "# Zai Cli\n\n## Overview\n\nZAI CLI provides access to Z.AI capabilities including image/video analysis, real-time web search, web page extraction, and GitHub code exploration. It integrates with Claude Code via MCP protocol for seamless AI-powered content analysis.\n\n## Prerequisites\n\n- Node.js 18+ installed\n- Z_AI_API_KEY environment variable set\n- API key from https://z.ai/manage-apikey/apikey-list\n- Network access to Z.AI API endpoints\n\n## Instructions\n\n1. Obtain an API key from Z.AI platform\n2. Export your API key: `export Z_AI_API_KEY=\"your-key\"`\n3. Run `npx zai-cli doctor` to verify setup\n4. Use `npx zai-cli --help` to see available commands\n5. Try basic commands like vision, search, read, or repo\n6. Use `--help` on any subcommand for detailed options\n\nAccess Z.AI capabilities via `npx zai-cli`. The CLI is self-documenting - use `--help` at any level.\n\n## Output\n\nDefault: **data-only** (raw output for token efficiency).\nUse `--output-format json` for `{ success, data, timestamp }` wrapping.\n\n## Error Handling\n\nSee `{baseDir}/references/errors.md` for comprehensive error handling.\n\n## Examples\n\nSee `{baseDir}/references/examples.md` for detailed examples.\n\n## Resources\n\n- [Z.AI Platform](https://z.ai/)\n- [Z.AI API Key Management](https://z.ai/manage-apikey/apikey-list)\n- [zai-cli npm Package](https://www.npmjs.com/package/zai-cli)\n- [Z.AI Documentation](https://docs.z.ai/)\n- [MCP Protocol Reference](https://modelcontextprotocol.io/)",
      "parentPlugin": {
        "name": "zai-cli",
        "category": "community",
        "path": "plugins/community/zai-cli",
        "version": "1.0.0",
        "description": "Z.AI vision, search, reader, and GitHub exploration via CLI and MCP. Analyze images, search the web, read pages as markdown, explore repos."
      },
      "filePath": "plugins/community/zai-cli/skills/zai-cli/SKILL.md"
    }
  ],
  "count": 308,
  "generatedAt": "2026-01-06T08:24:28.704Z",
  "categories": [
    "ai-ml",
    "api-development",
    "automation",
    "business-tools",
    "community",
    "crypto",
    "database",
    "devops",
    "examples",
    "jeremy-google-adk",
    "jeremy-vertex-ai",
    "packages",
    "performance",
    "productivity",
    "saas-packs",
    "security",
    "skill-enhancers",
    "testing"
  ],
  "allowedToolsUsed": [
    "\"Read",
    "Bash(analysis:*)\"",
    "Bash(artillery:*)",
    "Bash(audit:*)\"",
    "Bash(awk:*)",
    "Bash(ci:*)",
    "Bash(curl:*)",
    "Bash(date:*)\"",
    "Bash(general:*)",
    "Bash(gh:*)",
    "Bash(git:*)",
    "Bash(grep:*)",
    "Bash(iostat:*)",
    "Bash(jmeter:*)",
    "Bash(k6:*)",
    "Bash(lighthouse:*)",
    "Bash(logs:*)",
    "Bash(memory:*)\"",
    "Bash(metrics:*)",
    "Bash(metrics:*)\"",
    "Bash(monitoring:*)",
    "Bash(monitoring:*)\"",
    "Bash(npm:*)",
    "Bash(performance:*)",
    "Bash(performance:*)\"",
    "Bash(ping:*)",
    "Bash(profiling:*)",
    "Bash(prometheus:*)",
    "Bash(ps:*)",
    "Bash(python:*)",
    "Bash(rum:*)\"",
    "Bash(scan:*)",
    "Bash(security:*)",
    "Bash(system:*)\"",
    "Bash(testing:*)\"",
    "Bash(top:*)",
    "Bash(traceroute:*)",
    "Bash(util:*)\"",
    "Bash(vmstat:*)",
    "Bash(webpack:*)",
    "Edit",
    "Glob",
    "Glob\"",
    "Grep",
    "Grep\"",
    "WebFetch",
    "WebSearch",
    "Write"
  ]
}